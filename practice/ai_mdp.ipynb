{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackmontgomery/anaconda3/lib/python3.11/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jackmontgomery/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-0.mp4\n",
      "Episode: 0 - Reward: 11.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1.mp4\n",
      "Episode: 1 - Reward: 30.0\n",
      "Episode: 2 - Reward: 14.0\n",
      "Episode: 3 - Reward: 11.0\n",
      "Episode: 4 - Reward: 16.0\n",
      "Episode: 5 - Reward: 19.0\n",
      "Episode: 6 - Reward: 9.0\n",
      "Episode: 7 - Reward: 14.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-8.mp4\n",
      "Episode: 8 - Reward: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9 - Reward: 19.0\n",
      "Episode: 10 - Reward: 12.0\n",
      "Episode: 11 - Reward: 28.0\n",
      "Episode: 12 - Reward: 16.0\n",
      "Episode: 13 - Reward: 16.0\n",
      "Episode: 14 - Reward: 18.0\n",
      "Episode: 15 - Reward: 14.0\n",
      "Episode: 16 - Reward: 21.0\n",
      "Episode: 17 - Reward: 14.0\n",
      "Episode: 18 - Reward: 23.0\n",
      "Episode: 19 - Reward: 14.0\n",
      "Episode: 20 - Reward: 14.0\n",
      "Episode: 21 - Reward: 23.0\n",
      "Episode: 22 - Reward: 12.0\n",
      "Episode: 23 - Reward: 40.0\n",
      "Episode: 24 - Reward: 10.0\n",
      "Episode: 25 - Reward: 14.0\n",
      "Episode: 26 - Reward: 12.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-27.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-27.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-27.mp4\n",
      "Episode: 27 - Reward: 17.0\n",
      "Episode: 28 - Reward: 19.0\n",
      "Episode: 29 - Reward: 16.0\n",
      "Episode: 30 - Reward: 13.0\n",
      "Episode: 31 - Reward: 13.0\n",
      "Episode: 32 - Reward: 54.0\n",
      "Episode: 33 - Reward: 21.0\n",
      "Episode: 34 - Reward: 16.0\n",
      "Episode: 35 - Reward: 20.0\n",
      "Episode: 36 - Reward: 10.0\n",
      "Episode: 37 - Reward: 15.0\n",
      "Episode: 38 - Reward: 20.0\n",
      "Episode: 39 - Reward: 28.0\n",
      "Episode: 40 - Reward: 17.0\n",
      "Episode: 41 - Reward: 23.0\n",
      "Episode: 42 - Reward: 32.0\n",
      "Episode: 43 - Reward: 17.0\n",
      "Episode: 44 - Reward: 21.0\n",
      "Episode: 45 - Reward: 9.0\n",
      "Episode: 46 - Reward: 10.0\n",
      "Episode: 47 - Reward: 12.0\n",
      "Episode: 48 - Reward: 40.0\n",
      "Episode: 49 - Reward: 20.0\n",
      "Episode: 50 - Reward: 20.0\n",
      "Episode: 51 - Reward: 15.0\n",
      "Episode: 52 - Reward: 11.0\n",
      "Episode: 53 - Reward: 20.0\n",
      "Episode: 54 - Reward: 12.0\n",
      "Episode: 55 - Reward: 13.0\n",
      "Episode: 56 - Reward: 19.0\n",
      "Episode: 57 - Reward: 43.0\n",
      "Episode: 58 - Reward: 11.0\n",
      "Episode: 59 - Reward: 12.0\n",
      "Episode: 60 - Reward: 9.0\n",
      "Episode: 61 - Reward: 16.0\n",
      "Episode: 62 - Reward: 16.0\n",
      "Episode: 63 - Reward: 8.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-64.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-64.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-64.mp4\n",
      "Episode: 64 - Reward: 16.0\n",
      "Episode: 65 - Reward: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 66 - Reward: 20.0\n",
      "Episode: 67 - Reward: 16.0\n",
      "Episode: 68 - Reward: 14.0\n",
      "Episode: 69 - Reward: 9.0\n",
      "Episode: 70 - Reward: 11.0\n",
      "Episode: 71 - Reward: 17.0\n",
      "Episode: 72 - Reward: 20.0\n",
      "Episode: 73 - Reward: 16.0\n",
      "Episode: 74 - Reward: 15.0\n",
      "Episode: 75 - Reward: 18.0\n",
      "Episode: 76 - Reward: 14.0\n",
      "Episode: 77 - Reward: 12.0\n",
      "Episode: 78 - Reward: 9.0\n",
      "Episode: 79 - Reward: 19.0\n",
      "Episode: 80 - Reward: 11.0\n",
      "Episode: 81 - Reward: 16.0\n",
      "Episode: 82 - Reward: 15.0\n",
      "Episode: 83 - Reward: 22.0\n",
      "Episode: 84 - Reward: 20.0\n",
      "Episode: 85 - Reward: 17.0\n",
      "Episode: 86 - Reward: 17.0\n",
      "Episode: 87 - Reward: 9.0\n",
      "Episode: 88 - Reward: 28.0\n",
      "Episode: 89 - Reward: 11.0\n",
      "Episode: 90 - Reward: 19.0\n",
      "Episode: 91 - Reward: 16.0\n",
      "Episode: 92 - Reward: 33.0\n",
      "Episode: 93 - Reward: 18.0\n",
      "Episode: 94 - Reward: 25.0\n",
      "Episode: 95 - Reward: 12.0\n",
      "Episode: 96 - Reward: 21.0\n",
      "Episode: 97 - Reward: 16.0\n",
      "Episode: 98 - Reward: 16.0\n",
      "Episode: 99 - Reward: 12.0\n",
      "Episode: 100 - Reward: 10.0\n",
      "Episode: 101 - Reward: 13.0\n",
      "Episode: 102 - Reward: 14.0\n",
      "Episode: 103 - Reward: 10.0\n",
      "Episode: 104 - Reward: 21.0\n",
      "Episode: 105 - Reward: 12.0\n",
      "Episode: 106 - Reward: 13.0\n",
      "Episode: 107 - Reward: 10.0\n",
      "Episode: 108 - Reward: 9.0\n",
      "Episode: 109 - Reward: 15.0\n",
      "Episode: 110 - Reward: 12.0\n",
      "Episode: 111 - Reward: 13.0\n",
      "Episode: 112 - Reward: 13.0\n",
      "Episode: 113 - Reward: 8.0\n",
      "Episode: 114 - Reward: 13.0\n",
      "Episode: 115 - Reward: 12.0\n",
      "Episode: 116 - Reward: 13.0\n",
      "Episode: 117 - Reward: 31.0\n",
      "Episode: 118 - Reward: 37.0\n",
      "Episode: 119 - Reward: 10.0\n",
      "Episode: 120 - Reward: 17.0\n",
      "Episode: 121 - Reward: 30.0\n",
      "Episode: 122 - Reward: 14.0\n",
      "Episode: 123 - Reward: 9.0\n",
      "Episode: 124 - Reward: 9.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-125.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-125.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-125.mp4\n",
      "Episode: 125 - Reward: 26.0\n",
      "Episode: 126 - Reward: 20.0\n",
      "Episode: 127 - Reward: 11.0\n",
      "Episode: 128 - Reward: 17.0\n",
      "Episode: 129 - Reward: 13.0\n",
      "Episode: 130 - Reward: 16.0\n",
      "Episode: 131 - Reward: 11.0\n",
      "Episode: 132 - Reward: 31.0\n",
      "Episode: 133 - Reward: 14.0\n",
      "Episode: 134 - Reward: 15.0\n",
      "Episode: 135 - Reward: 9.0\n",
      "Episode: 136 - Reward: 11.0\n",
      "Episode: 137 - Reward: 14.0\n",
      "Episode: 138 - Reward: 10.0\n",
      "Episode: 139 - Reward: 15.0\n",
      "Episode: 140 - Reward: 9.0\n",
      "Episode: 141 - Reward: 18.0\n",
      "Episode: 142 - Reward: 12.0\n",
      "Episode: 143 - Reward: 13.0\n",
      "Episode: 144 - Reward: 10.0\n",
      "Episode: 145 - Reward: 9.0\n",
      "Episode: 146 - Reward: 8.0\n",
      "Episode: 147 - Reward: 10.0\n",
      "Episode: 148 - Reward: 16.0\n",
      "Episode: 149 - Reward: 11.0\n",
      "Episode: 150 - Reward: 22.0\n",
      "Episode: 151 - Reward: 15.0\n",
      "Episode: 152 - Reward: 13.0\n",
      "Episode: 153 - Reward: 12.0\n",
      "Episode: 154 - Reward: 13.0\n",
      "Episode: 155 - Reward: 10.0\n",
      "Episode: 156 - Reward: 16.0\n",
      "Episode: 157 - Reward: 13.0\n",
      "Episode: 158 - Reward: 14.0\n",
      "Episode: 159 - Reward: 10.0\n",
      "Episode: 160 - Reward: 13.0\n",
      "Episode: 161 - Reward: 15.0\n",
      "Episode: 162 - Reward: 15.0\n",
      "Episode: 163 - Reward: 10.0\n",
      "Episode: 164 - Reward: 13.0\n",
      "Episode: 165 - Reward: 12.0\n",
      "Episode: 166 - Reward: 12.0\n",
      "Episode: 167 - Reward: 16.0\n",
      "Episode: 168 - Reward: 21.0\n",
      "Episode: 169 - Reward: 10.0\n",
      "Episode: 170 - Reward: 15.0\n",
      "Episode: 171 - Reward: 11.0\n",
      "Episode: 172 - Reward: 14.0\n",
      "Episode: 173 - Reward: 12.0\n",
      "Episode: 174 - Reward: 25.0\n",
      "Episode: 175 - Reward: 32.0\n",
      "Episode: 176 - Reward: 12.0\n",
      "Episode: 177 - Reward: 11.0\n",
      "Episode: 178 - Reward: 10.0\n",
      "Episode: 179 - Reward: 8.0\n",
      "Episode: 180 - Reward: 17.0\n",
      "Episode: 181 - Reward: 12.0\n",
      "Episode: 182 - Reward: 10.0\n",
      "Episode: 183 - Reward: 9.0\n",
      "Episode: 184 - Reward: 17.0\n",
      "Episode: 185 - Reward: 12.0\n",
      "Episode: 186 - Reward: 10.0\n",
      "Episode: 187 - Reward: 15.0\n",
      "Episode: 188 - Reward: 12.0\n",
      "Episode: 189 - Reward: 10.0\n",
      "Episode: 190 - Reward: 13.0\n",
      "Episode: 191 - Reward: 17.0\n",
      "Episode: 192 - Reward: 11.0\n",
      "Episode: 193 - Reward: 11.0\n",
      "Episode: 194 - Reward: 14.0\n",
      "Episode: 195 - Reward: 10.0\n",
      "Episode: 196 - Reward: 18.0\n",
      "Episode: 197 - Reward: 9.0\n",
      "Episode: 198 - Reward: 9.0\n",
      "Episode: 199 - Reward: 14.0\n",
      "Episode: 200 - Reward: 20.0\n",
      "Episode: 201 - Reward: 14.0\n",
      "Episode: 202 - Reward: 28.0\n",
      "Episode: 203 - Reward: 18.0\n",
      "Episode: 204 - Reward: 19.0\n",
      "Episode: 205 - Reward: 8.0\n",
      "Episode: 206 - Reward: 10.0\n",
      "Episode: 207 - Reward: 19.0\n",
      "Episode: 208 - Reward: 19.0\n",
      "Episode: 209 - Reward: 20.0\n",
      "Episode: 210 - Reward: 15.0\n",
      "Episode: 211 - Reward: 20.0\n",
      "Episode: 212 - Reward: 9.0\n",
      "Episode: 213 - Reward: 11.0\n",
      "Episode: 214 - Reward: 11.0\n",
      "Episode: 215 - Reward: 17.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-216.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-216.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-216.mp4\n",
      "Episode: 216 - Reward: 15.0\n",
      "Episode: 217 - Reward: 9.0\n",
      "Episode: 218 - Reward: 10.0\n",
      "Episode: 219 - Reward: 18.0\n",
      "Episode: 220 - Reward: 10.0\n",
      "Episode: 221 - Reward: 24.0\n",
      "Episode: 222 - Reward: 8.0\n",
      "Episode: 223 - Reward: 12.0\n",
      "Episode: 224 - Reward: 22.0\n",
      "Episode: 225 - Reward: 19.0\n",
      "Episode: 226 - Reward: 13.0\n",
      "Episode: 227 - Reward: 11.0\n",
      "Episode: 228 - Reward: 14.0\n",
      "Episode: 229 - Reward: 16.0\n",
      "Episode: 230 - Reward: 12.0\n",
      "Episode: 231 - Reward: 24.0\n",
      "Episode: 232 - Reward: 12.0\n",
      "Episode: 233 - Reward: 10.0\n",
      "Episode: 234 - Reward: 15.0\n",
      "Episode: 235 - Reward: 26.0\n",
      "Episode: 236 - Reward: 18.0\n",
      "Episode: 237 - Reward: 12.0\n",
      "Episode: 238 - Reward: 10.0\n",
      "Episode: 239 - Reward: 14.0\n",
      "Episode: 240 - Reward: 11.0\n",
      "Episode: 241 - Reward: 13.0\n",
      "Episode: 242 - Reward: 11.0\n",
      "Episode: 243 - Reward: 12.0\n",
      "Episode: 244 - Reward: 23.0\n",
      "Episode: 245 - Reward: 24.0\n",
      "Episode: 246 - Reward: 10.0\n",
      "Episode: 247 - Reward: 34.0\n",
      "Episode: 248 - Reward: 18.0\n",
      "Episode: 249 - Reward: 10.0\n",
      "Episode: 250 - Reward: 16.0\n",
      "Episode: 251 - Reward: 26.0\n",
      "Episode: 252 - Reward: 10.0\n",
      "Episode: 253 - Reward: 10.0\n",
      "Episode: 254 - Reward: 33.0\n",
      "Episode: 255 - Reward: 22.0\n",
      "Episode: 256 - Reward: 18.0\n",
      "Episode: 257 - Reward: 14.0\n",
      "Episode: 258 - Reward: 11.0\n",
      "Episode: 259 - Reward: 11.0\n",
      "Episode: 260 - Reward: 15.0\n",
      "Episode: 261 - Reward: 14.0\n",
      "Episode: 262 - Reward: 15.0\n",
      "Episode: 263 - Reward: 13.0\n",
      "Episode: 264 - Reward: 39.0\n",
      "Episode: 265 - Reward: 19.0\n",
      "Episode: 266 - Reward: 20.0\n",
      "Episode: 267 - Reward: 24.0\n",
      "Episode: 268 - Reward: 11.0\n",
      "Episode: 269 - Reward: 15.0\n",
      "Episode: 270 - Reward: 12.0\n",
      "Episode: 271 - Reward: 14.0\n",
      "Episode: 272 - Reward: 19.0\n",
      "Episode: 273 - Reward: 9.0\n",
      "Episode: 274 - Reward: 10.0\n",
      "Episode: 275 - Reward: 10.0\n",
      "Episode: 276 - Reward: 28.0\n",
      "Episode: 277 - Reward: 22.0\n",
      "Episode: 278 - Reward: 32.0\n",
      "Episode: 279 - Reward: 19.0\n",
      "Episode: 280 - Reward: 25.0\n",
      "Episode: 281 - Reward: 19.0\n",
      "Episode: 282 - Reward: 11.0\n",
      "Episode: 283 - Reward: 10.0\n",
      "Episode: 284 - Reward: 23.0\n",
      "Episode: 285 - Reward: 24.0\n",
      "Episode: 286 - Reward: 21.0\n",
      "Episode: 287 - Reward: 13.0\n",
      "Episode: 288 - Reward: 13.0\n",
      "Episode: 289 - Reward: 18.0\n",
      "Episode: 290 - Reward: 17.0\n",
      "Episode: 291 - Reward: 15.0\n",
      "Episode: 292 - Reward: 17.0\n",
      "Episode: 293 - Reward: 36.0\n",
      "Episode: 294 - Reward: 38.0\n",
      "Episode: 295 - Reward: 14.0\n",
      "Episode: 296 - Reward: 21.0\n",
      "Episode: 297 - Reward: 23.0\n",
      "Episode: 298 - Reward: 22.0\n",
      "Episode: 299 - Reward: 48.0\n",
      "Episode: 300 - Reward: 22.0\n",
      "Episode: 301 - Reward: 14.0\n",
      "Episode: 302 - Reward: 21.0\n",
      "Episode: 303 - Reward: 13.0\n",
      "Episode: 304 - Reward: 42.0\n",
      "Episode: 305 - Reward: 17.0\n",
      "Episode: 306 - Reward: 14.0\n",
      "Episode: 307 - Reward: 23.0\n",
      "Episode: 308 - Reward: 30.0\n",
      "Episode: 309 - Reward: 14.0\n",
      "Episode: 310 - Reward: 14.0\n",
      "Episode: 311 - Reward: 11.0\n",
      "Episode: 312 - Reward: 18.0\n",
      "Episode: 313 - Reward: 24.0\n",
      "Episode: 314 - Reward: 11.0\n",
      "Episode: 315 - Reward: 12.0\n",
      "Episode: 316 - Reward: 27.0\n",
      "Episode: 317 - Reward: 33.0\n",
      "Episode: 318 - Reward: 15.0\n",
      "Episode: 319 - Reward: 11.0\n",
      "Episode: 320 - Reward: 11.0\n",
      "Episode: 321 - Reward: 10.0\n",
      "Episode: 322 - Reward: 9.0\n",
      "Episode: 323 - Reward: 14.0\n",
      "Episode: 324 - Reward: 19.0\n",
      "Episode: 325 - Reward: 19.0\n",
      "Episode: 326 - Reward: 19.0\n",
      "Episode: 327 - Reward: 15.0\n",
      "Episode: 328 - Reward: 19.0\n",
      "Episode: 329 - Reward: 15.0\n",
      "Episode: 330 - Reward: 16.0\n",
      "Episode: 331 - Reward: 22.0\n",
      "Episode: 332 - Reward: 22.0\n",
      "Episode: 333 - Reward: 22.0\n",
      "Episode: 334 - Reward: 24.0\n",
      "Episode: 335 - Reward: 11.0\n",
      "Episode: 336 - Reward: 11.0\n",
      "Episode: 337 - Reward: 36.0\n",
      "Episode: 338 - Reward: 22.0\n",
      "Episode: 339 - Reward: 45.0\n",
      "Episode: 340 - Reward: 23.0\n",
      "Episode: 341 - Reward: 17.0\n",
      "Episode: 342 - Reward: 19.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-343.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-343.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-343.mp4\n",
      "Episode: 343 - Reward: 15.0\n",
      "Episode: 344 - Reward: 19.0\n",
      "Episode: 345 - Reward: 26.0\n",
      "Episode: 346 - Reward: 28.0\n",
      "Episode: 347 - Reward: 14.0\n",
      "Episode: 348 - Reward: 10.0\n",
      "Episode: 349 - Reward: 14.0\n",
      "Episode: 350 - Reward: 18.0\n",
      "Episode: 351 - Reward: 17.0\n",
      "Episode: 352 - Reward: 21.0\n",
      "Episode: 353 - Reward: 15.0\n",
      "Episode: 354 - Reward: 15.0\n",
      "Episode: 355 - Reward: 33.0\n",
      "Episode: 356 - Reward: 18.0\n",
      "Episode: 357 - Reward: 16.0\n",
      "Episode: 358 - Reward: 18.0\n",
      "Episode: 359 - Reward: 18.0\n",
      "Episode: 360 - Reward: 35.0\n",
      "Episode: 361 - Reward: 31.0\n",
      "Episode: 362 - Reward: 13.0\n",
      "Episode: 363 - Reward: 36.0\n",
      "Episode: 364 - Reward: 26.0\n",
      "Episode: 365 - Reward: 34.0\n",
      "Episode: 366 - Reward: 29.0\n",
      "Episode: 367 - Reward: 21.0\n",
      "Episode: 368 - Reward: 27.0\n",
      "Episode: 369 - Reward: 16.0\n",
      "Episode: 370 - Reward: 22.0\n",
      "Episode: 371 - Reward: 30.0\n",
      "Episode: 372 - Reward: 25.0\n",
      "Episode: 373 - Reward: 20.0\n",
      "Episode: 374 - Reward: 17.0\n",
      "Episode: 375 - Reward: 23.0\n",
      "Episode: 376 - Reward: 28.0\n",
      "Episode: 377 - Reward: 19.0\n",
      "Episode: 378 - Reward: 15.0\n",
      "Episode: 379 - Reward: 29.0\n",
      "Episode: 380 - Reward: 31.0\n",
      "Episode: 381 - Reward: 15.0\n",
      "Episode: 382 - Reward: 14.0\n",
      "Episode: 383 - Reward: 35.0\n",
      "Episode: 384 - Reward: 16.0\n",
      "Episode: 385 - Reward: 22.0\n",
      "Episode: 386 - Reward: 19.0\n",
      "Episode: 387 - Reward: 39.0\n",
      "Episode: 388 - Reward: 19.0\n",
      "Episode: 389 - Reward: 18.0\n",
      "Episode: 390 - Reward: 12.0\n",
      "Episode: 391 - Reward: 15.0\n",
      "Episode: 392 - Reward: 35.0\n",
      "Episode: 393 - Reward: 15.0\n",
      "Episode: 394 - Reward: 25.0\n",
      "Episode: 395 - Reward: 14.0\n",
      "Episode: 396 - Reward: 16.0\n",
      "Episode: 397 - Reward: 12.0\n",
      "Episode: 398 - Reward: 24.0\n",
      "Episode: 399 - Reward: 40.0\n",
      "Episode: 400 - Reward: 20.0\n",
      "Episode: 401 - Reward: 17.0\n",
      "Episode: 402 - Reward: 14.0\n",
      "Episode: 403 - Reward: 19.0\n",
      "Episode: 404 - Reward: 19.0\n",
      "Episode: 405 - Reward: 24.0\n",
      "Episode: 406 - Reward: 11.0\n",
      "Episode: 407 - Reward: 28.0\n",
      "Episode: 408 - Reward: 22.0\n",
      "Episode: 409 - Reward: 27.0\n",
      "Episode: 410 - Reward: 12.0\n",
      "Episode: 411 - Reward: 60.0\n",
      "Episode: 412 - Reward: 36.0\n",
      "Episode: 413 - Reward: 48.0\n",
      "Episode: 414 - Reward: 30.0\n",
      "Episode: 415 - Reward: 23.0\n",
      "Episode: 416 - Reward: 21.0\n",
      "Episode: 417 - Reward: 24.0\n",
      "Episode: 418 - Reward: 14.0\n",
      "Episode: 419 - Reward: 17.0\n",
      "Episode: 420 - Reward: 15.0\n",
      "Episode: 421 - Reward: 15.0\n",
      "Episode: 422 - Reward: 27.0\n",
      "Episode: 423 - Reward: 19.0\n",
      "Episode: 424 - Reward: 38.0\n",
      "Episode: 425 - Reward: 11.0\n",
      "Episode: 426 - Reward: 14.0\n",
      "Episode: 427 - Reward: 21.0\n",
      "Episode: 428 - Reward: 23.0\n",
      "Episode: 429 - Reward: 46.0\n",
      "Episode: 430 - Reward: 20.0\n",
      "Episode: 431 - Reward: 20.0\n",
      "Episode: 432 - Reward: 27.0\n",
      "Episode: 433 - Reward: 68.0\n",
      "Episode: 434 - Reward: 27.0\n",
      "Episode: 435 - Reward: 29.0\n",
      "Episode: 436 - Reward: 16.0\n",
      "Episode: 437 - Reward: 47.0\n",
      "Episode: 438 - Reward: 15.0\n",
      "Episode: 439 - Reward: 11.0\n",
      "Episode: 440 - Reward: 41.0\n",
      "Episode: 441 - Reward: 20.0\n",
      "Episode: 442 - Reward: 54.0\n",
      "Episode: 443 - Reward: 19.0\n",
      "Episode: 444 - Reward: 21.0\n",
      "Episode: 445 - Reward: 24.0\n",
      "Episode: 446 - Reward: 15.0\n",
      "Episode: 447 - Reward: 54.0\n",
      "Episode: 448 - Reward: 28.0\n",
      "Episode: 449 - Reward: 21.0\n",
      "Episode: 450 - Reward: 12.0\n",
      "Episode: 451 - Reward: 21.0\n",
      "Episode: 452 - Reward: 15.0\n",
      "Episode: 453 - Reward: 33.0\n",
      "Episode: 454 - Reward: 38.0\n",
      "Episode: 455 - Reward: 25.0\n",
      "Episode: 456 - Reward: 15.0\n",
      "Episode: 457 - Reward: 21.0\n",
      "Episode: 458 - Reward: 53.0\n",
      "Episode: 459 - Reward: 58.0\n",
      "Episode: 460 - Reward: 18.0\n",
      "Episode: 461 - Reward: 23.0\n",
      "Episode: 462 - Reward: 22.0\n",
      "Episode: 463 - Reward: 23.0\n",
      "Episode: 464 - Reward: 26.0\n",
      "Episode: 465 - Reward: 19.0\n",
      "Episode: 466 - Reward: 49.0\n",
      "Episode: 467 - Reward: 12.0\n",
      "Episode: 468 - Reward: 67.0\n",
      "Episode: 469 - Reward: 47.0\n",
      "Episode: 470 - Reward: 44.0\n",
      "Episode: 471 - Reward: 10.0\n",
      "Episode: 472 - Reward: 60.0\n",
      "Episode: 473 - Reward: 32.0\n",
      "Episode: 474 - Reward: 22.0\n",
      "Episode: 475 - Reward: 12.0\n",
      "Episode: 476 - Reward: 41.0\n",
      "Episode: 477 - Reward: 27.0\n",
      "Episode: 478 - Reward: 17.0\n",
      "Episode: 479 - Reward: 23.0\n",
      "Episode: 480 - Reward: 12.0\n",
      "Episode: 481 - Reward: 40.0\n",
      "Episode: 482 - Reward: 15.0\n",
      "Episode: 483 - Reward: 25.0\n",
      "Episode: 484 - Reward: 27.0\n",
      "Episode: 485 - Reward: 18.0\n",
      "Episode: 486 - Reward: 17.0\n",
      "Episode: 487 - Reward: 86.0\n",
      "Episode: 488 - Reward: 28.0\n",
      "Episode: 489 - Reward: 34.0\n",
      "Episode: 490 - Reward: 23.0\n",
      "Episode: 491 - Reward: 34.0\n",
      "Episode: 492 - Reward: 54.0\n",
      "Episode: 493 - Reward: 21.0\n",
      "Episode: 494 - Reward: 72.0\n",
      "Episode: 495 - Reward: 23.0\n",
      "Episode: 496 - Reward: 36.0\n",
      "Episode: 497 - Reward: 13.0\n",
      "Episode: 498 - Reward: 19.0\n",
      "Episode: 499 - Reward: 33.0\n",
      "Episode: 500 - Reward: 16.0\n",
      "Episode: 501 - Reward: 90.0\n",
      "Episode: 502 - Reward: 18.0\n",
      "Episode: 503 - Reward: 19.0\n",
      "Episode: 504 - Reward: 29.0\n",
      "Episode: 505 - Reward: 27.0\n",
      "Episode: 506 - Reward: 26.0\n",
      "Episode: 507 - Reward: 44.0\n",
      "Episode: 508 - Reward: 68.0\n",
      "Episode: 509 - Reward: 50.0\n",
      "Episode: 510 - Reward: 18.0\n",
      "Episode: 511 - Reward: 12.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-512.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-512.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-512.mp4\n",
      "Episode: 512 - Reward: 18.0\n",
      "Episode: 513 - Reward: 36.0\n",
      "Episode: 514 - Reward: 18.0\n",
      "Episode: 515 - Reward: 11.0\n",
      "Episode: 516 - Reward: 13.0\n",
      "Episode: 517 - Reward: 39.0\n",
      "Episode: 518 - Reward: 12.0\n",
      "Episode: 519 - Reward: 25.0\n",
      "Episode: 520 - Reward: 28.0\n",
      "Episode: 521 - Reward: 34.0\n",
      "Episode: 522 - Reward: 14.0\n",
      "Episode: 523 - Reward: 38.0\n",
      "Episode: 524 - Reward: 34.0\n",
      "Episode: 525 - Reward: 40.0\n",
      "Episode: 526 - Reward: 22.0\n",
      "Episode: 527 - Reward: 24.0\n",
      "Episode: 528 - Reward: 45.0\n",
      "Episode: 529 - Reward: 51.0\n",
      "Episode: 530 - Reward: 50.0\n",
      "Episode: 531 - Reward: 14.0\n",
      "Episode: 532 - Reward: 43.0\n",
      "Episode: 533 - Reward: 23.0\n",
      "Episode: 534 - Reward: 58.0\n",
      "Episode: 535 - Reward: 19.0\n",
      "Episode: 536 - Reward: 13.0\n",
      "Episode: 537 - Reward: 34.0\n",
      "Episode: 538 - Reward: 25.0\n",
      "Episode: 539 - Reward: 22.0\n",
      "Episode: 540 - Reward: 25.0\n",
      "Episode: 541 - Reward: 54.0\n",
      "Episode: 542 - Reward: 111.0\n",
      "Episode: 543 - Reward: 71.0\n",
      "Episode: 544 - Reward: 38.0\n",
      "Episode: 545 - Reward: 56.0\n",
      "Episode: 546 - Reward: 12.0\n",
      "Episode: 547 - Reward: 14.0\n",
      "Episode: 548 - Reward: 18.0\n",
      "Episode: 549 - Reward: 33.0\n",
      "Episode: 550 - Reward: 70.0\n",
      "Episode: 551 - Reward: 48.0\n",
      "Episode: 552 - Reward: 26.0\n",
      "Episode: 553 - Reward: 20.0\n",
      "Episode: 554 - Reward: 38.0\n",
      "Episode: 555 - Reward: 28.0\n",
      "Episode: 556 - Reward: 46.0\n",
      "Episode: 557 - Reward: 61.0\n",
      "Episode: 558 - Reward: 23.0\n",
      "Episode: 559 - Reward: 35.0\n",
      "Episode: 560 - Reward: 86.0\n",
      "Episode: 561 - Reward: 13.0\n",
      "Episode: 562 - Reward: 18.0\n",
      "Episode: 563 - Reward: 20.0\n",
      "Episode: 564 - Reward: 117.0\n",
      "Episode: 565 - Reward: 75.0\n",
      "Episode: 566 - Reward: 57.0\n",
      "Episode: 567 - Reward: 28.0\n",
      "Episode: 568 - Reward: 38.0\n",
      "Episode: 569 - Reward: 13.0\n",
      "Episode: 570 - Reward: 40.0\n",
      "Episode: 571 - Reward: 18.0\n",
      "Episode: 572 - Reward: 39.0\n",
      "Episode: 573 - Reward: 18.0\n",
      "Episode: 574 - Reward: 16.0\n",
      "Episode: 575 - Reward: 13.0\n",
      "Episode: 576 - Reward: 23.0\n",
      "Episode: 577 - Reward: 48.0\n",
      "Episode: 578 - Reward: 78.0\n",
      "Episode: 579 - Reward: 28.0\n",
      "Episode: 580 - Reward: 53.0\n",
      "Episode: 581 - Reward: 41.0\n",
      "Episode: 582 - Reward: 26.0\n",
      "Episode: 583 - Reward: 12.0\n",
      "Episode: 584 - Reward: 61.0\n",
      "Episode: 585 - Reward: 49.0\n",
      "Episode: 586 - Reward: 34.0\n",
      "Episode: 587 - Reward: 37.0\n",
      "Episode: 588 - Reward: 60.0\n",
      "Episode: 589 - Reward: 40.0\n",
      "Episode: 590 - Reward: 38.0\n",
      "Episode: 591 - Reward: 55.0\n",
      "Episode: 592 - Reward: 26.0\n",
      "Episode: 593 - Reward: 42.0\n",
      "Episode: 594 - Reward: 23.0\n",
      "Episode: 595 - Reward: 17.0\n",
      "Episode: 596 - Reward: 36.0\n",
      "Episode: 597 - Reward: 31.0\n",
      "Episode: 598 - Reward: 21.0\n",
      "Episode: 599 - Reward: 63.0\n",
      "Episode: 600 - Reward: 34.0\n",
      "Episode: 601 - Reward: 43.0\n",
      "Episode: 602 - Reward: 38.0\n",
      "Episode: 603 - Reward: 46.0\n",
      "Episode: 604 - Reward: 28.0\n",
      "Episode: 605 - Reward: 99.0\n",
      "Episode: 606 - Reward: 46.0\n",
      "Episode: 607 - Reward: 44.0\n",
      "Episode: 608 - Reward: 41.0\n",
      "Episode: 609 - Reward: 30.0\n",
      "Episode: 610 - Reward: 51.0\n",
      "Episode: 611 - Reward: 16.0\n",
      "Episode: 612 - Reward: 43.0\n",
      "Episode: 613 - Reward: 32.0\n",
      "Episode: 614 - Reward: 26.0\n",
      "Episode: 615 - Reward: 82.0\n",
      "Episode: 616 - Reward: 19.0\n",
      "Episode: 617 - Reward: 13.0\n",
      "Episode: 618 - Reward: 48.0\n",
      "Episode: 619 - Reward: 41.0\n",
      "Episode: 620 - Reward: 74.0\n",
      "Episode: 621 - Reward: 101.0\n",
      "Episode: 622 - Reward: 50.0\n",
      "Episode: 623 - Reward: 85.0\n",
      "Episode: 624 - Reward: 67.0\n",
      "Episode: 625 - Reward: 25.0\n",
      "Episode: 626 - Reward: 24.0\n",
      "Episode: 627 - Reward: 30.0\n",
      "Episode: 628 - Reward: 25.0\n",
      "Episode: 629 - Reward: 27.0\n",
      "Episode: 630 - Reward: 68.0\n",
      "Episode: 631 - Reward: 14.0\n",
      "Episode: 632 - Reward: 67.0\n",
      "Episode: 633 - Reward: 57.0\n",
      "Episode: 634 - Reward: 17.0\n",
      "Episode: 635 - Reward: 51.0\n",
      "Episode: 636 - Reward: 58.0\n",
      "Episode: 637 - Reward: 22.0\n",
      "Episode: 638 - Reward: 21.0\n",
      "Episode: 639 - Reward: 31.0\n",
      "Episode: 640 - Reward: 30.0\n",
      "Episode: 641 - Reward: 27.0\n",
      "Episode: 642 - Reward: 21.0\n",
      "Episode: 643 - Reward: 82.0\n",
      "Episode: 644 - Reward: 66.0\n",
      "Episode: 645 - Reward: 42.0\n",
      "Episode: 646 - Reward: 45.0\n",
      "Episode: 647 - Reward: 43.0\n",
      "Episode: 648 - Reward: 43.0\n",
      "Episode: 649 - Reward: 39.0\n",
      "Episode: 650 - Reward: 30.0\n",
      "Episode: 651 - Reward: 47.0\n",
      "Episode: 652 - Reward: 29.0\n",
      "Episode: 653 - Reward: 41.0\n",
      "Episode: 654 - Reward: 48.0\n",
      "Episode: 655 - Reward: 25.0\n",
      "Episode: 656 - Reward: 19.0\n",
      "Episode: 657 - Reward: 32.0\n",
      "Episode: 658 - Reward: 39.0\n",
      "Episode: 659 - Reward: 95.0\n",
      "Episode: 660 - Reward: 60.0\n",
      "Episode: 661 - Reward: 80.0\n",
      "Episode: 662 - Reward: 30.0\n",
      "Episode: 663 - Reward: 36.0\n",
      "Episode: 664 - Reward: 28.0\n",
      "Episode: 665 - Reward: 25.0\n",
      "Episode: 666 - Reward: 55.0\n",
      "Episode: 667 - Reward: 70.0\n",
      "Episode: 668 - Reward: 50.0\n",
      "Episode: 669 - Reward: 34.0\n",
      "Episode: 670 - Reward: 29.0\n",
      "Episode: 671 - Reward: 25.0\n",
      "Episode: 672 - Reward: 45.0\n",
      "Episode: 673 - Reward: 36.0\n",
      "Episode: 674 - Reward: 49.0\n",
      "Episode: 675 - Reward: 44.0\n",
      "Episode: 676 - Reward: 54.0\n",
      "Episode: 677 - Reward: 16.0\n",
      "Episode: 678 - Reward: 64.0\n",
      "Episode: 679 - Reward: 26.0\n",
      "Episode: 680 - Reward: 49.0\n",
      "Episode: 681 - Reward: 17.0\n",
      "Episode: 682 - Reward: 99.0\n",
      "Episode: 683 - Reward: 18.0\n",
      "Episode: 684 - Reward: 75.0\n",
      "Episode: 685 - Reward: 62.0\n",
      "Episode: 686 - Reward: 40.0\n",
      "Episode: 687 - Reward: 52.0\n",
      "Episode: 688 - Reward: 28.0\n",
      "Episode: 689 - Reward: 15.0\n",
      "Episode: 690 - Reward: 20.0\n",
      "Episode: 691 - Reward: 49.0\n",
      "Episode: 692 - Reward: 36.0\n",
      "Episode: 693 - Reward: 21.0\n",
      "Episode: 694 - Reward: 86.0\n",
      "Episode: 695 - Reward: 12.0\n",
      "Episode: 696 - Reward: 46.0\n",
      "Episode: 697 - Reward: 35.0\n",
      "Episode: 698 - Reward: 48.0\n",
      "Episode: 699 - Reward: 33.0\n",
      "Episode: 700 - Reward: 53.0\n",
      "Episode: 701 - Reward: 82.0\n",
      "Episode: 702 - Reward: 50.0\n",
      "Episode: 703 - Reward: 36.0\n",
      "Episode: 704 - Reward: 65.0\n",
      "Episode: 705 - Reward: 49.0\n",
      "Episode: 706 - Reward: 46.0\n",
      "Episode: 707 - Reward: 25.0\n",
      "Episode: 708 - Reward: 19.0\n",
      "Episode: 709 - Reward: 19.0\n",
      "Episode: 710 - Reward: 21.0\n",
      "Episode: 711 - Reward: 19.0\n",
      "Episode: 712 - Reward: 33.0\n",
      "Episode: 713 - Reward: 111.0\n",
      "Episode: 714 - Reward: 37.0\n",
      "Episode: 715 - Reward: 35.0\n",
      "Episode: 716 - Reward: 96.0\n",
      "Episode: 717 - Reward: 41.0\n",
      "Episode: 718 - Reward: 21.0\n",
      "Episode: 719 - Reward: 58.0\n",
      "Episode: 720 - Reward: 15.0\n",
      "Episode: 721 - Reward: 58.0\n",
      "Episode: 722 - Reward: 25.0\n",
      "Episode: 723 - Reward: 45.0\n",
      "Episode: 724 - Reward: 59.0\n",
      "Episode: 725 - Reward: 46.0\n",
      "Episode: 726 - Reward: 30.0\n",
      "Episode: 727 - Reward: 54.0\n",
      "Episode: 728 - Reward: 37.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-729.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-729.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-729.mp4\n",
      "Episode: 729 - Reward: 23.0\n",
      "Episode: 730 - Reward: 104.0\n",
      "Episode: 731 - Reward: 21.0\n",
      "Episode: 732 - Reward: 28.0\n",
      "Episode: 733 - Reward: 100.0\n",
      "Episode: 734 - Reward: 46.0\n",
      "Episode: 735 - Reward: 33.0\n",
      "Episode: 736 - Reward: 45.0\n",
      "Episode: 737 - Reward: 37.0\n",
      "Episode: 738 - Reward: 44.0\n",
      "Episode: 739 - Reward: 50.0\n",
      "Episode: 740 - Reward: 42.0\n",
      "Episode: 741 - Reward: 16.0\n",
      "Episode: 742 - Reward: 26.0\n",
      "Episode: 743 - Reward: 47.0\n",
      "Episode: 744 - Reward: 56.0\n",
      "Episode: 745 - Reward: 35.0\n",
      "Episode: 746 - Reward: 33.0\n",
      "Episode: 747 - Reward: 38.0\n",
      "Episode: 748 - Reward: 68.0\n",
      "Episode: 749 - Reward: 84.0\n",
      "Episode: 750 - Reward: 27.0\n",
      "Episode: 751 - Reward: 49.0\n",
      "Episode: 752 - Reward: 25.0\n",
      "Episode: 753 - Reward: 51.0\n",
      "Episode: 754 - Reward: 49.0\n",
      "Episode: 755 - Reward: 43.0\n",
      "Episode: 756 - Reward: 26.0\n",
      "Episode: 757 - Reward: 29.0\n",
      "Episode: 758 - Reward: 17.0\n",
      "Episode: 759 - Reward: 39.0\n",
      "Episode: 760 - Reward: 89.0\n",
      "Episode: 761 - Reward: 34.0\n",
      "Episode: 762 - Reward: 42.0\n",
      "Episode: 763 - Reward: 47.0\n",
      "Episode: 764 - Reward: 45.0\n",
      "Episode: 765 - Reward: 38.0\n",
      "Episode: 766 - Reward: 29.0\n",
      "Episode: 767 - Reward: 55.0\n",
      "Episode: 768 - Reward: 18.0\n",
      "Episode: 769 - Reward: 31.0\n",
      "Episode: 770 - Reward: 22.0\n",
      "Episode: 771 - Reward: 79.0\n",
      "Episode: 772 - Reward: 33.0\n",
      "Episode: 773 - Reward: 51.0\n",
      "Episode: 774 - Reward: 46.0\n",
      "Episode: 775 - Reward: 33.0\n",
      "Episode: 776 - Reward: 55.0\n",
      "Episode: 777 - Reward: 24.0\n",
      "Episode: 778 - Reward: 88.0\n",
      "Episode: 779 - Reward: 29.0\n",
      "Episode: 780 - Reward: 22.0\n",
      "Episode: 781 - Reward: 41.0\n",
      "Episode: 782 - Reward: 44.0\n",
      "Episode: 783 - Reward: 33.0\n",
      "Episode: 784 - Reward: 24.0\n",
      "Episode: 785 - Reward: 50.0\n",
      "Episode: 786 - Reward: 66.0\n",
      "Episode: 787 - Reward: 41.0\n",
      "Episode: 788 - Reward: 27.0\n",
      "Episode: 789 - Reward: 68.0\n",
      "Episode: 790 - Reward: 31.0\n",
      "Episode: 791 - Reward: 47.0\n",
      "Episode: 792 - Reward: 46.0\n",
      "Episode: 793 - Reward: 56.0\n",
      "Episode: 794 - Reward: 46.0\n",
      "Episode: 795 - Reward: 72.0\n",
      "Episode: 796 - Reward: 26.0\n",
      "Episode: 797 - Reward: 43.0\n",
      "Episode: 798 - Reward: 31.0\n",
      "Episode: 799 - Reward: 82.0\n",
      "Episode: 800 - Reward: 170.0\n",
      "Episode: 801 - Reward: 21.0\n",
      "Episode: 802 - Reward: 57.0\n",
      "Episode: 803 - Reward: 35.0\n",
      "Episode: 804 - Reward: 49.0\n",
      "Episode: 805 - Reward: 28.0\n",
      "Episode: 806 - Reward: 41.0\n",
      "Episode: 807 - Reward: 44.0\n",
      "Episode: 808 - Reward: 29.0\n",
      "Episode: 809 - Reward: 67.0\n",
      "Episode: 810 - Reward: 46.0\n",
      "Episode: 811 - Reward: 57.0\n",
      "Episode: 812 - Reward: 55.0\n",
      "Episode: 813 - Reward: 43.0\n",
      "Episode: 814 - Reward: 25.0\n",
      "Episode: 815 - Reward: 48.0\n",
      "Episode: 816 - Reward: 89.0\n",
      "Episode: 817 - Reward: 28.0\n",
      "Episode: 818 - Reward: 23.0\n",
      "Episode: 819 - Reward: 28.0\n",
      "Episode: 820 - Reward: 39.0\n",
      "Episode: 821 - Reward: 23.0\n",
      "Episode: 822 - Reward: 78.0\n",
      "Episode: 823 - Reward: 39.0\n",
      "Episode: 824 - Reward: 50.0\n",
      "Episode: 825 - Reward: 27.0\n",
      "Episode: 826 - Reward: 45.0\n",
      "Episode: 827 - Reward: 44.0\n",
      "Episode: 828 - Reward: 33.0\n",
      "Episode: 829 - Reward: 40.0\n",
      "Episode: 830 - Reward: 35.0\n",
      "Episode: 831 - Reward: 64.0\n",
      "Episode: 832 - Reward: 16.0\n",
      "Episode: 833 - Reward: 46.0\n",
      "Episode: 834 - Reward: 12.0\n",
      "Episode: 835 - Reward: 47.0\n",
      "Episode: 836 - Reward: 26.0\n",
      "Episode: 837 - Reward: 48.0\n",
      "Episode: 838 - Reward: 39.0\n",
      "Episode: 839 - Reward: 45.0\n",
      "Episode: 840 - Reward: 15.0\n",
      "Episode: 841 - Reward: 47.0\n",
      "Episode: 842 - Reward: 29.0\n",
      "Episode: 843 - Reward: 64.0\n",
      "Episode: 844 - Reward: 44.0\n",
      "Episode: 845 - Reward: 20.0\n",
      "Episode: 846 - Reward: 35.0\n",
      "Episode: 847 - Reward: 54.0\n",
      "Episode: 848 - Reward: 67.0\n",
      "Episode: 849 - Reward: 43.0\n",
      "Episode: 850 - Reward: 29.0\n",
      "Episode: 851 - Reward: 22.0\n",
      "Episode: 852 - Reward: 24.0\n",
      "Episode: 853 - Reward: 67.0\n",
      "Episode: 854 - Reward: 38.0\n",
      "Episode: 855 - Reward: 56.0\n",
      "Episode: 856 - Reward: 49.0\n",
      "Episode: 857 - Reward: 66.0\n",
      "Episode: 858 - Reward: 37.0\n",
      "Episode: 859 - Reward: 51.0\n",
      "Episode: 860 - Reward: 86.0\n",
      "Episode: 861 - Reward: 44.0\n",
      "Episode: 862 - Reward: 36.0\n",
      "Episode: 863 - Reward: 94.0\n",
      "Episode: 864 - Reward: 36.0\n",
      "Episode: 865 - Reward: 24.0\n",
      "Episode: 866 - Reward: 72.0\n",
      "Episode: 867 - Reward: 31.0\n",
      "Episode: 868 - Reward: 21.0\n",
      "Episode: 869 - Reward: 41.0\n",
      "Episode: 870 - Reward: 31.0\n",
      "Episode: 871 - Reward: 39.0\n",
      "Episode: 872 - Reward: 50.0\n",
      "Episode: 873 - Reward: 43.0\n",
      "Episode: 874 - Reward: 35.0\n",
      "Episode: 875 - Reward: 25.0\n",
      "Episode: 876 - Reward: 43.0\n",
      "Episode: 877 - Reward: 73.0\n",
      "Episode: 878 - Reward: 45.0\n",
      "Episode: 879 - Reward: 47.0\n",
      "Episode: 880 - Reward: 81.0\n",
      "Episode: 881 - Reward: 47.0\n",
      "Episode: 882 - Reward: 54.0\n",
      "Episode: 883 - Reward: 38.0\n",
      "Episode: 884 - Reward: 57.0\n",
      "Episode: 885 - Reward: 30.0\n",
      "Episode: 886 - Reward: 60.0\n",
      "Episode: 887 - Reward: 41.0\n",
      "Episode: 888 - Reward: 25.0\n",
      "Episode: 889 - Reward: 56.0\n",
      "Episode: 890 - Reward: 70.0\n",
      "Episode: 891 - Reward: 26.0\n",
      "Episode: 892 - Reward: 44.0\n",
      "Episode: 893 - Reward: 33.0\n",
      "Episode: 894 - Reward: 77.0\n",
      "Episode: 895 - Reward: 32.0\n",
      "Episode: 896 - Reward: 39.0\n",
      "Episode: 897 - Reward: 59.0\n",
      "Episode: 898 - Reward: 68.0\n",
      "Episode: 899 - Reward: 26.0\n",
      "Episode: 900 - Reward: 24.0\n",
      "Episode: 901 - Reward: 32.0\n",
      "Episode: 902 - Reward: 39.0\n",
      "Episode: 903 - Reward: 35.0\n",
      "Episode: 904 - Reward: 41.0\n",
      "Episode: 905 - Reward: 77.0\n",
      "Episode: 906 - Reward: 39.0\n",
      "Episode: 907 - Reward: 41.0\n",
      "Episode: 908 - Reward: 127.0\n",
      "Episode: 909 - Reward: 42.0\n",
      "Episode: 910 - Reward: 86.0\n",
      "Episode: 911 - Reward: 70.0\n",
      "Episode: 912 - Reward: 30.0\n",
      "Episode: 913 - Reward: 31.0\n",
      "Episode: 914 - Reward: 27.0\n",
      "Episode: 915 - Reward: 62.0\n",
      "Episode: 916 - Reward: 77.0\n",
      "Episode: 917 - Reward: 58.0\n",
      "Episode: 918 - Reward: 108.0\n",
      "Episode: 919 - Reward: 30.0\n",
      "Episode: 920 - Reward: 20.0\n",
      "Episode: 921 - Reward: 40.0\n",
      "Episode: 922 - Reward: 86.0\n",
      "Episode: 923 - Reward: 72.0\n",
      "Episode: 924 - Reward: 37.0\n",
      "Episode: 925 - Reward: 34.0\n",
      "Episode: 926 - Reward: 25.0\n",
      "Episode: 927 - Reward: 79.0\n",
      "Episode: 928 - Reward: 61.0\n",
      "Episode: 929 - Reward: 35.0\n",
      "Episode: 930 - Reward: 49.0\n",
      "Episode: 931 - Reward: 83.0\n",
      "Episode: 932 - Reward: 34.0\n",
      "Episode: 933 - Reward: 39.0\n",
      "Episode: 934 - Reward: 20.0\n",
      "Episode: 935 - Reward: 28.0\n",
      "Episode: 936 - Reward: 75.0\n",
      "Episode: 937 - Reward: 33.0\n",
      "Episode: 938 - Reward: 45.0\n",
      "Episode: 939 - Reward: 52.0\n",
      "Episode: 940 - Reward: 63.0\n",
      "Episode: 941 - Reward: 35.0\n",
      "Episode: 942 - Reward: 159.0\n",
      "Episode: 943 - Reward: 47.0\n",
      "Episode: 944 - Reward: 40.0\n",
      "Episode: 945 - Reward: 54.0\n",
      "Episode: 946 - Reward: 50.0\n",
      "Episode: 947 - Reward: 29.0\n",
      "Episode: 948 - Reward: 50.0\n",
      "Episode: 949 - Reward: 31.0\n",
      "Episode: 950 - Reward: 52.0\n",
      "Episode: 951 - Reward: 29.0\n",
      "Episode: 952 - Reward: 47.0\n",
      "Episode: 953 - Reward: 35.0\n",
      "Episode: 954 - Reward: 62.0\n",
      "Episode: 955 - Reward: 33.0\n",
      "Episode: 956 - Reward: 38.0\n",
      "Episode: 957 - Reward: 80.0\n",
      "Episode: 958 - Reward: 47.0\n",
      "Episode: 959 - Reward: 41.0\n",
      "Episode: 960 - Reward: 28.0\n",
      "Episode: 961 - Reward: 77.0\n",
      "Episode: 962 - Reward: 25.0\n",
      "Episode: 963 - Reward: 35.0\n",
      "Episode: 964 - Reward: 95.0\n",
      "Episode: 965 - Reward: 80.0\n",
      "Episode: 966 - Reward: 25.0\n",
      "Episode: 967 - Reward: 37.0\n",
      "Episode: 968 - Reward: 37.0\n",
      "Episode: 969 - Reward: 62.0\n",
      "Episode: 970 - Reward: 58.0\n",
      "Episode: 971 - Reward: 35.0\n",
      "Episode: 972 - Reward: 39.0\n",
      "Episode: 973 - Reward: 34.0\n",
      "Episode: 974 - Reward: 71.0\n",
      "Episode: 975 - Reward: 63.0\n",
      "Episode: 976 - Reward: 65.0\n",
      "Episode: 977 - Reward: 60.0\n",
      "Episode: 978 - Reward: 41.0\n",
      "Episode: 979 - Reward: 72.0\n",
      "Episode: 980 - Reward: 100.0\n",
      "Episode: 981 - Reward: 23.0\n",
      "Episode: 982 - Reward: 31.0\n",
      "Episode: 983 - Reward: 70.0\n",
      "Episode: 984 - Reward: 18.0\n",
      "Episode: 985 - Reward: 42.0\n",
      "Episode: 986 - Reward: 47.0\n",
      "Episode: 987 - Reward: 60.0\n",
      "Episode: 988 - Reward: 63.0\n",
      "Episode: 989 - Reward: 43.0\n",
      "Episode: 990 - Reward: 36.0\n",
      "Episode: 991 - Reward: 54.0\n",
      "Episode: 992 - Reward: 20.0\n",
      "Episode: 993 - Reward: 98.0\n",
      "Episode: 994 - Reward: 41.0\n",
      "Episode: 995 - Reward: 58.0\n",
      "Episode: 996 - Reward: 45.0\n",
      "Episode: 997 - Reward: 43.0\n",
      "Episode: 998 - Reward: 49.0\n",
      "Episode: 999 - Reward: 29.0\n",
      "Moviepy - Building video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1000.mp4.\n",
      "Moviepy - Writing video /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/jackmontgomery/Desktop/UCT/Research_Project/honours-project/practice/CartPole-v1/_rX/rl-video-episode-1000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gym\n",
    "\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity, obs_shape, device='cpu'):\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.capacity = capacity # The maximum number of items to be stored in memory\n",
    "        \n",
    "        # Initialize (empty) memory tensors\n",
    "        self.obs_mem    = torch.empty([capacity]+[dim for dim in obs_shape], dtype=torch.float32, device=self.device)\n",
    "        self.action_mem = torch.empty(capacity, dtype=torch.int64, device=self.device)\n",
    "        self.reward_mem = torch.empty(capacity, dtype=torch.int8, device=self.device)\n",
    "        self.done_mem   = torch.empty(capacity, dtype=torch.int8, device=self.device)\n",
    "        \n",
    "        self.push_count = 0 # The number of times new data has been pushed to memory\n",
    "        \n",
    "    def push(self, obs, action, reward, done):\n",
    "        \n",
    "        # Store data to memory\n",
    "        self.obs_mem[self.position()] = obs \n",
    "        self.action_mem[self.position()] = action\n",
    "        self.reward_mem[self.position()] = reward\n",
    "        self.done_mem[self.position()] = done\n",
    "        \n",
    "        self.push_count += 1\n",
    "    \n",
    "    def position(self):\n",
    "        # Returns the next position (index) to which data is pushed\n",
    "        return self.push_count % self.capacity\n",
    "    \n",
    "    \n",
    "    def sample(self, obs_indices, action_indices, reward_indices, done_indices, max_n_indices, batch_size):\n",
    "        # Fine as long as max_n is not greater than the fewest number of time steps an episode can take\n",
    "        \n",
    "        # Pick indices at random\n",
    "        end_indices = np.random.choice(min(self.push_count, self.capacity)-max_n_indices*2, batch_size, replace=False) + max_n_indices\n",
    "        \n",
    "        # Correct for sampling near the position where data was last pushed\n",
    "        for i in range(len(end_indices)):\n",
    "            if end_indices[i] in range(self.position(), self.position()+max_n_indices):\n",
    "                end_indices[i] += max_n_indices\n",
    "        \n",
    "        # Retrieve the specified indices that come before the end_indices\n",
    "        obs_batch = self.obs_mem[np.array([index-obs_indices for index in end_indices])]\n",
    "        action_batch = self.action_mem[np.array([index-action_indices for index in end_indices])]\n",
    "        reward_batch = self.reward_mem[np.array([index-reward_indices for index in end_indices])]\n",
    "        done_batch = self.done_mem[np.array([index-done_indices for index in end_indices])]\n",
    "        \n",
    "        # Correct for sampling over multiple episodes\n",
    "        for i in range(len(end_indices)):\n",
    "            index = end_indices[i]\n",
    "            for j in range(1, max_n_indices):\n",
    "                if self.done_mem[index-j]:\n",
    "                    for k in range(len(obs_indices)):\n",
    "                        if obs_indices[k] >= j:\n",
    "                            obs_batch[i, k] = torch.zeros_like(self.obs_mem[0]) \n",
    "                    for k in range(len(action_indices)):\n",
    "                        if action_indices[k] >= j:\n",
    "                            action_batch[i, k] = torch.zeros_like(self.action_mem[0]) # Assigning action '0' might not be the best solution, perhaps as assigning at random, or adding an action for this specific case would be better\n",
    "                    for k in range(len(reward_indices)):\n",
    "                        if reward_indices[k] >= j:\n",
    "                            reward_batch[i, k] = torch.zeros_like(self.reward_mem[0]) # Reward of 0 will probably not make sense for every environment\n",
    "                    for k in range(len(done_indices)):\n",
    "                        if done_indices[k] >= j:\n",
    "                            done_batch[i, k] = torch.zeros_like(self.done_mem[0]) \n",
    "                    break\n",
    "                \n",
    "        return obs_batch, action_batch, reward_batch, done_batch\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden=64, lr=1e-3, softmax=False, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs # Number of inputs\n",
    "        self.n_hidden = n_hidden # Number of hidden units\n",
    "        self.n_outputs = n_outputs # Number of outputs\n",
    "        self.softmax = softmax # If true apply a softmax function to the output\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, self.n_hidden) # Hidden layer\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_outputs) # Output layer\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr) # Adam optimizer\n",
    "        \n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass:\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        \n",
    "        if self.softmax: # If true apply a softmax function to the output\n",
    "            return F.softmax(self.fc2(h_relu), dim=-1).clamp(min=1e-9, max=1-1e-9)\n",
    "        else:\n",
    "            return self.fc2(h_relu)\n",
    "    \n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.set_parameters() # Set parameters\n",
    "        \n",
    "        self.obs_shape = self.env.observation_space.shape # The shape of observations\n",
    "        self.obs_size = np.prod(self.obs_shape) # The size of the observation\n",
    "        self.n_actions = self.env.action_space.n # The number of actions available to the agent\n",
    "        \n",
    "        self.freeze_cntr = 0 # Keeps track of when to (un)freeze the target network\n",
    "        \n",
    "        # Initialize the networks:\n",
    "        self.transition_net = Model(self.obs_size+1, self.obs_size, self.n_hidden_trans, lr=self.lr_trans, device=self.device)\n",
    "        self.policy_net = Model(self.obs_size, self.n_actions, self.n_hidden_pol, lr=self.lr_pol, softmax=True, device=self.device)\n",
    "        self.value_net = Model(self.obs_size, self.n_actions, self.n_hidden_val, lr=self.lr_val, device=self.device)\n",
    "        \n",
    "        if self.load_network: # If true: load the networks given paths\n",
    "            self.transition_net.load_state_dict(torch.load(self.network_load_path.format(\"trans\")))\n",
    "            self.transition_net.eval()\n",
    "            self.policy_net.load_state_dict(torch.load(self.network_load_path.format(\"pol\")))\n",
    "            self.policy_net.eval()\n",
    "            self.value_net.load_state_dict(torch.load(self.network_load_path.format(\"val\")))\n",
    "            self.value_net.eval()\n",
    "        self.target_net = Model(self.obs_size, self.n_actions, self.n_hidden_val, lr=self.lr_val, device=self.device)\n",
    "        self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "        \n",
    "        # Initialize the replay memory\n",
    "        self.memory = ReplayMemory(self.memory_capacity, self.obs_shape, device=self.device)\n",
    "        \n",
    "        # When sampling from memory at index i, obs_indices indicates that we want observations with indices i-obs_indices, works the same for the others\n",
    "        self.obs_indices = [2, 1, 0]\n",
    "        self.action_indices = [2, 1]\n",
    "        self.reward_indices = [1]\n",
    "        self.done_indices = [0]\n",
    "        self.max_n_indices = max(max(self.obs_indices, self.action_indices, self.reward_indices, self.done_indices)) + 1\n",
    "\n",
    "    def set_parameters(self):\n",
    "        \n",
    "        # The default parameters\n",
    "        default_parameters = {\n",
    "            'run_id':\"_rX\", \n",
    "            'device':'cpu',\n",
    "            'env':'CartPole-v1', \n",
    "            'n_episodes':1000, \n",
    "            'n_hidden_trans':64, \n",
    "            'lr_trans':1e-3, \n",
    "            'n_hidden_pol':64, \n",
    "            'lr_pol':1e-3, \n",
    "            'n_hidden_val':64, \n",
    "            'lr_val':1e-4,\n",
    "            'memory_capacity':65536, \n",
    "            'batch_size':64,\n",
    "            'freeze_period':25,\n",
    "            'Beta':0.99,\n",
    "            'gamma':1.00, \n",
    "            'print_timer':100,\n",
    "            'log_save_timer':10,\n",
    "            'save_results':True,\n",
    "            'results_path':\"results/ai_mdp_results{}.npz\",\n",
    "            'results_save_timer':500,\n",
    "            'save_network':True, \n",
    "            'network_save_path':\"networks/ai_mdp_{}net{}.pth\",\n",
    "            'network_save_timer':500,\n",
    "            'load_network':False,\n",
    "            'network_load_path':\"networks/ai_mdp_{}net_rX.pth\",\n",
    "            'record_video': False,\n",
    "            'record_statistics': True\n",
    "            }\n",
    "        \n",
    "        # Possible command:\n",
    "            # python ai_mdp_agent.py device=cuda:0\n",
    "        \n",
    "        # Adjust the custom parameters according to the arguments in\n",
    "        custom_parameters = default_parameters.copy()\n",
    "        \n",
    "        # Set all parameters\n",
    "        self.run_id = custom_parameters['run_id'] # Is appended to paths to distinguish between runs\n",
    "        self.device = custom_parameters['device'] # The device used to run the code\n",
    "        \n",
    "        self.env = gym.make(custom_parameters['env'], render_mode='rgb_array') # The environment in which to train\n",
    "\n",
    "        if custom_parameters['record_video']:\n",
    "            self.env  = gym.wrappers.RecordVideo(self.env, f'{custom_parameters[\"env\"]}/{self.run_id}')\n",
    "\n",
    "        if custom_parameters['record_statistics']:\n",
    "            self.env  = gym.wrappers.RecordEpisodeStatistics(self.env)\n",
    "\n",
    "        self.n_episodes = int(custom_parameters['n_episodes']) # The number of episodes for which to train\n",
    "        \n",
    "        # Set number of hidden nodes and learning rate for each network\n",
    "        self.n_hidden_trans = int(custom_parameters['n_hidden_trans'])\n",
    "        self.lr_trans = float(custom_parameters['lr_trans'])\n",
    "        self.n_hidden_pol = int(custom_parameters['n_hidden_pol'])\n",
    "        self.lr_pol = float(custom_parameters['lr_pol'])\n",
    "        self.n_hidden_val = int(custom_parameters['n_hidden_val'])\n",
    "        self.lr_val = float(custom_parameters['lr_val'])\n",
    "        \n",
    "        self.memory_capacity = int(custom_parameters['memory_capacity']) # The maximum number of items to be stored in memory\n",
    "        self.batch_size = int(custom_parameters['batch_size']) # The mini-batch size\n",
    "        self.freeze_period = int(custom_parameters['freeze_period']) # The number of time-steps the target network is frozen\n",
    "        \n",
    "        self.gamma = float(custom_parameters['gamma']) # A precision parameter\n",
    "        self.Beta = float(custom_parameters['Beta']) # The discount rate\n",
    "        \n",
    "        self.save_network = custom_parameters['save_network'] # If true saves the policy network (state_dict) to a .pth file\n",
    "        self.network_save_path = custom_parameters['network_save_path'].format(\"{}\", self.run_id) # The path to which the network is saved\n",
    "        self.network_save_timer = int(custom_parameters['network_save_timer']) # The number of episodes after which the network is saved\n",
    "                \n",
    "        self.load_network = custom_parameters['load_network'] # If true loads a (policy) network (state_dict) instead of initializing a new one\n",
    "        self.network_load_path = custom_parameters['network_load_path'] # The path from which to laod the network\n",
    "        \n",
    "    def select_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            # Determine the action distribution given the current observation:\n",
    "            policy = self.policy_net(obs)\n",
    "            return torch.multinomial(policy, 1)\n",
    "    \n",
    "    def get_mini_batches(self):\n",
    "        # Retrieve transition data in mini batches\n",
    "        all_obs_batch, all_actions_batch, reward_batch_t1, done_batch_t2 = self.memory.sample(\n",
    "                self.obs_indices, self.action_indices, self.reward_indices,\n",
    "                self.done_indices, self.max_n_indices, self.batch_size)\n",
    "        \n",
    "        # Retrieve a batch of observations for 3 consecutive points in time\n",
    "        obs_batch_t0 = all_obs_batch[:, 0].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        obs_batch_t1 = all_obs_batch[:, 1].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        obs_batch_t2 = all_obs_batch[:, 2].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        \n",
    "        # Retrieve the agent's action history for time t0 and time t1\n",
    "        action_batch_t0 = all_actions_batch[:, 0].unsqueeze(1)\n",
    "        action_batch_t1 = all_actions_batch[:, 1].unsqueeze(1)\n",
    "        \n",
    "        # At time t0 predict the state at time t1:\n",
    "        X = torch.cat((obs_batch_t0, action_batch_t0.float()), dim=1)\n",
    "        pred_batch_t0t1 = self.transition_net(X)\n",
    "\n",
    "        # Determine the prediction error wrt time t0-t1:\n",
    "        pred_error_batch_t0t1 = torch.mean(F.mse_loss(\n",
    "                pred_batch_t0t1, obs_batch_t1, reduction='none'), dim=1).unsqueeze(1)\n",
    "        \n",
    "        return (obs_batch_t0, obs_batch_t1, obs_batch_t2, action_batch_t0,\n",
    "                action_batch_t1, reward_batch_t1, done_batch_t2, pred_error_batch_t0t1)\n",
    "    \n",
    "    def compute_value_net_loss(self, obs_batch_t1, obs_batch_t2,\n",
    "                               action_batch_t1, reward_batch_t1,\n",
    "                               done_batch_t2, pred_error_batch_t0t1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Determine the action distribution for time t2:\n",
    "            policy_batch_t2 = self.policy_net(obs_batch_t2)\n",
    "            \n",
    "            # Determine the target EFEs for time t2:\n",
    "            target_EFEs_batch_t2 = self.target_net(obs_batch_t2)\n",
    "            \n",
    "            # Weigh the target EFEs according to the action distribution:\n",
    "            weighted_targets = ((1-done_batch_t2) * policy_batch_t2 *\n",
    "                                target_EFEs_batch_t2).sum(-1).unsqueeze(1)\n",
    "                \n",
    "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
    "            EFE_estimate_batch = -reward_batch_t1 + pred_error_batch_t0t1 + self.Beta * weighted_targets\n",
    "        \n",
    "        # Determine the EFE at time t1 according to the value network:\n",
    "        EFE_batch_t1 = self.value_net(obs_batch_t1).gather(1, action_batch_t1)\n",
    "            \n",
    "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
    "        value_net_loss = F.mse_loss(EFE_estimate_batch, EFE_batch_t1)\n",
    "        \n",
    "        return value_net_loss\n",
    "    \n",
    "    def compute_VFE(self, obs_batch_t1, pred_error_batch_t0t1):\n",
    "        \n",
    "        # Determine the action distribution for time t1:\n",
    "        policy_batch_t1 = self.policy_net(obs_batch_t1)\n",
    "        \n",
    "        # Determine the EFEs for time t1:\n",
    "        EFEs_batch_t1 = self.value_net(obs_batch_t1).detach()\n",
    "\n",
    "        # Take a gamma-weighted Boltzmann distribution over the EFEs:\n",
    "        boltzmann_EFEs_batch_t1 = torch.softmax(-self.gamma * EFEs_batch_t1, dim=1).clamp(min=1e-9, max=1-1e-9)\n",
    "        \n",
    "        # Weigh them according to the action distribution:\n",
    "        energy_batch = -(policy_batch_t1 * torch.log(boltzmann_EFEs_batch_t1)).sum(-1).view(self.batch_size, 1)\n",
    "        \n",
    "        # Determine the entropy of the action distribution\n",
    "        entropy_batch = -(policy_batch_t1 * torch.log(policy_batch_t1)).sum(-1).view(self.batch_size, 1)\n",
    "        \n",
    "        # Determine the VFE, then take the mean over all batch samples:\n",
    "        VFE_batch = pred_error_batch_t0t1 + (energy_batch - entropy_batch)\n",
    "        VFE = torch.mean(VFE_batch)\n",
    "        \n",
    "        return VFE\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        # If there are not enough transitions stored in memory, return:\n",
    "        if self.memory.push_count - self.max_n_indices*2 < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # After every freeze_period time steps, update the target network:\n",
    "        if self.freeze_cntr % self.freeze_period == 0:\n",
    "            self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "        self.freeze_cntr += 1\n",
    "        \n",
    "        # Retrieve transition data in mini batches:\n",
    "        (obs_batch_t0, obs_batch_t1, obs_batch_t2, action_batch_t0,\n",
    "         action_batch_t1, reward_batch_t1, done_batch_t2,\n",
    "         pred_error_batch_t0t1) = self.get_mini_batches()\n",
    "        \n",
    "        # Compute the value network loss:\n",
    "        value_net_loss = self.compute_value_net_loss(obs_batch_t1, obs_batch_t2, \n",
    "                                         action_batch_t1, reward_batch_t1,\n",
    "                                         done_batch_t2, pred_error_batch_t0t1)\n",
    "        \n",
    "        # Compute the variational free energy:\n",
    "        VFE = self.compute_VFE(obs_batch_t1, pred_error_batch_t0t1)\n",
    "        \n",
    "        # Reset the gradients:\n",
    "        self.transition_net.optimizer.zero_grad()\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        self.value_net.optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the gradients:\n",
    "        VFE.backward()\n",
    "        value_net_loss.backward()\n",
    "        \n",
    "        # Perform gradient descent:\n",
    "        self.transition_net.optimizer.step()\n",
    "        self.policy_net.optimizer.step()\n",
    "        self.value_net.optimizer.step()\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ith_episode in range(self.n_episodes):\n",
    "            \n",
    "            total_reward = 0\n",
    "            obs, _ = self.env.reset()\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            done = False\n",
    "            reward = 0\n",
    "            while not done:\n",
    "                \n",
    "                action = self.select_action(obs)\n",
    "                self.memory.push(obs, action, reward, done)\n",
    "                \n",
    "                obs, reward, done, _, _ = self.env.step(action[0].item())\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "                total_reward += reward\n",
    "                \n",
    "                self.learn()\n",
    "                \n",
    "                if done:\n",
    "                    self.memory.push(obs, -99, -99, done)\n",
    "                    \n",
    "            results.append(total_reward)\n",
    "\n",
    "            print(f'Episode: {ith_episode} - Reward: {total_reward}')\n",
    "\n",
    "        self.env.close()\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
