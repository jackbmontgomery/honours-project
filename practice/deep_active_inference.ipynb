{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Active Inference\n",
    "\n",
    "We are creating neural networks to scale certain models within the active inference framework.\n",
    "\n",
    "**Transition Network**: \n",
    "The input of the network is the state plus the action and the output is the likely values for the next state (obs)\n",
    "- We use this network to compute the Predition Error (Pt) which is the difference between what the predicted next state was and what is actually was:\n",
    "```python\n",
    "transition_network(state_t0 + action_t0) - state_t1\n",
    "```\n",
    "\n",
    "**Policy Network**: \n",
    "\n",
    "The input to this network is the current state (observation in MDP's) of the environment and outputs a distribution over actions to be taken by the agent. Generally we then run this through a softmax with precision of 1\n",
    "\n",
    "**Value Network**: \n",
    "\n",
    "This network is detetmining the EFE given a state, similar to the q-value network in reinforcement learning. \n",
    "\n",
    "- Input: State (Obs)\n",
    "- Output: EFE of each action\n",
    "\n",
    "The training of this network works based on the ReplayMemory. \n",
    "\n",
    "We gather a set of say 64 transitions from memory. We then compute the transition prediction error. This is what is used to train the transition network. We are then aiming to create some value approximate for the true EFE of taking an action (policy) in a certain state. \n",
    "\n",
    "We can compute what our policy network is outputing at time t2 given an observation, along with the target EFE from the value target network. We then weight the EFE in each state given by the respective probability of performing these actions, we then add the transition mse and with the reward obtained at t1. We then obtain the bootstrap EFE of performing the action we took at t0. Which we then calculate and use the MSE between the bootstrap EFE and the calculated one to train the value network. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-F= & -E_{Q\\left(s_t\\right)}\\left[\\log p\\left(o_t \\mid s_t\\right)\\right]-K L\\left[Q\\left(s_t\\right) \\| p\\left(s_t \\mid s_{t-1}, a_{t-1}\\right)\\right] \\\\\n",
    "& -E_{Q\\left(s_t\\right)} K L\\left[Q\\left(a_t \\mid s_t\\right) \\| p\\left(a_t \\mid s_t\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note:\n",
    "\n",
    "- In this case the observation term, $E_{Q\\left(s_t\\right)}\\left[\\log p\\left(o_t \\mid s_t\\right)\\right]$ , is not neccesary since this is an mdp so the state is the observation and this infernce does not need to occur\n",
    "\n",
    "- The different between the inferred states and the acutal states. Can I make this the difference between these 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension: int, output_dimention: int, pass_through_softmax: bool = False, lr: float = 0.01):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(input_dimension, 64)\n",
    "        self.layer_2 = nn.Linear(64, output_dimention)\n",
    "        \n",
    "        self.pass_through_softmax = pass_through_softmax\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.layer_1(x))\n",
    "\n",
    "        if self.pass_through_softmax:\n",
    "            return F.softmax(self.layer_2(x), dim=-1)\n",
    "        else:\n",
    "            return self.layer_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 done - Average Rewards Last 100: 13.0\n",
      "Episode 200 done - Average Rewards Last 100: 9.0\n",
      "Episode 300 done - Average Rewards Last 100: 9.0\n",
      "Episode 400 done - Average Rewards Last 100: 9.0\n",
      "Episode 500 done - Average Rewards Last 100: 9.0\n",
      "Episode 600 done - Average Rewards Last 100: 9.0\n",
      "Episode 700 done - Average Rewards Last 100: 9.0\n",
      "Episode 800 done - Average Rewards Last 100: 10.0\n",
      "Episode 900 done - Average Rewards Last 100: 9.0\n",
      "Episode 1000 done - Average Rewards Last 100: 9.0\n",
      "Episode 1100 done - Average Rewards Last 100: 9.0\n",
      "Episode 1200 done - Average Rewards Last 100: 8.0\n",
      "Episode 1300 done - Average Rewards Last 100: 11.0\n",
      "Episode 1400 done - Average Rewards Last 100: 10.0\n",
      "Episode 1500 done - Average Rewards Last 100: 10.0\n",
      "Episode 1600 done - Average Rewards Last 100: 8.0\n",
      "Episode 1700 done - Average Rewards Last 100: 9.0\n",
      "Episode 1800 done - Average Rewards Last 100: 10.0\n",
      "Episode 1900 done - Average Rewards Last 100: 10.0\n",
      "Episode 2000 done - Average Rewards Last 100: 9.0\n",
      "Episode 2100 done - Average Rewards Last 100: 9.0\n",
      "Episode 2200 done - Average Rewards Last 100: 10.0\n",
      "Episode 2300 done - Average Rewards Last 100: 10.0\n",
      "Episode 2400 done - Average Rewards Last 100: 10.0\n",
      "Episode 2500 done - Average Rewards Last 100: 10.0\n",
      "Episode 2600 done - Average Rewards Last 100: 10.0\n",
      "Episode 2700 done - Average Rewards Last 100: 8.0\n",
      "Episode 2800 done - Average Rewards Last 100: 9.0\n",
      "Episode 2900 done - Average Rewards Last 100: 10.0\n",
      "Episode 3000 done - Average Rewards Last 100: 9.0\n",
      "Episode 3100 done - Average Rewards Last 100: 9.0\n",
      "Episode 3200 done - Average Rewards Last 100: 9.0\n",
      "Episode 3300 done - Average Rewards Last 100: 11.0\n",
      "Episode 3400 done - Average Rewards Last 100: 9.0\n",
      "Episode 3500 done - Average Rewards Last 100: 10.0\n",
      "Episode 3600 done - Average Rewards Last 100: 8.0\n",
      "Episode 3700 done - Average Rewards Last 100: 10.0\n",
      "Episode 3800 done - Average Rewards Last 100: 8.0\n",
      "Episode 3900 done - Average Rewards Last 100: 9.0\n",
      "Episode 4000 done - Average Rewards Last 100: 10.0\n",
      "Episode 4100 done - Average Rewards Last 100: 10.0\n",
      "Episode 4200 done - Average Rewards Last 100: 9.0\n",
      "Episode 4300 done - Average Rewards Last 100: 9.0\n",
      "Episode 4400 done - Average Rewards Last 100: 10.0\n",
      "Episode 4500 done - Average Rewards Last 100: 9.0\n",
      "Episode 4600 done - Average Rewards Last 100: 10.0\n",
      "Episode 4700 done - Average Rewards Last 100: 10.0\n",
      "Episode 4800 done - Average Rewards Last 100: 9.0\n",
      "Episode 4900 done - Average Rewards Last 100: 9.0\n",
      "Episode 5000 done - Average Rewards Last 100: 10.0\n",
      "Episode 5100 done - Average Rewards Last 100: 9.0\n",
      "Episode 5200 done - Average Rewards Last 100: 9.0\n",
      "Episode 5300 done - Average Rewards Last 100: 9.0\n",
      "Episode 5400 done - Average Rewards Last 100: 8.0\n",
      "Episode 5500 done - Average Rewards Last 100: 10.0\n",
      "Episode 5600 done - Average Rewards Last 100: 9.0\n",
      "Episode 5700 done - Average Rewards Last 100: 10.0\n",
      "Episode 5800 done - Average Rewards Last 100: 10.0\n",
      "Episode 5900 done - Average Rewards Last 100: 11.0\n",
      "Episode 6000 done - Average Rewards Last 100: 9.0\n",
      "Episode 6100 done - Average Rewards Last 100: 10.0\n",
      "Episode 6200 done - Average Rewards Last 100: 10.0\n",
      "Episode 6300 done - Average Rewards Last 100: 10.0\n",
      "Episode 6400 done - Average Rewards Last 100: 9.0\n",
      "Episode 6500 done - Average Rewards Last 100: 10.0\n",
      "Episode 6600 done - Average Rewards Last 100: 9.0\n",
      "Episode 6700 done - Average Rewards Last 100: 8.0\n",
      "Episode 6800 done - Average Rewards Last 100: 10.0\n",
      "Episode 6900 done - Average Rewards Last 100: 9.0\n",
      "Episode 7000 done - Average Rewards Last 100: 8.0\n",
      "Episode 7100 done - Average Rewards Last 100: 11.0\n",
      "Episode 7200 done - Average Rewards Last 100: 10.0\n",
      "Episode 7300 done - Average Rewards Last 100: 10.0\n",
      "Episode 7400 done - Average Rewards Last 100: 9.0\n",
      "Episode 7500 done - Average Rewards Last 100: 10.0\n",
      "Episode 7600 done - Average Rewards Last 100: 9.0\n",
      "Episode 7700 done - Average Rewards Last 100: 9.0\n",
      "Episode 7800 done - Average Rewards Last 100: 10.0\n",
      "Episode 7900 done - Average Rewards Last 100: 9.0\n",
      "Episode 8000 done - Average Rewards Last 100: 10.0\n",
      "Episode 8100 done - Average Rewards Last 100: 9.0\n",
      "Episode 8200 done - Average Rewards Last 100: 9.0\n",
      "Episode 8300 done - Average Rewards Last 100: 9.0\n",
      "Episode 8400 done - Average Rewards Last 100: 9.0\n",
      "Episode 8500 done - Average Rewards Last 100: 9.0\n",
      "Episode 8600 done - Average Rewards Last 100: 9.0\n",
      "Episode 8700 done - Average Rewards Last 100: 8.0\n",
      "Episode 8800 done - Average Rewards Last 100: 10.0\n",
      "Episode 8900 done - Average Rewards Last 100: 8.0\n",
      "Episode 9000 done - Average Rewards Last 100: 11.0\n",
      "Episode 9100 done - Average Rewards Last 100: 9.0\n",
      "Episode 9200 done - Average Rewards Last 100: 10.0\n",
      "Episode 9300 done - Average Rewards Last 100: 10.0\n",
      "Episode 9400 done - Average Rewards Last 100: 9.0\n",
      "Episode 9500 done - Average Rewards Last 100: 9.0\n",
      "Episode 9600 done - Average Rewards Last 100: 10.0\n",
      "Episode 9700 done - Average Rewards Last 100: 9.0\n",
      "Episode 9800 done - Average Rewards Last 100: 9.0\n",
      "Episode 9900 done - Average Rewards Last 100: 10.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, observation_space: gymnasium.Space, action_space: gymnasium.Space, memory_size: int = 1000, gamma = 1) -> None:\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size=memory_size, observation_space=observation_space, action_space=action_space, handle_timeout_termination=False)\n",
    "\n",
    "        state_dimention = observation_space.shape[0]\n",
    "        num_actions     = action_space.n\n",
    "\n",
    "        self.transition_network = NeuralNetwork(input_dimension=state_dimention + 1, output_dimention=state_dimention)\n",
    "        self.policy_network     = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions, pass_through_softmax=True)\n",
    "\n",
    "        self.value_network      = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions)\n",
    "        self.target_network     = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions)\n",
    "        self.target_network.load_state_dict(self.value_network.state_dict())\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_bootstrap_loss(self, batch_transition_loss, observation_batch, action_batch, next_observation_batch, reward_batch, dones_batch):\n",
    "\n",
    "        # observation_batch       = torch.tensor(observation_batch)\n",
    "        # next_observation_batch  = torch.tensor(next_observation_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            target_efe_batch            = self.target_network(next_observation_batch)\n",
    "            action_distribution_batch   = self.policy_network(next_observation_batch)\n",
    "\n",
    "            # print(target_efe_batch)\n",
    "            # print(action_distribution_batch)\n",
    "\n",
    "            weighted_target_efe = ((1-dones_batch) * action_distribution_batch * target_efe_batch).sum(-1).unsqueeze(1)\n",
    "\n",
    "            # print(weighted_target_efe)\n",
    "\n",
    "            bootstrap_efe   = - reward_batch + batch_transition_loss + weighted_target_efe\n",
    "\n",
    "            # print(bootstrap_efe)\n",
    "\n",
    "        efe = self.value_network(observation_batch).gather(1, action_batch)\n",
    "\n",
    "        # print(efe)\n",
    "        # print(bootstrap_efe)\n",
    "\n",
    "        bootstrap_loss = F.mse_loss(bootstrap_efe, efe)\n",
    "\n",
    "        return bootstrap_loss\n",
    "\n",
    "    def compute_vfe(self, transition_loss, observation_batch) -> torch.Tensor:\n",
    "\n",
    "        # observation_batch = torch.tensor(observation_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            efes_batch = self.value_network(observation_batch)\n",
    "\n",
    "        action_distribution_batch = self.policy_network(observation_batch)\n",
    "\n",
    "        boltzmann_efe_dist = torch.softmax(-self.gamma * efes_batch, dim=1)\n",
    "\n",
    "        energy_batch = -(action_distribution_batch * torch.log(action_distribution_batch)).sum(1).unsqueeze(1)\n",
    "\n",
    "        entropy_batch = -(action_distribution_batch * torch.log(boltzmann_efe_dist)).sum(1).unsqueeze(1)\n",
    "\n",
    "        vfe = transition_loss + (energy_batch - entropy_batch)\n",
    "\n",
    "        mean_vfe = torch.mean(vfe)\n",
    "\n",
    "        return mean_vfe\n",
    "    \n",
    "    def compute_transition_loss(self, observation_batch, action_batch, next_observation_batch):\n",
    "        \n",
    "        # observation_batch           = torch.tensor(observation_batch)\n",
    "        # action_batch                = torch.tensor(action_batch)\n",
    "\n",
    "        batch_state_action_pairs    = torch.cat((observation_batch, action_batch), dim=1)\n",
    "        transition_batch            = self.transition_network(batch_state_action_pairs)\n",
    "        # next_observation_batch      = torch.tensor(next_obs)\n",
    "\n",
    "        batch_transition_loss = torch.mean(F.mse_loss(transition_batch, next_observation_batch, reduction='none'), dim=1).unsqueeze(1)\n",
    "\n",
    "        # print(f'transition_loss: {batch_transition_loss}')\n",
    "\n",
    "        return batch_transition_loss\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            action_distribution = self.policy_network(torch.tensor(obs))\n",
    "\n",
    "            action_sample = Categorical(action_distribution).sample()\n",
    "\n",
    "        return action_sample.numpy()\n",
    "    \n",
    "    def learn(self):\n",
    "\n",
    "        batch           = self.memory.sample(4)\n",
    "\n",
    "        transition_loss = self.compute_transition_loss(batch.observations, batch.actions, batch.next_observations)\n",
    "        bootstrap_loss  = self.compute_bootstrap_loss(transition_loss, batch.observations, batch.actions, batch.next_observations, batch.rewards , batch.dones)\n",
    "        vfe             = self.compute_vfe(transition_loss, batch.observations)\n",
    "\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        self.transition_network.optimizer.zero_grad()\n",
    "        self.value_network.optimizer.zero_grad()\n",
    "        \n",
    "        vfe.backward()\n",
    "        bootstrap_loss.backward()\n",
    "\n",
    "        self.policy_network.optimizer.step()\n",
    "        self.transition_network.optimizer.step()\n",
    "        self.value_network.optimizer.step()\n",
    "\n",
    "    def train(self, env: gymnasium.Env, episodes: int = 100, max_episode_length: int = 1000000) -> None:\n",
    "\n",
    "        episode_rewards = []\n",
    "\n",
    "        for e in range(episodes):\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            total_episode_rewards = 0 \n",
    "            total_length    = 0\n",
    "            episode_length  = 0\n",
    "            done            = False\n",
    "\n",
    "            while not done and episode_length < max_episode_length:\n",
    "\n",
    "                episode_length += 1\n",
    "                total_length   += 1\n",
    "\n",
    "                action = self.sample_action(obs)\n",
    "\n",
    "                next_obs, reward, terminated, truncated, infos = env.step(action)\n",
    "\n",
    "                self.memory.add(obs, next_obs, action, reward, done, infos)\n",
    "\n",
    "                obs     = next_obs\n",
    "                done    = terminated or truncated\n",
    "                \n",
    "                total_episode_rewards   += reward\n",
    "\n",
    "                if total_length % 5 == 0:\n",
    "                    self.learn()\n",
    "                \n",
    "                if total_length % 50 == 0:\n",
    "                    self.target_network.load_state_dict(self.value_network.state_dict())\n",
    "\n",
    "        \n",
    "            episode_rewards.append(total_episode_rewards)\n",
    "            if e % 100 == 0 and e != 0:\n",
    "                print(f'Episode {e} done - Average Rewards Last 100: {np.average(episode_rewards[-100])}')\n",
    "\n",
    "env = gymnasium.make(\"CartPole-v1\")\n",
    "agent = Agent(observation_space=env.observation_space, action_space=env.action_space)\n",
    "agent.train(env=env, episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
