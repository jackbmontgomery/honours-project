{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Active Inference\n",
    "\n",
    "We are creating neural networks to scale certain models within the active inference framework.\n",
    "\n",
    "**Transition Network**: \n",
    "The input of the network is the state plus the action and the output is the likely values for the next state (obs)\n",
    "- We use this network to compute the Predition Error (Pt) which is the difference between what the predicted next state was and what is actually was:\n",
    "```python\n",
    "transition_network(state_t0 + action_t0) - state_t1\n",
    "```\n",
    "\n",
    "**Policy Network**: \n",
    "\n",
    "The input to this network is the current state (observation in MDP's) of the environment and outputs a distribution over actions to be taken by the agent. Generally we then run this through a softmax with precision of 1\n",
    "\n",
    "**Value Network**: \n",
    "\n",
    "This network is detetmining the EFE given a state, similar to the q-value network in reinforcement learning. \n",
    "\n",
    "- Input: State (Obs)\n",
    "- Output: EFE of each action\n",
    "\n",
    "The training of this network works based on the ReplayMemory. \n",
    "\n",
    "We gather a set of say 64 transitions from memory. We then compute the transition prediction error. This is what is used to train the transition network. We are then aiming to create some value approximate for the true EFE of taking an action (policy) in a certain state. \n",
    "\n",
    "We can compute what our policy network is outputing at time t2 given an observation, along with the target EFE from the value target network. We then weight the EFE in each state given by the respective probability of performing these actions, we then add the transition mse and with the reward obtained at t1. We then obtain the bootstrap EFE of performing the action we took at t0. Which we then calculate and use the MSE between the bootstrap EFE and the calculated one to train the value network. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-F= & -E_{Q\\left(s_t\\right)}\\left[\\log p\\left(o_t \\mid s_t\\right)\\right]-K L\\left[Q\\left(s_t\\right) \\| p\\left(s_t \\mid s_{t-1}, a_{t-1}\\right)\\right] \\\\\n",
    "& -E_{Q\\left(s_t\\right)} K L\\left[Q\\left(a_t \\mid s_t\\right) \\| p\\left(a_t \\mid s_t\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note:\n",
    "\n",
    "- In this case the observation term, $E_{Q\\left(s_t\\right)}\\left[\\log p\\left(o_t \\mid s_t\\right)\\right]$ , is not neccesary since this is an mdp so the state is the observation and this infernce does not need to occur\n",
    "\n",
    "- The different between the inferred states and the acutal states. Can I make this the difference between these 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension: int, output_dimention: int, pass_through_softmax: bool = False, lr: float = 0.01):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(input_dimension, 64)\n",
    "        self.layer_2 = nn.Linear(64, output_dimention)\n",
    "        \n",
    "        self.pass_through_softmax = pass_through_softmax\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.layer_1(x))\n",
    "\n",
    "        if self.pass_through_softmax:\n",
    "            return F.softmax(self.layer_2(x), dim=-1)\n",
    "        else:\n",
    "            return self.layer_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, observation_space: gymnasium.Space, action_space: gymnasium.Space, memory_size: int = 1000) -> None:\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size=memory_size, observation_space=observation_space, action_space=action_space, handle_timeout_termination=False)\n",
    "\n",
    "        state_dimention = observation_space.shape[0]\n",
    "        num_actions     = action_space.n\n",
    "\n",
    "        self.transition_network = NeuralNetwork(input_dimension=state_dimention + 1, output_dimention=state_dimention)\n",
    "        self.policy_network     = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions, pass_through_softmax=True)\n",
    "\n",
    "        self.value_network      = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions)\n",
    "        self.target_network     = NeuralNetwork(input_dimension=state_dimention, output_dimention=num_actions)\n",
    "        self.target_network.load_state_dict(self.value_network.state_dict())\n",
    "\n",
    "    def compute_bootstrap_loss(self, transition_loss, obs, action, next_obs, reward):\n",
    "\n",
    "        obs      = torch.tensor(obs)\n",
    "        next_obs = torch.tensor(next_obs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            target_efe                  = self.target_network(next_obs)\n",
    "            next_action_distribution    = self.policy_network(next_obs)\n",
    "\n",
    "            weighted_target_efe         = torch.sum(target_efe * next_action_distribution, dim=-1)\n",
    "\n",
    "            bootstrap_efe   = - reward + transition_loss + weighted_target_efe\n",
    "\n",
    "        efe = self.value_network(obs)[action]\n",
    "\n",
    "        bootstrap_loss = F.mse_loss(bootstrap_efe, efe)\n",
    "\n",
    "        return bootstrap_loss\n",
    "\n",
    "    def compute_vfe(self, transition_loss, obs) -> torch.Tensor:\n",
    "\n",
    "        obs = torch.tensor(obs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            efes = self.value_network(obs)\n",
    "\n",
    "        action_distribution = self.policy_network(obs)\n",
    "\n",
    "        boltzmann_efe_dist = torch.softmax(-1 * efes, dim=0)\n",
    "\n",
    "        energy_batch = action_distribution * torch.log(boltzmann_efe_dist)\n",
    "        entropy_batch = action_distribution * torch.log(boltzmann_efe_dist)\n",
    "\n",
    "        vfe = transition_loss - (energy_batch + entropy_batch).sum(0)\n",
    "        return vfe\n",
    "    \n",
    "    def compute_transition_loss(self, obs, action, next_obs):\n",
    "        \n",
    "        obs     = torch.tensor(obs)\n",
    "        action  = torch.tensor(np.array([action]))\n",
    "\n",
    "        state_action_pair   = torch.cat((obs, action), dim=0)\n",
    "        predicted_next_obs  = self.transition_network(state_action_pair)\n",
    "        next_obs            = torch.tensor(next_obs)\n",
    "\n",
    "        return F.mse_loss(predicted_next_obs, next_obs)\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            action_distribution = self.policy_network(torch.tensor(obs))\n",
    "\n",
    "            action_sample = Categorical(action_distribution).sample()\n",
    "\n",
    "        return action_sample.numpy()\n",
    "\n",
    "    def train(self, env: gymnasium.Env, episodes: int = 100, max_episode_length: int = 1000000) -> None:\n",
    "\n",
    "        episode_rewards = []\n",
    "\n",
    "        for e in range(episodes):\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            total_episode_rewards = 0 \n",
    "            total_length    = 0\n",
    "            episode_length  = 0\n",
    "            done            = False\n",
    "\n",
    "            while not done and episode_length < max_episode_length:\n",
    "\n",
    "                episode_length += 1\n",
    "                total_length   += 1\n",
    "\n",
    "                action = self.sample_action(obs)\n",
    "\n",
    "                new_obs, reward, terminated, truncated, infos = env.step(action)\n",
    "\n",
    "                transition_loss = self.compute_transition_loss(obs, action, new_obs)\n",
    "                bootstrap_loss  = self.compute_bootstrap_loss(transition_loss, obs, action, new_obs, reward )\n",
    "                vfe             = self.compute_vfe(transition_loss, obs)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                # self.memory.add(obs, new_obs, action_sample, reward, done, infos)\n",
    "\n",
    "                self.policy_network.optimizer.zero_grad()\n",
    "                self.transition_network.optimizer.zero_grad()\n",
    "                self.value_network.optimizer.zero_grad()\n",
    "                \n",
    "                vfe.backward()\n",
    "                bootstrap_loss.backward()\n",
    "\n",
    "                self.policy_network.optimizer.step()\n",
    "                self.transition_network.optimizer.step()\n",
    "                self.value_network.optimizer.step()\n",
    "\n",
    "                obs             = new_obs\n",
    "                total_episode_rewards    += reward\n",
    "                \n",
    "                if total_length % 25 == 0:\n",
    "\n",
    "                    # Load new values into target policy\n",
    "                    self.target_network.load_state_dict(self.value_network.state_dict())\n",
    "        \n",
    "            episode_rewards.append(total_episode_rewards)\n",
    "            if e % 100 == 0 and e != 0:\n",
    "                print(f'Episode {e} done - Average Rewards Last 100: {np.average(episode_rewards[-100])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 done - Average Rewards Last 100: 10.0\n",
      "Episode 200 done - Average Rewards Last 100: 8.0\n",
      "Episode 300 done - Average Rewards Last 100: 9.0\n",
      "Episode 400 done - Average Rewards Last 100: 9.0\n",
      "Episode 500 done - Average Rewards Last 100: 10.0\n",
      "Episode 600 done - Average Rewards Last 100: 9.0\n",
      "Episode 700 done - Average Rewards Last 100: 10.0\n",
      "Episode 800 done - Average Rewards Last 100: 10.0\n",
      "Episode 900 done - Average Rewards Last 100: 8.0\n",
      "Episode 1000 done - Average Rewards Last 100: 8.0\n",
      "Episode 1100 done - Average Rewards Last 100: 9.0\n",
      "Episode 1200 done - Average Rewards Last 100: 10.0\n",
      "Episode 1300 done - Average Rewards Last 100: 10.0\n",
      "Episode 1400 done - Average Rewards Last 100: 9.0\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"CartPole-v1\")\n",
    "agent = Agent(observation_space=env.observation_space, action_space=env.action_space)\n",
    "agent.train(env=env, episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
