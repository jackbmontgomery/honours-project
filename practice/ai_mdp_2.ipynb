{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is: CartPole-v1\n",
      "Training started at 2024-08-05 08:37:16.883145\n",
      "Episodes:   10, avg score: 21.27, over last 10: 20.80\n",
      "Episodes:   20, avg score: 18.67, over last 10: 15.80\n",
      "Episodes:   30, avg score: 17.84, over last 10: 16.10\n",
      "Episodes:   40, avg score: 18.32, over last 10: 19.80\n",
      "Episodes:   50, avg score: 18.31, over last 10: 18.30\n",
      "Episodes:   60, avg score: 18.69, over last 10: 20.60\n",
      "Episodes:   70, avg score: 18.87, over last 10: 20.00\n",
      "Episodes:   80, avg score: 18.74, over last 10: 17.80\n",
      "Episodes:   90, avg score: 19.19, over last 10: 22.80\n",
      "Episodes:  100, avg score: 18.93, over last 10: 16.60\n",
      "Episodes:  110, avg score: 18.66, over last 10: 15.90\n",
      "Episodes:  120, avg score: 19.26, over last 10: 26.00\n",
      "Episodes:  130, avg score: 19.24, over last 10: 19.00\n",
      "Episodes:  140, avg score: 20.00, over last 10: 29.90\n",
      "Episodes:  150, avg score: 20.09, over last 10: 21.30\n",
      "Episodes:  160, avg score: 20.25, over last 10: 22.70\n",
      "Episodes:  170, avg score: 20.74, over last 10: 28.70\n",
      "Episodes:  180, avg score: 20.69, over last 10: 19.80\n",
      "Episodes:  190, avg score: 21.13, over last 10: 29.00\n",
      "Episodes:  200, avg score: 21.35, over last 10: 25.60\n",
      "Episodes:  210, avg score: 21.48, over last 10: 24.20\n",
      "Episodes:  220, avg score: 21.85, over last 10: 29.50\n",
      "Episodes:  230, avg score: 21.84, over last 10: 21.70\n",
      "Episodes:  240, avg score: 22.37, over last 10: 34.70\n",
      "Episodes:  250, avg score: 22.44, over last 10: 24.00\n",
      "Episodes:  260, avg score: 22.36, over last 10: 20.30\n",
      "Episodes:  270, avg score: 22.63, over last 10: 29.80\n",
      "Episodes:  280, avg score: 22.90, over last 10: 30.30\n",
      "Episodes:  290, avg score: 22.88, over last 10: 22.20\n",
      "Episodes:  300, avg score: 23.05, over last 10: 28.10\n",
      "Episodes:  310, avg score: 23.11, over last 10: 24.80\n",
      "Episodes:  320, avg score: 23.11, over last 10: 23.00\n",
      "Episodes:  330, avg score: 23.16, over last 10: 24.90\n",
      "Episodes:  340, avg score: 23.33, over last 10: 28.80\n",
      "Episodes:  350, avg score: 23.47, over last 10: 28.50\n",
      "Episodes:  360, avg score: 23.43, over last 10: 22.10\n",
      "Episodes:  370, avg score: 23.64, over last 10: 30.90\n",
      "Episodes:  380, avg score: 23.73, over last 10: 27.10\n",
      "Episodes:  390, avg score: 23.60, over last 10: 18.80\n",
      "Episodes:  400, avg score: 23.54, over last 10: 21.30\n",
      "Episodes:  410, avg score: 23.58, over last 10: 24.90\n",
      "Episodes:  420, avg score: 23.70, over last 10: 28.60\n",
      "Episodes:  430, avg score: 23.69, over last 10: 23.50\n",
      "Episodes:  440, avg score: 23.65, over last 10: 21.80\n",
      "Episodes:  450, avg score: 23.71, over last 10: 26.40\n",
      "Episodes:  460, avg score: 23.71, over last 10: 23.70\n",
      "Episodes:  470, avg score: 23.56, over last 10: 16.70\n",
      "Episodes:  480, avg score: 23.53, over last 10: 22.20\n",
      "Episodes:  490, avg score: 23.54, over last 10: 24.10\n",
      "Episodes:  500, avg score: 23.61, over last 10: 26.80\n",
      "Episodes:  510, avg score: 23.77, over last 10: 31.90\n",
      "Episodes:  520, avg score: 23.67, over last 10: 18.70\n",
      "Episodes:  530, avg score: 23.60, over last 10: 19.60\n",
      "Episodes:  540, avg score: 23.53, over last 10: 19.80\n",
      "Episodes:  550, avg score: 23.54, over last 10: 24.40\n",
      "Episodes:  560, avg score: 23.45, over last 10: 18.30\n",
      "Episodes:  570, avg score: 23.48, over last 10: 25.40\n",
      "Episodes:  580, avg score: 23.40, over last 10: 18.70\n",
      "Episodes:  590, avg score: 23.38, over last 10: 22.20\n",
      "Episodes:  600, avg score: 23.39, over last 10: 24.20\n",
      "Episodes:  610, avg score: 23.32, over last 10: 18.90\n",
      "Episodes:  620, avg score: 23.34, over last 10: 24.80\n",
      "Episodes:  630, avg score: 23.17, over last 10: 12.30\n",
      "Episodes:  640, avg score: 23.14, over last 10: 21.40\n",
      "Episodes:  650, avg score: 23.10, over last 10: 20.50\n",
      "Episodes:  660, avg score: 23.08, over last 10: 22.00\n",
      "Episodes:  670, avg score: 23.12, over last 10: 25.70\n",
      "Episodes:  680, avg score: 23.05, over last 10: 18.00\n",
      "Episodes:  690, avg score: 22.95, over last 10: 16.10\n",
      "Episodes:  700, avg score: 22.90, over last 10: 19.50\n",
      "Episodes:  710, avg score: 22.84, over last 10: 18.50\n",
      "Episodes:  720, avg score: 22.70, over last 10: 12.80\n",
      "Episodes:  730, avg score: 22.60, over last 10: 15.30\n",
      "Episodes:  740, avg score: 22.55, over last 10: 18.90\n",
      "Episodes:  750, avg score: 22.55, over last 10: 22.50\n",
      "Episodes:  760, avg score: 22.56, over last 10: 23.50\n",
      "Episodes:  770, avg score: 22.50, over last 10: 17.80\n",
      "Episodes:  780, avg score: 22.47, over last 10: 20.50\n",
      "Episodes:  790, avg score: 22.40, over last 10: 16.70\n",
      "Episodes:  800, avg score: 22.39, over last 10: 21.90\n",
      "Episodes:  810, avg score: 22.35, over last 10: 19.10\n",
      "Episodes:  820, avg score: 22.26, over last 10: 15.20\n",
      "Episodes:  830, avg score: 22.25, over last 10: 20.70\n",
      "Episodes:  840, avg score: 22.24, over last 10: 21.50\n",
      "Episodes:  850, avg score: 22.21, over last 10: 19.80\n",
      "Episodes:  860, avg score: 22.17, over last 10: 18.60\n",
      "Episodes:  870, avg score: 22.13, over last 10: 18.60\n",
      "Episodes:  880, avg score: 22.07, over last 10: 17.70\n",
      "Episodes:  890, avg score: 22.02, over last 10: 17.20\n",
      "Episodes:  900, avg score: 21.99, over last 10: 19.00\n",
      "Episodes:  910, avg score: 21.94, over last 10: 17.30\n",
      "Episodes:  920, avg score: 21.88, over last 10: 16.70\n",
      "Episodes:  930, avg score: 21.87, over last 10: 20.70\n",
      "Episodes:  940, avg score: 21.80, over last 10: 16.10\n",
      "Episodes:  950, avg score: 21.76, over last 10: 17.80\n",
      "Episodes:  960, avg score: 21.77, over last 10: 22.70\n",
      "Episodes:  970, avg score: 21.69, over last 10: 14.10\n",
      "Episodes:  980, avg score: 21.61, over last 10: 13.50\n",
      "Episodes:  990, avg score: 21.54, over last 10: 14.40\n",
      "Episodes: 1000, avg score: 21.50, over last 10: 17.90\n",
      "Episodes: 1010, avg score: 21.46, over last 10: 17.40\n",
      "Episodes: 1020, avg score: 21.40, over last 10: 14.90\n",
      "Episodes: 1030, avg score: 21.38, over last 10: 19.80\n",
      "Episodes: 1040, avg score: 21.34, over last 10: 17.70\n",
      "Episodes: 1050, avg score: 21.29, over last 10: 15.40\n",
      "Episodes: 1060, avg score: 21.23, over last 10: 15.20\n",
      "Episodes: 1070, avg score: 21.20, over last 10: 18.40\n",
      "Episodes: 1080, avg score: 21.15, over last 10: 15.30\n",
      "Episodes: 1090, avg score: 21.10, over last 10: 16.10\n",
      "Episodes: 1100, avg score: 21.06, over last 10: 16.00\n",
      "Episodes: 1110, avg score: 21.01, over last 10: 16.00\n",
      "Episodes: 1120, avg score: 20.98, over last 10: 17.00\n",
      "Episodes: 1130, avg score: 20.93, over last 10: 15.60\n",
      "Episodes: 1140, avg score: 20.91, over last 10: 18.30\n",
      "Episodes: 1150, avg score: 20.89, over last 10: 18.90\n",
      "Episodes: 1160, avg score: 20.88, over last 10: 19.60\n",
      "Episodes: 1170, avg score: 20.89, over last 10: 22.10\n",
      "Episodes: 1180, avg score: 20.85, over last 10: 16.00\n",
      "Episodes: 1190, avg score: 20.84, over last 10: 20.00\n",
      "Episodes: 1200, avg score: 20.81, over last 10: 17.70\n",
      "Episodes: 1210, avg score: 20.79, over last 10: 18.50\n",
      "Episodes: 1220, avg score: 20.78, over last 10: 18.60\n",
      "Episodes: 1230, avg score: 20.73, over last 10: 15.70\n",
      "Episodes: 1240, avg score: 20.67, over last 10: 12.70\n",
      "Episodes: 1250, avg score: 20.64, over last 10: 16.90\n",
      "Episodes: 1260, avg score: 20.63, over last 10: 19.30\n",
      "Episodes: 1270, avg score: 20.59, over last 10: 16.10\n",
      "Episodes: 1280, avg score: 20.58, over last 10: 19.40\n",
      "Episodes: 1290, avg score: 20.54, over last 10: 15.50\n",
      "Episodes: 1300, avg score: 20.48, over last 10: 12.70\n",
      "Episodes: 1310, avg score: 20.50, over last 10: 22.90\n",
      "Episodes: 1320, avg score: 20.46, over last 10: 15.20\n",
      "Episodes: 1330, avg score: 20.43, over last 10: 15.70\n",
      "Episodes: 1340, avg score: 20.40, over last 10: 16.60\n",
      "Episodes: 1350, avg score: 20.38, over last 10: 18.20\n",
      "Episodes: 1360, avg score: 20.38, over last 10: 19.80\n",
      "Episodes: 1370, avg score: 20.33, over last 10: 13.70\n",
      "Episodes: 1380, avg score: 20.29, over last 10: 15.60\n",
      "Episodes: 1390, avg score: 20.27, over last 10: 17.00\n",
      "Episodes: 1400, avg score: 20.23, over last 10: 14.30\n",
      "Episodes: 1410, avg score: 20.22, over last 10: 19.10\n",
      "Episodes: 1420, avg score: 20.20, over last 10: 17.90\n",
      "Episodes: 1430, avg score: 20.15, over last 10: 12.90\n",
      "Episodes: 1440, avg score: 20.14, over last 10: 18.50\n",
      "Episodes: 1450, avg score: 20.10, over last 10: 13.40\n",
      "Episodes: 1460, avg score: 20.06, over last 10: 14.90\n",
      "Episodes: 1470, avg score: 20.05, over last 10: 19.30\n",
      "Episodes: 1480, avg score: 20.04, over last 10: 18.00\n",
      "Episodes: 1490, avg score: 20.04, over last 10: 20.40\n",
      "Episodes: 1500, avg score: 20.02, over last 10: 16.00\n",
      "Episodes: 1510, avg score: 20.01, over last 10: 18.60\n",
      "Episodes: 1520, avg score: 19.98, over last 10: 15.50\n",
      "Episodes: 1530, avg score: 19.95, over last 10: 16.30\n",
      "Episodes: 1540, avg score: 19.97, over last 10: 22.90\n",
      "Episodes: 1550, avg score: 19.92, over last 10: 12.40\n",
      "Episodes: 1560, avg score: 19.90, over last 10: 16.70\n",
      "Episodes: 1570, avg score: 19.87, over last 10: 15.20\n",
      "Episodes: 1580, avg score: 19.84, over last 10: 14.00\n",
      "Episodes: 1590, avg score: 19.83, over last 10: 18.90\n",
      "Episodes: 1600, avg score: 19.81, over last 10: 16.90\n",
      "Episodes: 1610, avg score: 19.78, over last 10: 14.50\n",
      "Episodes: 1620, avg score: 19.75, over last 10: 15.30\n",
      "Episodes: 1630, avg score: 19.74, over last 10: 18.00\n",
      "Episodes: 1640, avg score: 19.71, over last 10: 15.00\n",
      "Episodes: 1650, avg score: 19.70, over last 10: 18.50\n",
      "Episodes: 1660, avg score: 19.67, over last 10: 13.80\n",
      "Episodes: 1670, avg score: 19.65, over last 10: 16.40\n",
      "Episodes: 1680, avg score: 19.63, over last 10: 16.40\n",
      "Episodes: 1690, avg score: 19.60, over last 10: 15.50\n",
      "Episodes: 1700, avg score: 19.57, over last 10: 14.20\n",
      "Episodes: 1710, avg score: 19.55, over last 10: 14.90\n",
      "Episodes: 1720, avg score: 19.54, over last 10: 18.50\n",
      "Episodes: 1730, avg score: 19.51, over last 10: 14.30\n",
      "Episodes: 1740, avg score: 19.52, over last 10: 20.70\n",
      "Episodes: 1750, avg score: 19.49, over last 10: 14.20\n",
      "Episodes: 1760, avg score: 19.45, over last 10: 12.90\n",
      "Episodes: 1770, avg score: 19.42, over last 10: 14.90\n",
      "Episodes: 1780, avg score: 19.39, over last 10: 14.20\n",
      "Episodes: 1790, avg score: 19.36, over last 10: 13.90\n",
      "Episodes: 1800, avg score: 19.33, over last 10: 12.90\n",
      "Episodes: 1810, avg score: 19.35, over last 10: 23.70\n",
      "Episodes: 1820, avg score: 19.33, over last 10: 15.20\n",
      "Episodes: 1830, avg score: 19.31, over last 10: 16.50\n",
      "Episodes: 1840, avg score: 19.29, over last 10: 14.70\n",
      "Episodes: 1850, avg score: 19.28, over last 10: 18.00\n",
      "Episodes: 1860, avg score: 19.27, over last 10: 17.00\n",
      "Episodes: 1870, avg score: 19.23, over last 10: 12.70\n",
      "Episodes: 1880, avg score: 19.23, over last 10: 19.20\n",
      "Episodes: 1890, avg score: 19.20, over last 10: 13.10\n",
      "Episodes: 1900, avg score: 19.17, over last 10: 13.50\n",
      "Episodes: 1910, avg score: 19.16, over last 10: 17.10\n",
      "Episodes: 1920, avg score: 19.15, over last 10: 17.60\n",
      "Episodes: 1930, avg score: 19.15, over last 10: 18.30\n",
      "Episodes: 1940, avg score: 19.12, over last 10: 14.60\n",
      "Episodes: 1950, avg score: 19.10, over last 10: 14.80\n",
      "Episodes: 1960, avg score: 19.08, over last 10: 15.30\n",
      "Episodes: 1970, avg score: 19.08, over last 10: 18.40\n",
      "Episodes: 1980, avg score: 19.06, over last 10: 16.20\n",
      "Episodes: 1990, avg score: 19.07, over last 10: 19.80\n",
      "Episodes: 2000, avg score: 19.04, over last 10: 14.40\n",
      "Episodes: 2010, avg score: 19.02, over last 10: 14.90\n",
      "Episodes: 2020, avg score: 19.03, over last 10: 19.80\n",
      "Episodes: 2030, avg score: 19.01, over last 10: 14.50\n",
      "Episodes: 2040, avg score: 19.00, over last 10: 18.70\n",
      "Episodes: 2050, avg score: 18.98, over last 10: 14.20\n",
      "Episodes: 2060, avg score: 18.96, over last 10: 13.80\n",
      "Episodes: 2070, avg score: 18.94, over last 10: 14.80\n",
      "Episodes: 2080, avg score: 18.92, over last 10: 15.20\n",
      "Episodes: 2090, avg score: 18.91, over last 10: 18.00\n",
      "Episodes: 2100, avg score: 18.90, over last 10: 16.10\n",
      "Episodes: 2110, avg score: 18.89, over last 10: 16.40\n",
      "Episodes: 2120, avg score: 18.93, over last 10: 26.80\n",
      "Episodes: 2130, avg score: 18.91, over last 10: 14.90\n",
      "Episodes: 2140, avg score: 18.92, over last 10: 21.50\n",
      "Episodes: 2150, avg score: 18.92, over last 10: 19.20\n",
      "Episodes: 2160, avg score: 18.93, over last 10: 21.00\n",
      "Episodes: 2170, avg score: 18.95, over last 10: 23.30\n",
      "Episodes: 2180, avg score: 18.93, over last 10: 14.10\n",
      "Episodes: 2190, avg score: 18.92, over last 10: 17.80\n",
      "Episodes: 2200, avg score: 18.93, over last 10: 21.10\n",
      "Episodes: 2210, avg score: 18.92, over last 10: 17.00\n",
      "Episodes: 2220, avg score: 18.91, over last 10: 15.60\n",
      "Episodes: 2230, avg score: 18.89, over last 10: 14.20\n",
      "Episodes: 2240, avg score: 18.88, over last 10: 16.60\n",
      "Episodes: 2250, avg score: 18.86, over last 10: 15.90\n",
      "Episodes: 2260, avg score: 18.85, over last 10: 16.60\n",
      "Episodes: 2270, avg score: 18.86, over last 10: 19.20\n",
      "Episodes: 2280, avg score: 18.84, over last 10: 15.90\n",
      "Episodes: 2290, avg score: 18.84, over last 10: 17.20\n",
      "Episodes: 2300, avg score: 18.82, over last 10: 16.30\n",
      "Episodes: 2310, avg score: 18.80, over last 10: 14.00\n",
      "Episodes: 2320, avg score: 18.82, over last 10: 22.50\n",
      "Episodes: 2330, avg score: 18.79, over last 10: 12.90\n",
      "Episodes: 2340, avg score: 18.80, over last 10: 19.60\n",
      "Episodes: 2350, avg score: 18.80, over last 10: 18.40\n",
      "Episodes: 2360, avg score: 18.79, over last 10: 16.70\n",
      "Episodes: 2370, avg score: 18.78, over last 10: 17.20\n",
      "Episodes: 2380, avg score: 18.79, over last 10: 21.00\n",
      "Episodes: 2390, avg score: 18.78, over last 10: 16.50\n",
      "Episodes: 2400, avg score: 18.77, over last 10: 15.70\n",
      "Episodes: 2410, avg score: 18.74, over last 10: 12.80\n",
      "Episodes: 2420, avg score: 18.73, over last 10: 16.90\n",
      "Episodes: 2430, avg score: 18.73, over last 10: 17.00\n",
      "Episodes: 2440, avg score: 18.73, over last 10: 18.70\n",
      "Episodes: 2450, avg score: 18.71, over last 10: 14.80\n",
      "Episodes: 2460, avg score: 18.71, over last 10: 19.00\n",
      "Episodes: 2470, avg score: 18.70, over last 10: 14.70\n",
      "Episodes: 2480, avg score: 18.72, over last 10: 25.10\n",
      "Episodes: 2490, avg score: 18.71, over last 10: 14.50\n",
      "Episodes: 2500, avg score: 18.68, over last 10: 13.10\n",
      "Episodes: 2510, avg score: 18.68, over last 10: 19.20\n",
      "Episodes: 2520, avg score: 18.68, over last 10: 18.50\n",
      "Episodes: 2530, avg score: 18.67, over last 10: 15.10\n",
      "Episodes: 2540, avg score: 18.65, over last 10: 14.20\n",
      "Episodes: 2550, avg score: 18.64, over last 10: 15.20\n",
      "Episodes: 2560, avg score: 18.63, over last 10: 16.80\n",
      "Episodes: 2570, avg score: 18.63, over last 10: 19.20\n",
      "Episodes: 2580, avg score: 18.62, over last 10: 15.40\n",
      "Episodes: 2590, avg score: 18.60, over last 10: 14.20\n",
      "Episodes: 2600, avg score: 18.59, over last 10: 15.10\n",
      "Episodes: 2610, avg score: 18.59, over last 10: 17.30\n",
      "Episodes: 2620, avg score: 18.59, over last 10: 18.90\n",
      "Episodes: 2630, avg score: 18.57, over last 10: 15.30\n",
      "Episodes: 2640, avg score: 18.56, over last 10: 14.90\n",
      "Episodes: 2650, avg score: 18.56, over last 10: 17.40\n",
      "Episodes: 2660, avg score: 18.54, over last 10: 13.20\n",
      "Episodes: 2670, avg score: 18.54, over last 10: 19.70\n",
      "Episodes: 2680, avg score: 18.52, over last 10: 13.90\n",
      "Episodes: 2690, avg score: 18.51, over last 10: 15.10\n",
      "Episodes: 2700, avg score: 18.50, over last 10: 16.40\n",
      "Episodes: 2710, avg score: 18.49, over last 10: 13.80\n",
      "Episodes: 2720, avg score: 18.48, over last 10: 17.70\n",
      "Episodes: 2730, avg score: 18.47, over last 10: 15.10\n",
      "Episodes: 2740, avg score: 18.46, over last 10: 16.50\n",
      "Episodes: 2750, avg score: 18.45, over last 10: 16.10\n",
      "Episodes: 2760, avg score: 18.45, over last 10: 16.60\n",
      "Episodes: 2770, avg score: 18.46, over last 10: 20.50\n",
      "Episodes: 2780, avg score: 18.44, over last 10: 14.80\n",
      "Episodes: 2790, avg score: 18.44, over last 10: 18.50\n",
      "Episodes: 2800, avg score: 18.43, over last 10: 16.10\n",
      "Episodes: 2810, avg score: 18.43, over last 10: 18.50\n",
      "Episodes: 2820, avg score: 18.45, over last 10: 22.70\n",
      "Episodes: 2830, avg score: 18.45, over last 10: 18.50\n",
      "Episodes: 2840, avg score: 18.45, over last 10: 19.60\n",
      "Episodes: 2850, avg score: 18.45, over last 10: 17.00\n",
      "Episodes: 2860, avg score: 18.44, over last 10: 15.80\n",
      "Episodes: 2870, avg score: 18.45, over last 10: 21.20\n",
      "Episodes: 2880, avg score: 18.44, over last 10: 16.40\n",
      "Episodes: 2890, avg score: 18.45, over last 10: 19.80\n",
      "Episodes: 2900, avg score: 18.44, over last 10: 16.80\n",
      "Episodes: 2910, avg score: 18.44, over last 10: 18.60\n",
      "Episodes: 2920, avg score: 18.44, over last 10: 19.40\n",
      "Episodes: 2930, avg score: 18.44, over last 10: 15.90\n",
      "Episodes: 2940, avg score: 18.44, over last 10: 20.60\n",
      "Episodes: 2950, avg score: 18.44, over last 10: 16.60\n",
      "Episodes: 2960, avg score: 18.43, over last 10: 15.40\n",
      "Episodes: 2970, avg score: 18.41, over last 10: 14.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 336\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    335\u001b[0m     agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[0;32m--> 336\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[5], line 316\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(obs, action, reward, done)\n\u001b[1;32m    314\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# if done:\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m#     self.memory.push(obs, -99, -99, done)\u001b[39;00m\n\u001b[1;32m    320\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[0;32mIn[5], line 291\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m value_net_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Perform gradient descent:\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_net\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[1;32m    229\u001b[0m         exp_avgs,\n\u001b[1;32m    230\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    231\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    232\u001b[0m         state_steps,\n\u001b[1;32m    233\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    234\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    235\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    236\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    237\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    239\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    240\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    241\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    242\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    243\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    244\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m func(\n\u001b[1;32m    767\u001b[0m     params,\n\u001b[1;32m    768\u001b[0m     grads,\n\u001b[1;32m    769\u001b[0m     exp_avgs,\n\u001b[1;32m    770\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    771\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    772\u001b[0m     state_steps,\n\u001b[1;32m    773\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    774\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    775\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    776\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    777\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    778\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    779\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    780\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    781\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    782\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    783\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    784\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    785\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:380\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    379\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 380\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    383\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity, obs_shape, device='cpu'):\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.capacity = capacity # The maximum number of items to be stored in memory\n",
    "        \n",
    "        # Initialize (empty) memory tensors\n",
    "        self.obs_mem = torch.empty([capacity]+[dim for dim in obs_shape], dtype=torch.float32, device=self.device)\n",
    "        self.action_mem = torch.empty(capacity, dtype=torch.int64, device=self.device)\n",
    "        self.reward_mem = torch.empty(capacity, dtype=torch.int8, device=self.device)\n",
    "        self.done_mem = torch.empty(capacity, dtype=torch.int8, device=self.device)\n",
    "        \n",
    "        self.push_count = 0 # The number of times new data has been pushed to memory\n",
    "        \n",
    "    def push(self, obs, action, reward, done):\n",
    "        \n",
    "        # Store data to memory\n",
    "        self.obs_mem[self.position()] = obs \n",
    "        self.action_mem[self.position()] = action\n",
    "        self.reward_mem[self.position()] = reward\n",
    "        self.done_mem[self.position()] = done\n",
    "        \n",
    "        self.push_count += 1\n",
    "    \n",
    "    def position(self):\n",
    "        # Returns the next position (index) to which data is pushed\n",
    "        return self.push_count % self.capacity\n",
    "    \n",
    "    \n",
    "    def sample(self, obs_indices, action_indices, reward_indices, done_indices, max_n_indices, batch_size):\n",
    "        # Fine as long as max_n is not greater than the fewest number of time steps an episode can take\n",
    "        \n",
    "        # Pick indices at random\n",
    "        end_indices = np.random.choice(min(self.push_count, self.capacity)-max_n_indices*2, batch_size, replace=False) + max_n_indices\n",
    "        \n",
    "        # Correct for sampling near the position where data was last pushed\n",
    "        for i in range(len(end_indices)):\n",
    "            if end_indices[i] in range(self.position(), self.position()+max_n_indices):\n",
    "                end_indices[i] += max_n_indices\n",
    "        \n",
    "        # Retrieve the specified indices that come before the end_indices\n",
    "        obs_batch = self.obs_mem[np.array([index-obs_indices for index in end_indices])]\n",
    "        action_batch = self.action_mem[np.array([index-action_indices for index in end_indices])]\n",
    "        reward_batch = self.reward_mem[np.array([index-reward_indices for index in end_indices])]\n",
    "        done_batch = self.done_mem[np.array([index-done_indices for index in end_indices])]\n",
    "        \n",
    "        # Correct for sampling over multiple episodes\n",
    "        for i in range(len(end_indices)):\n",
    "            index = end_indices[i]\n",
    "            for j in range(1, max_n_indices):\n",
    "                if self.done_mem[index-j]:\n",
    "                    for k in range(len(obs_indices)):\n",
    "                        if obs_indices[k] >= j:\n",
    "                            obs_batch[i, k] = torch.zeros_like(self.obs_mem[0]) \n",
    "                    for k in range(len(action_indices)):\n",
    "                        if action_indices[k] >= j:\n",
    "                            action_batch[i, k] = torch.zeros_like(self.action_mem[0]) # Assigning action '0' might not be the best solution, perhaps as assigning at random, or adding an action for this specific case would be better\n",
    "                    for k in range(len(reward_indices)):\n",
    "                        if reward_indices[k] >= j:\n",
    "                            reward_batch[i, k] = torch.zeros_like(self.reward_mem[0]) # Reward of 0 will probably not make sense for every environment\n",
    "                    for k in range(len(done_indices)):\n",
    "                        if done_indices[k] >= j:\n",
    "                            done_batch[i, k] = torch.zeros_like(self.done_mem[0]) \n",
    "                    break\n",
    "                \n",
    "        return obs_batch, action_batch, reward_batch, done_batch\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden=64, lr=1e-3, softmax=False, device='cpu'):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs # Number of inputs\n",
    "        self.n_hidden = n_hidden # Number of hidden units\n",
    "        self.n_outputs = n_outputs # Number of outputs\n",
    "        self.softmax = softmax # If true apply a softmax function to the output\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.n_inputs, self.n_hidden) # Hidden layer\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_outputs) # Output layer\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr) # Adam optimizer\n",
    "        \n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass:\n",
    "        h_relu = F.relu(self.fc1(x))\n",
    "        y = self.fc2(h_relu)\n",
    "        \n",
    "        if self.softmax: # If true apply a softmax function to the output\n",
    "            y = F.softmax(self.fc2(h_relu), dim=-1).clamp(min=1e-9, max=1-1e-9)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.set_parameters() # Set parameters\n",
    "        \n",
    "        self.obs_shape = self.env.observation_space.shape # The shape of observations\n",
    "        self.obs_size = np.prod(self.obs_shape) # The size of the observation\n",
    "        self.n_actions = self.env.action_space.n # The number of actions available to the agent\n",
    "        \n",
    "        self.freeze_cntr = 0 # Keeps track of when to (un)freeze the target network\n",
    "        \n",
    "        # Initialize the networks:\n",
    "        self.transition_net = Model(self.obs_size+1, self.obs_size, self.n_hidden_trans, lr=self.lr_trans, device=self.device)\n",
    "        self.policy_net = Model(self.obs_size, self.n_actions, self.n_hidden_pol, lr=self.lr_pol, softmax=True, device=self.device)\n",
    "        self.value_net = Model(self.obs_size, self.n_actions, self.n_hidden_val, lr=self.lr_val, device=self.device)\n",
    "        \n",
    "        self.target_net = Model(self.obs_size, self.n_actions, self.n_hidden_val, lr=self.lr_val, device=self.device)\n",
    "        self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "        \n",
    "        # Initialize the replay memory\n",
    "        self.memory = ReplayMemory(self.memory_capacity, self.obs_shape, device=self.device)\n",
    "        \n",
    "        # When sampling from memory at index i, obs_indices indicates that we want observations with indices i-obs_indices, works the same for the others\n",
    "        self.obs_indices = [2, 1, 0]\n",
    "        self.action_indices = [2, 1]\n",
    "        self.reward_indices = [1]\n",
    "        self.done_indices = [0]\n",
    "        self.max_n_indices = max(max(self.obs_indices, self.action_indices, self.reward_indices, self.done_indices)) + 1\n",
    "\n",
    "    def set_parameters(self):\n",
    "        \n",
    "        # The default parameterss\n",
    "        default_parameters = {'run_id':\"_rX\", 'device':'cpu',\n",
    "              'env':'CartPole-v1', 'n_episodes':5000, \n",
    "              'n_hidden_trans':64, 'lr_trans':1e-3, \n",
    "              'n_hidden_pol':64, 'lr_pol':1e-3, \n",
    "              'n_hidden_val':64, 'lr_val':1e-4,\n",
    "              'memory_capacity':65536, 'batch_size':64, 'freeze_period':25,\n",
    "              'Beta':0.99, 'gamma':1.00, \n",
    "              'print_timer':100,\n",
    "              'keep_log':True, 'log_path':\"logs/ai_mdp_log{}.txt\", 'log_save_timer':10,\n",
    "              'save_results':True, 'results_path':\"results/ai_mdp_results{}.npz\", 'results_save_timer':500,\n",
    "              'save_network':True, 'network_save_path':\"networks/ai_mdp_{}net{}.pth\", 'network_save_timer':500,\n",
    "              'load_network':False, 'network_load_path':\"networks/ai_mdp_{}net_rX.pth\"}\n",
    "        \n",
    "        # Set all parameters\n",
    "        self.run_id = default_parameters['run_id'] # Is appended to paths to distinguish between runs\n",
    "        self.device = default_parameters['device'] # The device used to run the code\n",
    "        \n",
    "        self.env = gym.make(default_parameters['env']) # The environment in which to train\n",
    "        self.n_episodes = int(default_parameters['n_episodes']) # The number of episodes for which to train\n",
    "        \n",
    "        # Set number of hidden nodes and learning rate for each network\n",
    "        self.n_hidden_trans = int(default_parameters['n_hidden_trans'])\n",
    "        self.lr_trans = float(default_parameters['lr_trans'])\n",
    "        self.n_hidden_pol = int(default_parameters['n_hidden_pol'])\n",
    "        self.lr_pol = float(default_parameters['lr_pol'])\n",
    "        self.n_hidden_val = int(default_parameters['n_hidden_val'])\n",
    "        self.lr_val = float(default_parameters['lr_val'])\n",
    "        \n",
    "        self.memory_capacity = int(default_parameters['memory_capacity']) # The maximum number of items to be stored in memory\n",
    "        self.batch_size = int(default_parameters['batch_size']) # The mini-batch size\n",
    "        self.freeze_period = int(default_parameters['freeze_period']) # The number of time-steps the target network is frozen\n",
    "        \n",
    "        self.gamma = float(default_parameters['gamma']) # A precision parameter\n",
    "        self.Beta = float(default_parameters['Beta']) # The discount rate\n",
    "        \n",
    "    def select_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            # Determine the action distribution given the current observation:\n",
    "            policy = self.policy_net(obs)\n",
    "            return torch.multinomial(policy, 1)\n",
    "    \n",
    "    def get_mini_batches(self):\n",
    "        # Retrieve transition data in mini batches\n",
    "        all_obs_batch, all_actions_batch, reward_batch_t1, done_batch_t2 = self.memory.sample(\n",
    "                self.obs_indices, self.action_indices, self.reward_indices,\n",
    "                self.done_indices, self.max_n_indices, self.batch_size)\n",
    "        \n",
    "        # Retrieve a batch of observations for 3 consecutive points in time\n",
    "        obs_batch_t0 = all_obs_batch[:, 0].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        obs_batch_t1 = all_obs_batch[:, 1].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        obs_batch_t2 = all_obs_batch[:, 2].view([self.batch_size] + [dim for dim in self.obs_shape])\n",
    "        \n",
    "        # Retrieve the agent's action history for time t0 and time t1\n",
    "        action_batch_t0 = all_actions_batch[:, 0].unsqueeze(1)\n",
    "        action_batch_t1 = all_actions_batch[:, 1].unsqueeze(1)\n",
    "        \n",
    "        # At time t0 predict the state at time t1:\n",
    "        X = torch.cat((obs_batch_t1, action_batch_t1.float()), dim=1)\n",
    "        pred_batch_t0t1 = self.transition_net(X)\n",
    "\n",
    "        # Determine the prediction error wrt time t0-t1:\n",
    "        pred_error_batch_t0t1 = torch.mean(F.mse_loss(\n",
    "                pred_batch_t0t1, obs_batch_t2, reduction='none'), dim=1).unsqueeze(1)\n",
    "        \n",
    "        return (obs_batch_t0, obs_batch_t1, obs_batch_t2, action_batch_t0,\n",
    "                action_batch_t1, reward_batch_t1, done_batch_t2, pred_error_batch_t0t1)\n",
    "    \n",
    "    def compute_value_net_loss(self, obs_batch_t1, obs_batch_t2,\n",
    "                               action_batch_t1, reward_batch_t1,\n",
    "                               done_batch_t2, pred_error_batch_t0t1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Determine the action distribution for time t2:\n",
    "            policy_batch_t2 = self.policy_net(obs_batch_t2)\n",
    "            \n",
    "            # Determine the target EFEs for time t2:\n",
    "            target_EFEs_batch_t2 = self.target_net(obs_batch_t2)\n",
    "            \n",
    "            # Weigh the target EFEs according to the action distribution:\n",
    "            weighted_targets = ((1-done_batch_t2) * policy_batch_t2 *\n",
    "                                target_EFEs_batch_t2).sum(-1).unsqueeze(1)\n",
    "                \n",
    "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
    "            EFE_estimate_batch = -reward_batch_t1 + pred_error_batch_t0t1 + self.Beta * weighted_targets\n",
    "        \n",
    "        # Determine the EFE at time t1 according to the value network:\n",
    "        EFE_batch_t1 = self.value_net(obs_batch_t1).gather(1, action_batch_t1)\n",
    "            \n",
    "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
    "        value_net_loss = F.mse_loss(EFE_estimate_batch, EFE_batch_t1)\n",
    "        \n",
    "        return value_net_loss\n",
    "    \n",
    "    def compute_VFE(self, obs_batch_t1, pred_error_batch_t0t1):\n",
    "        \n",
    "        # Determine the action distribution for time t1:\n",
    "        policy_batch_t1 = self.policy_net(obs_batch_t1)\n",
    "        \n",
    "        # Determine the EFEs for time t1:\n",
    "        EFEs_batch_t1 = self.value_net(obs_batch_t1).detach()\n",
    "\n",
    "        # Take a gamma-weighted Boltzmann distribution over the EFEs:\n",
    "        boltzmann_EFEs_batch_t1 = torch.softmax(-self.gamma * EFEs_batch_t1, dim=1).clamp(min=1e-9, max=1-1e-9)\n",
    "        \n",
    "        # Weigh them according to the action distribution:\n",
    "        energy_batch = -(policy_batch_t1 * torch.log(boltzmann_EFEs_batch_t1)).sum(-1).view(self.batch_size, 1)\n",
    "        \n",
    "        # Determine the entropy of the action distribution\n",
    "        entropy_batch = -(policy_batch_t1 * torch.log(policy_batch_t1)).sum(-1).view(self.batch_size, 1)\n",
    "        \n",
    "        # Determine the VFE, then take the mean over all batch samples:\n",
    "        VFE_batch = pred_error_batch_t0t1 + (energy_batch - entropy_batch)\n",
    "        VFE = torch.mean(VFE_batch)\n",
    "        \n",
    "        return VFE\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        # If there are not enough transitions stored in memory, return:\n",
    "        if self.memory.push_count - self.max_n_indices*2 < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # After every freeze_period time steps, update the target network:\n",
    "        if self.freeze_cntr % self.freeze_period == 0:\n",
    "            self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "        self.freeze_cntr += 1\n",
    "        \n",
    "        # Retrieve transition data in mini batches:\n",
    "        (obs_batch_t0, obs_batch_t1, obs_batch_t2, action_batch_t0,\n",
    "         action_batch_t1, reward_batch_t1, done_batch_t2,\n",
    "         pred_error_batch_t0t1) = self.get_mini_batches()\n",
    "        \n",
    "        # Compute the value network loss:\n",
    "        value_net_loss = self.compute_value_net_loss(obs_batch_t1, obs_batch_t2, \n",
    "                                         action_batch_t1, reward_batch_t1,\n",
    "                                         done_batch_t2, pred_error_batch_t0t1)\n",
    "        \n",
    "        # Compute the variational free energy:\n",
    "        VFE = self.compute_VFE(obs_batch_t1, pred_error_batch_t0t1)\n",
    "        \n",
    "        # Reset the gradients:\n",
    "        self.transition_net.optimizer.zero_grad()\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        self.value_net.optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the gradients:\n",
    "        VFE.backward()\n",
    "        value_net_loss.backward()\n",
    "        \n",
    "        # Perform gradient descent:\n",
    "        self.transition_net.optimizer.step()\n",
    "        self.policy_net.optimizer.step()\n",
    "        self.value_net.optimizer.step()\n",
    "        \n",
    "    def train(self):\n",
    "        msg = \"Environment is: {}\\nTraining started at {}\".format(self.env.unwrapped.spec.id, datetime.datetime.now())\n",
    "        print(msg)\n",
    "        \n",
    "        results = []\n",
    "        for ith_episode in range(self.n_episodes):\n",
    "            \n",
    "            total_reward = 0\n",
    "            obs, _ = self.env.reset()\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            done = False\n",
    "            reward = 0\n",
    "            while not done:\n",
    "                \n",
    "                action = self.select_action(obs)\n",
    "                \n",
    "                obs, reward, done, _, _ = self.env.step(action[0].item())\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "                self.memory.push(obs, action, reward, done)\n",
    "                total_reward += reward\n",
    "                \n",
    "                self.learn()\n",
    "                \n",
    "                # if done:\n",
    "                #     self.memory.push(obs, -99, -99, done)\n",
    "            results.append(total_reward)\n",
    "            \n",
    "            # Print and keep a (.txt) record of stuff\n",
    "            if ith_episode > 0 and ith_episode % 10 == 0:\n",
    "                avg_reward = np.mean(results)\n",
    "                last_x = np.mean(results[-10:])\n",
    "                msg = \"Episodes: {:4d}, avg score: {:3.2f}, over last {:d}: {:3.2f}\".format(ith_episode, avg_reward, 10, last_x)\n",
    "                print(msg)\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        # Print and keep a (.txt) record of stuff\n",
    "        msg = \"Training finished at {}\".format(datetime.datetime.now())\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
