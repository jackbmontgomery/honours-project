{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import predictive_coding as pc\n",
    "\n",
    "# import analysis_utils as au\n",
    "\n",
    "\n",
    "# def plot(df, plot=\"Reward\"):\n",
    "\n",
    "#     df = au.nature_pre(df)\n",
    "\n",
    "#     groups = ['Env', 'Rule', 'pc_learning_rate']\n",
    "\n",
    "#     df = au.add_metric_per_group(\n",
    "#         df, groups,\n",
    "#         lambda df: (\n",
    "#             'mean per group', df['Mean of episode reward'].mean()\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     groups.pop(-1)\n",
    "\n",
    "#     df = au.select_rows_per_group(\n",
    "#         df, groups,\n",
    "#         lambda df: df['mean per group'] == df['mean per group'].max()\n",
    "#     )\n",
    "\n",
    "#     df = au.drop_cols(df, ['mean per group'])\n",
    "\n",
    "#     df = au.extract_plot(df, f'Episode {plot}', 'training_iteration')\n",
    "\n",
    "#     df = df[df['training_iteration'].isin(list(range(0, 10000, 100)))]\n",
    "\n",
    "#     g = au.nature_relplot_curve(\n",
    "#         data=df,\n",
    "#         x='training_iteration',\n",
    "#         y=f'Episode {plot}',\n",
    "#         hue='Rule', style='Rule',\n",
    "#         hue_order=['PC', 'BP'],\n",
    "#         style_order=['PC', 'BP'],\n",
    "#         col='Env',\n",
    "#         aspect=0.8,\n",
    "#         sharey=False\n",
    "#     )\n",
    "\n",
    "#     au.nature_post(g, is_grid=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "class RunningStats(object):\n",
    "    \"\"\"Computes running mean and standard deviation\n",
    "    Url: https://gist.github.com/wassname/a9502f562d4d3e73729dc5b184db2501\n",
    "    Adapted from:\n",
    "        *\n",
    "        <http://stackoverflow.com/questions/1174984/how-to-efficiently-\\\n",
    "calculate-a-running-standard-deviation>\n",
    "        * <http://mathcentral.uregina.ca/QQ/database/QQ.09.02/carlos1.html>\n",
    "        * <https://gist.github.com/fvisin/5a10066258e43cf6acfa0a474fcdb59f>\n",
    "\n",
    "    Usage:\n",
    "        rs = RunningStats()\n",
    "        for i in range(10):\n",
    "            rs += np.random.randn()\n",
    "            print(rs)\n",
    "        print(rs.mean, rs.std)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=0., m=None, s=None, per_dim=True):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.per_dim = per_dim\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0.\n",
    "\n",
    "    def push(self, x):\n",
    "        # process input\n",
    "        if self.per_dim:\n",
    "            self.update_params(x)\n",
    "        else:\n",
    "            for el in x.flatten():\n",
    "                self.update_params(el)\n",
    "\n",
    "    def update_params(self, x):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.m = x\n",
    "            self.s = 0.\n",
    "        else:\n",
    "            prev_m = self.m.copy()\n",
    "            self.m += (x - self.m) / self.n\n",
    "            self.s += (x - prev_m) * (x - self.m)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, RunningStats):\n",
    "            sum_ns = self.n + other.n\n",
    "            prod_ns = self.n * other.n\n",
    "            delta2 = (other.m - self.m) ** 2.\n",
    "            return RunningStats(sum_ns,\n",
    "                                (self.m * self.n + other.m * other.n) / sum_ns,\n",
    "                                self.s + other.s + delta2 * prod_ns / sum_ns)\n",
    "        else:\n",
    "            self.push(other)\n",
    "            return self\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.s / (self.n) if self.n else 0.0\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<RunningMean(mean={: 2.4f}, std={: 2.4f}, n={: 2f}, m={: 2.4f}, s={: 2.4f})>'.format(self.mean, self.std, self.n, self.m, self.s)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'mean={: 2.4f}, std={: 2.4f}'.format(self.mean, self.std)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (\n",
    "            x - self.mean\n",
    "        ) / (\n",
    "            self.std if np.all(self.std) else 1.0\n",
    "        )\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit, sample_to_device):\n",
    "\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "        self.sample_to_device = sample_to_device\n",
    "\n",
    "    def put(self, transition):\n",
    "\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float).to(self.sample_to_device), torch.tensor(a_lst).to(self.sample_to_device), \\\n",
    "            torch.Tensor(r_lst).to(self.sample_to_device), torch.tensor(s_prime_lst, dtype=torch.float).to(self.sample_to_device), \\\n",
    "            torch.Tensor(done_mask_lst).to(self.sample_to_device)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "\n",
    "    def __init__(self, predictive_coding, num_obs, num_act, bias=True, pc_layer_at='before_acf', hidden_size=128, num_hidden=1, acf='Sigmoid'):\n",
    "\n",
    "        super(Qnet, self).__init__()\n",
    "\n",
    "        self.predictive_coding = predictive_coding\n",
    "        self.num_act = num_act\n",
    "\n",
    "        model = []\n",
    "\n",
    "        # input layer\n",
    "        model.append(nn.Linear(num_obs, hidden_size, bias=bias))\n",
    "        if self.predictive_coding and pc_layer_at == 'before_acf':\n",
    "            model.append(pc.PCLayer())\n",
    "        model.append(eval('nn.{}()'.format(acf)))\n",
    "        if self.predictive_coding and pc_layer_at == 'after_acf':\n",
    "            model.append(pc.PCLayer())\n",
    "\n",
    "        for i in range(num_hidden):\n",
    "\n",
    "            # hidden layer\n",
    "            model.append(nn.Linear(hidden_size, hidden_size, bias=bias))\n",
    "            if self.predictive_coding and pc_layer_at == 'before_acf':\n",
    "                model.append(pc.PCLayer())\n",
    "            model.append(eval('nn.{}()'.format(acf)))\n",
    "            if self.predictive_coding and pc_layer_at == 'after_acf':\n",
    "                model.append(pc.PCLayer())\n",
    "\n",
    "        # output layer\n",
    "        model.append(nn.Linear(hidden_size, num_act, bias=bias))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.num_act - 1)\n",
    "\n",
    "        else:\n",
    "            return self.forward(obs).argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import predictive_coding as pc\n",
    "\n",
    "class BaseTrainable:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['device'])\n",
    "        self.seed = config['seed']\n",
    "        self.env = None\n",
    "        self.q = None\n",
    "        self.q_target = None\n",
    "        self.memory = None\n",
    "        self.pc_trainer = None\n",
    "        self.episode_rewards = []\n",
    "        self.rs_s = None\n",
    "        self.rs_r = None\n",
    "        self._iteration = 0\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # Setup code\n",
    "        self.env = gym.make(self.config['env'])\n",
    "\n",
    "\n",
    "        # create q\n",
    "        self.q = Qnet(predictive_coding=True, num_obs=self.env.observation_space.shape[0], num_act=self.env.action_space.n).to(self.device)\n",
    "\n",
    "        # create q_target\n",
    "        if self.config['is_q_target']:\n",
    "            self.q_target = Qnet(predictive_coding=True, num_obs=self.env.observation_space.shape[0], num_act=self.env.action_space.n).to(self.device)\n",
    "            self.q_target.load_state_dict(self.q.state_dict(), strict=False)\n",
    "        else:\n",
    "            self.q_target = None\n",
    "\n",
    "        # create memory\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_limit=self.config['buffer_limit'],\n",
    "            sample_to_device=self.device,\n",
    "        )\n",
    "\n",
    "        # create pc_trainer\n",
    "        self.pc_trainer = pc.PCTrainer(model=self.q, plot_progress_at=[])\n",
    "\n",
    "        if self.config['is_norm_obs']:\n",
    "            self.rs_s = RunningStats()\n",
    "        if self.config['is_norm_rew']:\n",
    "            self.rs_r = RunningStats()\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        # Train code\n",
    "        epsilon = max(\n",
    "            self.config['bottom_epsilon'], \n",
    "            self.config['top_epsilon'] - 0.01 * (self._iteration / self.config['anneal_epsilon_scaler'])\n",
    "        )\n",
    "\n",
    "        for e in range(num_episodes):\n",
    "\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            episode_reward = 0.0\n",
    "            done = False\n",
    "\n",
    "            self.q.eval()\n",
    "            while not done:\n",
    "                if self.rs_s is not None:\n",
    "                    self.rs_s += s\n",
    "                a = self.q.sample_action(\n",
    "                    obs=torch.from_numpy(self.rs_s.normalize(s) if self.rs_s is not None else s).float().to(self.device),\n",
    "                    epsilon=epsilon,\n",
    "                )\n",
    "                s_prime, r, done, trunc, info = self.env.step(a)\n",
    "                if self.rs_r is not None:\n",
    "                    self.rs_r += np.asarray([r])\n",
    "                done_mask = 0.0 if done else 1.0\n",
    "                self.memory.put(\n",
    "                    (\n",
    "                        self.rs_s.normalize(s) if self.rs_s is not None else s,\n",
    "                        a,\n",
    "                        self.rs_r.normalize(np.asarray([r])).item() if self.rs_r is not None else r,\n",
    "                        self.rs_s.normalize(s_prime) if self.rs_s is not None else s_prime,\n",
    "                        done_mask,\n",
    "                    )\n",
    "                )\n",
    "                s = s_prime\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    if self.rs_s is not None:\n",
    "                        self.rs_s += s_prime\n",
    "                    break\n",
    "\n",
    "            if self.memory.size() > self.config['start_learn_at_memory_size']:\n",
    "                for i in range(self.config['num_learn_epochs_per_eposide']):\n",
    "                    s, a, r, s_prime, done_mask = self.memory.sample(\n",
    "                        batch_size=self.config['batch_size'],\n",
    "                    )\n",
    "                    max_q_prime_estimator = self.q_target if self.config['is_q_target'] else self.q\n",
    "                    max_q_prime_estimator.eval()\n",
    "                    max_q_prime = max_q_prime_estimator(s_prime).max(1)[0].unsqueeze(1)\n",
    "                    target = r + self.config['gamma'] * max_q_prime * done_mask\n",
    "\n",
    "                    self.q.train()\n",
    "                    def loss_fn(outputs, a, target):\n",
    "                        q_a = outputs.gather(1, a)\n",
    "                        loss = (q_a - target).pow(2).sum() * 0.5\n",
    "                        return loss\n",
    "\n",
    "                    self.pc_trainer.train_on_batch(\n",
    "                        s, loss_fn,\n",
    "                        loss_fn_kwargs={\n",
    "                            'a': a,\n",
    "                            'target': target.detach() if self.config['is_detach_target'] else target,\n",
    "                        },\n",
    "                        **self.config['train_on_batch_kwargs'],\n",
    "                    )\n",
    "\n",
    "            if self.config['is_q_target']:\n",
    "                if self._iteration % self.config['interval_update_target_q'] == 0 and self._iteration != 0:\n",
    "                    self.q_target.load_state_dict(self.q.state_dict(), strict=False)\n",
    "\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            if len(self.episode_rewards) > self.config['interval_compute_episode_reward']:\n",
    "                self.episode_rewards.pop(0)\n",
    "\n",
    "            print(f'Episode {e}: {episode_reward}')\n",
    "            result_dict = {e: np.mean(self.episode_rewards)}\n",
    "            self._iteration += 1\n",
    "        return result_dict\n",
    "\n",
    "    def stop(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 9.0\n",
      "Episode 1: 9.0\n",
      "Episode 2: 10.0\n",
      "Episode 3: 9.0\n",
      "Episode 4: 10.0\n",
      "Episode 5: 9.0\n",
      "Episode 6: 11.0\n",
      "Episode 7: 9.0\n",
      "Episode 8: 9.0\n",
      "Episode 9: 9.0\n",
      "Episode 10: 9.0\n",
      "Episode 11: 9.0\n",
      "Episode 12: 12.0\n",
      "Episode 13: 22.0\n",
      "Episode 14: 9.0\n",
      "Episode 15: 13.0\n",
      "Episode 16: 24.0\n",
      "Episode 17: 10.0\n",
      "Episode 18: 10.0\n",
      "Episode 19: 10.0\n",
      "Episode 20: 10.0\n",
      "Episode 21: 26.0\n",
      "Episode 22: 13.0\n",
      "Episode 23: 10.0\n",
      "Episode 24: 10.0\n",
      "Episode 25: 10.0\n",
      "Episode 26: 10.0\n",
      "Episode 27: 10.0\n",
      "Episode 28: 11.0\n",
      "Episode 29: 12.0\n",
      "Episode 30: 11.0\n",
      "Episode 31: 15.0\n",
      "Episode 32: 13.0\n",
      "Episode 33: 10.0\n",
      "Episode 34: 10.0\n",
      "Episode 35: 13.0\n",
      "Episode 36: 10.0\n",
      "Episode 37: 10.0\n",
      "Episode 38: 10.0\n",
      "Episode 39: 10.0\n",
      "Episode 40: 11.0\n",
      "Episode 41: 12.0\n",
      "Episode 42: 12.0\n",
      "Episode 43: 11.0\n",
      "Episode 44: 14.0\n",
      "Episode 45: 13.0\n",
      "Episode 46: 21.0\n",
      "Episode 47: 14.0\n",
      "Episode 48: 12.0\n",
      "Episode 49: 12.0\n",
      "Episode 50: 14.0\n",
      "Episode 51: 14.0\n",
      "Episode 52: 13.0\n",
      "Episode 53: 13.0\n",
      "Episode 54: 14.0\n",
      "Episode 55: 21.0\n",
      "Episode 56: 15.0\n",
      "Episode 57: 14.0\n",
      "Episode 58: 13.0\n",
      "Episode 59: 17.0\n",
      "Episode 60: 15.0\n",
      "Episode 61: 13.0\n",
      "Episode 62: 18.0\n",
      "Episode 63: 17.0\n",
      "Episode 64: 13.0\n",
      "Episode 65: 14.0\n",
      "Episode 66: 13.0\n",
      "Episode 67: 15.0\n",
      "Episode 68: 14.0\n",
      "Episode 69: 16.0\n",
      "Episode 70: 14.0\n",
      "Episode 71: 17.0\n",
      "Episode 72: 17.0\n",
      "Episode 73: 17.0\n",
      "Episode 74: 18.0\n",
      "Episode 75: 18.0\n",
      "Episode 76: 17.0\n",
      "Episode 77: 19.0\n",
      "Episode 78: 16.0\n",
      "Episode 79: 14.0\n",
      "Episode 80: 14.0\n",
      "Episode 81: 14.0\n",
      "Episode 82: 14.0\n",
      "Episode 83: 17.0\n",
      "Episode 84: 13.0\n",
      "Episode 85: 13.0\n",
      "Episode 86: 16.0\n",
      "Episode 87: 17.0\n",
      "Episode 88: 35.0\n",
      "Episode 89: 19.0\n",
      "Episode 90: 38.0\n",
      "Episode 91: 41.0\n",
      "Episode 92: 44.0\n",
      "Episode 93: 77.0\n",
      "Episode 94: 80.0\n",
      "Episode 95: 50.0\n",
      "Episode 96: 93.0\n",
      "Episode 97: 98.0\n",
      "Episode 98: 100.0\n",
      "Episode 99: 103.0\n",
      "Episode 100: 106.0\n",
      "Episode 101: 108.0\n",
      "Episode 102: 60.0\n",
      "Episode 103: 118.0\n",
      "Episode 104: 69.0\n",
      "Episode 105: 79.0\n",
      "Episode 106: 144.0\n",
      "Episode 107: 155.0\n",
      "Episode 108: 165.0\n",
      "Episode 109: 174.0\n",
      "Episode 110: 187.0\n",
      "Episode 111: 199.0\n",
      "Episode 112: 32.0\n",
      "Episode 113: 84.0\n",
      "Episode 114: 38.0\n",
      "Episode 115: 35.0\n",
      "Episode 116: 74.0\n",
      "Episode 117: 43.0\n",
      "Episode 118: 31.0\n",
      "Episode 119: 82.0\n",
      "Episode 120: 27.0\n",
      "Episode 121: 90.0\n",
      "Episode 122: 82.0\n",
      "Episode 123: 48.0\n",
      "Episode 124: 195.0\n",
      "Episode 125: 184.0\n",
      "Episode 126: 305.0\n",
      "Episode 127: 504.0\n",
      "Episode 128: 506.0\n",
      "Episode 129: 505.0\n",
      "Episode 130: 385.0\n",
      "Episode 131: 530.0\n",
      "Episode 132: 839.0\n",
      "Episode 133: 395.0\n",
      "Episode 134: 458.0\n",
      "Episode 135: 391.0\n",
      "Episode 136: 401.0\n",
      "Episode 137: 461.0\n",
      "Episode 138: 756.0\n",
      "Episode 139: 771.0\n",
      "Episode 140: 699.0\n",
      "Episode 141: 639.0\n",
      "Episode 142: 1236.0\n",
      "Episode 143: 1013.0\n",
      "Episode 144: 805.0\n",
      "Episode 145: 571.0\n",
      "Episode 146: 883.0\n",
      "Episode 147: 1060.0\n",
      "Episode 148: 575.0\n",
      "Episode 149: 570.0\n",
      "Episode 150: 442.0\n",
      "Episode 151: 522.0\n",
      "Episode 152: 549.0\n",
      "Episode 153: 576.0\n",
      "Episode 154: 945.0\n",
      "Episode 155: 563.0\n",
      "Episode 156: 829.0\n",
      "Episode 157: 846.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2024\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     },\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m BaseTrainable(config)\n\u001b[0;32m---> 30\u001b[0m x\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10000\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 113\u001b[0m, in \u001b[0;36mBaseTrainable.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    110\u001b[0m             loss \u001b[38;5;241m=\u001b[39m (q_a \u001b[38;5;241m-\u001b[39m target)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpc_trainer\u001b[38;5;241m.\u001b[39mtrain_on_batch(\n\u001b[1;32m    114\u001b[0m             s, loss_fn,\n\u001b[1;32m    115\u001b[0m             loss_fn_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    116\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m: a,\n\u001b[1;32m    117\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: target\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_detach_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m target,\n\u001b[1;32m    118\u001b[0m             },\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_on_batch_kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_q_target\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_update_target_q\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/UCT/Research_Project/deep-predictive-active-inference/predictive_coding/predictive_coding/pc_trainer.py:804\u001b[0m, in \u001b[0;36mPCTrainer.train_on_batch\u001b[0;34m(self, inputs, loss_fn, loss_fn_kwargs, is_sample_x_at_batch_start, is_reset_optimizer_x_at_batch_start, is_reset_optimizer_p_at_batch_start, is_unwrap_inputs, is_optimize_inputs, callback_after_backward, callback_after_backward_kwargs, callback_after_t, callback_after_t_kwargs, is_log_progress, is_return_results_every_t, is_checking_after_callback_after_t, debug, backward_kwargs, is_clear_energy_after_use, is_return_outputs)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_p\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m overall\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackward_kwargs)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# callback_after_backward\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_after_backward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"seed\": 2024,\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    \"is_q_target\": False,\n",
    "    \"buffer_limit\": 1000,\n",
    "    \"is_norm_obs\": True,\n",
    "    \"is_norm_rew\": False,\n",
    "    'interval_compute_episode_reward': 200,\n",
    "    'top_epsilon': 0.08,\n",
    "    'bottom_epsilon': 0.01,\n",
    "    'anneal_epsilon_scaler': 200,\n",
    "    'start_learn_at_memory_size': 100,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 64,\n",
    "    'num_learn_epochs_per_eposide': 1,\n",
    "    'interval_update_target_q': 20,\n",
    "    'is_detach_target': True,\n",
    "    'interval_compute_episode_reward': 200,\n",
    "    'top_epsilon': 0.08,\n",
    "    'bottom_epsilon': 0.01,\n",
    "    'anneal_epsilon_scaler': 200,\n",
    "    'train_on_batch_kwargs': {\n",
    "        'is_log_progress': False,\n",
    "        'is_return_results_every_t': False,\n",
    "        'is_checking_after_callback_after_t': False,\n",
    "    },\n",
    "}\n",
    "x = BaseTrainable(config)\n",
    "x.train(10000)\n",
    "\n",
    "# 'PCTrainer_kwargs': {\n",
    "#     'update_x_at': 'all',\n",
    "#     'optimizer_x_fn': 'SGD',\n",
    "#     'optimizer_x_kwargs': {\n",
    "#         'lr': 0.05,\n",
    "#     },\n",
    "#     'x_lr_discount': 1.0,\n",
    "#     'x_lr_amplifier': 1.0,\n",
    "#     'update_p_at': 'last',\n",
    "#     'optimizer_p_fn': 'SGD',\n",
    "#     'optimizer_p_kwargs': {\n",
    "#         'lr': 0.05,\n",
    "#     },\n",
    "#     'T': \"self.config['T'] if self.config['predictive_coding'] else 1\",\n",
    "#     'plot_progress_at': \"[]\",\n",
    "# },"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
