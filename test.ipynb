{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import predictive_coding as pc\n",
    "\n",
    "# import analysis_utils as au\n",
    "\n",
    "\n",
    "# def plot(df, plot=\"Reward\"):\n",
    "\n",
    "#     df = au.nature_pre(df)\n",
    "\n",
    "#     groups = ['Env', 'Rule', 'pc_learning_rate']\n",
    "\n",
    "#     df = au.add_metric_per_group(\n",
    "#         df, groups,\n",
    "#         lambda df: (\n",
    "#             'mean per group', df['Mean of episode reward'].mean()\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     groups.pop(-1)\n",
    "\n",
    "#     df = au.select_rows_per_group(\n",
    "#         df, groups,\n",
    "#         lambda df: df['mean per group'] == df['mean per group'].max()\n",
    "#     )\n",
    "\n",
    "#     df = au.drop_cols(df, ['mean per group'])\n",
    "\n",
    "#     df = au.extract_plot(df, f'Episode {plot}', 'training_iteration')\n",
    "\n",
    "#     df = df[df['training_iteration'].isin(list(range(0, 10000, 100)))]\n",
    "\n",
    "#     g = au.nature_relplot_curve(\n",
    "#         data=df,\n",
    "#         x='training_iteration',\n",
    "#         y=f'Episode {plot}',\n",
    "#         hue='Rule', style='Rule',\n",
    "#         hue_order=['PC', 'BP'],\n",
    "#         style_order=['PC', 'BP'],\n",
    "#         col='Env',\n",
    "#         aspect=0.8,\n",
    "#         sharey=False\n",
    "#     )\n",
    "\n",
    "#     au.nature_post(g, is_grid=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "class RunningStats(object):\n",
    "    \"\"\"Computes running mean and standard deviation\n",
    "    Url: https://gist.github.com/wassname/a9502f562d4d3e73729dc5b184db2501\n",
    "    Adapted from:\n",
    "        *\n",
    "        <http://stackoverflow.com/questions/1174984/how-to-efficiently-\\\n",
    "calculate-a-running-standard-deviation>\n",
    "        * <http://mathcentral.uregina.ca/QQ/database/QQ.09.02/carlos1.html>\n",
    "        * <https://gist.github.com/fvisin/5a10066258e43cf6acfa0a474fcdb59f>\n",
    "\n",
    "    Usage:\n",
    "        rs = RunningStats()\n",
    "        for i in range(10):\n",
    "            rs += np.random.randn()\n",
    "            print(rs)\n",
    "        print(rs.mean, rs.std)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=0., m=None, s=None, per_dim=True):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.per_dim = per_dim\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0.\n",
    "\n",
    "    def push(self, x):\n",
    "        # process input\n",
    "        if self.per_dim:\n",
    "            self.update_params(x)\n",
    "        else:\n",
    "            for el in x.flatten():\n",
    "                self.update_params(el)\n",
    "\n",
    "    def update_params(self, x):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.m = x\n",
    "            self.s = 0.\n",
    "        else:\n",
    "            prev_m = self.m.copy()\n",
    "            self.m += (x - self.m) / self.n\n",
    "            self.s += (x - prev_m) * (x - self.m)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, RunningStats):\n",
    "            sum_ns = self.n + other.n\n",
    "            prod_ns = self.n * other.n\n",
    "            delta2 = (other.m - self.m) ** 2.\n",
    "            return RunningStats(sum_ns,\n",
    "                                (self.m * self.n + other.m * other.n) / sum_ns,\n",
    "                                self.s + other.s + delta2 * prod_ns / sum_ns)\n",
    "        else:\n",
    "            self.push(other)\n",
    "            return self\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.s / (self.n) if self.n else 0.0\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<RunningMean(mean={: 2.4f}, std={: 2.4f}, n={: 2f}, m={: 2.4f}, s={: 2.4f})>'.format(self.mean, self.std, self.n, self.m, self.s)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'mean={: 2.4f}, std={: 2.4f}'.format(self.mean, self.std)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (\n",
    "            x - self.mean\n",
    "        ) / (\n",
    "            self.std if np.all(self.std) else 1.0\n",
    "        )\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit, sample_to_device):\n",
    "\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "        self.sample_to_device = sample_to_device\n",
    "\n",
    "    def put(self, transition):\n",
    "\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float).to(self.sample_to_device), torch.tensor(a_lst).to(self.sample_to_device), \\\n",
    "            torch.Tensor(r_lst).to(self.sample_to_device), torch.tensor(s_prime_lst, dtype=torch.float).to(self.sample_to_device), \\\n",
    "            torch.Tensor(done_mask_lst).to(self.sample_to_device)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "\n",
    "    def __init__(self, predictive_coding, num_obs, num_act, bias=True, pc_layer_at='before_acf', hidden_size=128, num_hidden=1, acf='Sigmoid'):\n",
    "\n",
    "        super(Qnet, self).__init__()\n",
    "\n",
    "        self.predictive_coding = predictive_coding\n",
    "        self.num_act = num_act\n",
    "\n",
    "        model = []\n",
    "\n",
    "        # input layer\n",
    "        model.append(nn.Linear(num_obs, hidden_size, bias=bias))\n",
    "        if self.predictive_coding and pc_layer_at == 'before_acf':\n",
    "            model.append(pc.PCLayer())\n",
    "        model.append(eval('nn.{}()'.format(acf)))\n",
    "        if self.predictive_coding and pc_layer_at == 'after_acf':\n",
    "            model.append(pc.PCLayer())\n",
    "\n",
    "        for i in range(num_hidden):\n",
    "\n",
    "            # hidden layer\n",
    "            model.append(nn.Linear(hidden_size, hidden_size, bias=bias))\n",
    "            if self.predictive_coding and pc_layer_at == 'before_acf':\n",
    "                model.append(pc.PCLayer())\n",
    "            model.append(eval('nn.{}()'.format(acf)))\n",
    "            if self.predictive_coding and pc_layer_at == 'after_acf':\n",
    "                model.append(pc.PCLayer())\n",
    "\n",
    "        # output layer\n",
    "        model.append(nn.Linear(hidden_size, num_act, bias=bias))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.num_act - 1)\n",
    "\n",
    "        else:\n",
    "            return self.forward(obs).argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import predictive_coding as pc\n",
    "\n",
    "class BaseTrainable:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['device'])\n",
    "        self.seed = config['seed']\n",
    "        self.env = None\n",
    "        self.q = None\n",
    "        self.q_target = None\n",
    "        self.memory = None\n",
    "        self.pc_trainer = None\n",
    "        self.episode_rewards = []\n",
    "        self.rs_s = None\n",
    "        self.rs_r = None\n",
    "        self._iteration = 0\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # Setup code\n",
    "        self.env = gym.make(self.config['env'])\n",
    "\n",
    "\n",
    "        # create q\n",
    "        self.q = Qnet(predictive_coding=True, num_obs=self.env.observation_space.shape[0], num_act=self.env.action_space.n).to(self.device)\n",
    "\n",
    "        # create q_target\n",
    "        if self.config['is_q_target']:\n",
    "            self.q_target = Qnet(predictive_coding=True, num_obs=self.env.observation_space.shape[0], num_act=self.env.action_space.n).to(self.device)\n",
    "            self.q_target.load_state_dict(self.q.state_dict(), strict=False)\n",
    "        else:\n",
    "            self.q_target = None\n",
    "\n",
    "        # create memory\n",
    "        self.memory = ReplayBuffer(\n",
    "            buffer_limit=self.config['buffer_limit'],\n",
    "            sample_to_device=self.device,\n",
    "        )\n",
    "\n",
    "        # create pc_trainer\n",
    "        self.pc_trainer = pc.PCTrainer(model=self.q)\n",
    "\n",
    "        if self.config['is_norm_obs']:\n",
    "            self.rs_s = RunningStats()\n",
    "        if self.config['is_norm_rew']:\n",
    "            self.rs_r = RunningStats()\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        # Train code\n",
    "        epsilon = max(\n",
    "            self.config['bottom_epsilon'], \n",
    "            self.config['top_epsilon'] - 0.01 * (self._iteration / self.config['anneal_epsilon_scaler'])\n",
    "        )\n",
    "\n",
    "        for e in range(num_episodes):\n",
    "\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            episode_reward = 0.0\n",
    "            done = False\n",
    "\n",
    "            self.q.eval()\n",
    "            while not done:\n",
    "                if self.rs_s is not None:\n",
    "                    self.rs_s += s\n",
    "                a = self.q.sample_action(\n",
    "                    obs=torch.from_numpy(self.rs_s.normalize(s) if self.rs_s is not None else s).float().to(self.device),\n",
    "                    epsilon=epsilon,\n",
    "                )\n",
    "                s_prime, r, done, trunc, info = self.env.step(a)\n",
    "                if self.rs_r is not None:\n",
    "                    self.rs_r += np.asarray([r])\n",
    "                done_mask = 0.0 if done else 1.0\n",
    "                self.memory.put(\n",
    "                    (\n",
    "                        self.rs_s.normalize(s) if self.rs_s is not None else s,\n",
    "                        a,\n",
    "                        self.rs_r.normalize(np.asarray([r])).item() if self.rs_r is not None else r,\n",
    "                        self.rs_s.normalize(s_prime) if self.rs_s is not None else s_prime,\n",
    "                        done_mask,\n",
    "                    )\n",
    "                )\n",
    "                s = s_prime\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    if self.rs_s is not None:\n",
    "                        self.rs_s += s_prime\n",
    "                    break\n",
    "\n",
    "            if self.memory.size() > self.config['start_learn_at_memory_size']:\n",
    "                for i in range(self.config['num_learn_epochs_per_eposide']):\n",
    "                    s, a, r, s_prime, done_mask = self.memory.sample(\n",
    "                        batch_size=self.config['batch_size'],\n",
    "                    )\n",
    "                    max_q_prime_estimator = self.q_target if self.config['is_q_target'] else self.q\n",
    "                    max_q_prime_estimator.eval()\n",
    "                    max_q_prime = max_q_prime_estimator(s_prime).max(1)[0].unsqueeze(1)\n",
    "                    target = r + self.config['gamma'] * max_q_prime * done_mask\n",
    "\n",
    "                    self.q.train()\n",
    "                    def loss_fn(outputs, a, target):\n",
    "                        q_a = outputs.gather(1, a)\n",
    "                        loss = (q_a - target).pow(2).sum() * 0.5\n",
    "                        return loss\n",
    "\n",
    "                    self.pc_trainer.train_on_batch(\n",
    "                        s, loss_fn,\n",
    "                        loss_fn_kwargs={\n",
    "                            'a': a,\n",
    "                            'target': target.detach() if self.config['is_detach_target'] else target,\n",
    "                        },\n",
    "                        **self.config['train_on_batch_kwargs'],\n",
    "                    )\n",
    "\n",
    "            if self.config['is_q_target']:\n",
    "                if self._iteration % self.config['interval_update_target_q'] == 0 and self._iteration != 0:\n",
    "                    self.q_target.load_state_dict(self.q.state_dict(), strict=False)\n",
    "\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            if len(self.episode_rewards) > self.config['interval_compute_episode_reward']:\n",
    "                self.episode_rewards.pop(0)\n",
    "\n",
    "            print(f'Episode {e}}: {np.mean(self.episode_rewards)}')\n",
    "            result_dict = {e: np.mean(self.episode_rewards)}\n",
    "            self._iteration += 1\n",
    "        return result_dict\n",
    "\n",
    "    def stop(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackmontgomery/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/var/folders/qc/h4k546bs6j9bl76bcjwyw5cw0000gn/T/ipykernel_42873/2471445531.py:161: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.tensor(s_lst, dtype=torch.float).to(self.sample_to_device), torch.tensor(a_lst).to(self.sample_to_device), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.333333333333334\n",
      "Episode Reward: 9.25\n",
      "Episode Reward: 9.6\n",
      "Episode Reward: 9.5\n",
      "Episode Reward: 9.428571428571429\n",
      "Episode Reward: 9.375\n",
      "Episode Reward: 9.444444444444445\n",
      "Episode Reward: 9.4\n",
      "Episode Reward: 9.363636363636363\n",
      "Episode Reward: 9.416666666666666\n",
      "Episode Reward: 9.692307692307692\n",
      "Episode Reward: 9.642857142857142\n",
      "Episode Reward: 9.666666666666666\n",
      "Episode Reward: 9.8125\n",
      "Episode Reward: 10.176470588235293\n",
      "Episode Reward: 10.333333333333334\n",
      "Episode Reward: 10.263157894736842\n",
      "Episode Reward: 10.9\n",
      "Episode Reward: 11.523809523809524\n",
      "Episode Reward: 12.090909090909092\n",
      "Episode Reward: 12.08695652173913\n",
      "Episode Reward: 12.166666666666666\n",
      "Episode Reward: 12.48\n",
      "Episode Reward: 12.961538461538462\n",
      "Episode Reward: 14.222222222222221\n",
      "Episode Reward: 15.214285714285714\n",
      "Episode Reward: 16.20689655172414\n",
      "Episode Reward: 17.266666666666666\n",
      "Episode Reward: 18.35483870967742\n",
      "Episode Reward: 39.9375\n",
      "Episode Reward: 39.0\n",
      "Episode Reward: 41.26470588235294\n",
      "Episode Reward: 40.628571428571426\n",
      "Episode Reward: 44.69444444444444\n",
      "Episode Reward: 47.83783783783784\n",
      "Episode Reward: 52.60526315789474\n",
      "Episode Reward: 55.94871794871795\n",
      "Episode Reward: 56.4\n",
      "Episode Reward: 55.75609756097561\n",
      "Episode Reward: 55.404761904761905\n",
      "Episode Reward: 57.83720930232558\n",
      "Episode Reward: 60.63636363636363\n",
      "Episode Reward: 62.733333333333334\n",
      "Episode Reward: 64.93478260869566\n",
      "Episode Reward: 66.46808510638297\n",
      "Episode Reward: 67.64583333333333\n",
      "Episode Reward: 68.44897959183673\n",
      "Episode Reward: 69.08\n",
      "Episode Reward: 68.41176470588235\n",
      "Episode Reward: 70.3076923076923\n",
      "Episode Reward: 127.20754716981132\n",
      "Episode Reward: 141.33333333333334\n",
      "Episode Reward: 140.03636363636363\n",
      "Episode Reward: 139.64285714285714\n",
      "Episode Reward: 191.73684210526315\n",
      "Episode Reward: 190.20689655172413\n",
      "Episode Reward: 192.40677966101694\n",
      "Episode Reward: 192.25\n",
      "Episode Reward: 192.60655737704917\n",
      "Episode Reward: 191.40322580645162\n",
      "Episode Reward: 190.25396825396825\n",
      "Episode Reward: 189.46875\n",
      "Episode Reward: 188.63076923076923\n",
      "Episode Reward: 188.4848484848485\n",
      "Episode Reward: 188.5820895522388\n",
      "Episode Reward: 190.0441176470588\n",
      "Episode Reward: 187.68115942028984\n",
      "Episode Reward: 185.37142857142857\n",
      "Episode Reward: 186.29577464788733\n",
      "Episode Reward: 219.59722222222223\n",
      "Episode Reward: 216.72602739726028\n",
      "Episode Reward: 214.3108108108108\n",
      "Episode Reward: 219.30666666666667\n",
      "Episode Reward: 217.81578947368422\n",
      "Episode Reward: 217.63636363636363\n",
      "Episode Reward: 216.82051282051282\n",
      "Episode Reward: 215.32911392405063\n",
      "Episode Reward: 213.9375\n",
      "Episode Reward: 212.6172839506173\n",
      "Episode Reward: 211.5487804878049\n",
      "Episode Reward: 210.51807228915663\n",
      "Episode Reward: 209.60714285714286\n",
      "Episode Reward: 207.50588235294117\n",
      "Episode Reward: 207.1046511627907\n",
      "Episode Reward: 207.8505747126437\n",
      "Episode Reward: 208.0340909090909\n",
      "Episode Reward: 208.1123595505618\n",
      "Episode Reward: 209.63333333333333\n",
      "Episode Reward: 215.45054945054946\n",
      "Episode Reward: 217.34782608695653\n",
      "Episode Reward: 219.30107526881721\n",
      "Episode Reward: 220.96808510638297\n",
      "Episode Reward: 222.0421052631579\n",
      "Episode Reward: 223.3125\n",
      "Episode Reward: 225.23711340206185\n",
      "Episode Reward: 224.98979591836735\n",
      "Episode Reward: 224.46464646464648\n",
      "Episode Reward: 222.49\n",
      "Episode Reward: 222.40594059405942\n",
      "Episode Reward: 220.7156862745098\n",
      "Episode Reward: 244.9126213592233\n",
      "Episode Reward: 248.30769230769232\n",
      "Episode Reward: 248.95238095238096\n",
      "Episode Reward: 250.78301886792454\n",
      "Episode Reward: 253.01869158878506\n",
      "Episode Reward: 255.39814814814815\n",
      "Episode Reward: 254.67889908256882\n",
      "Episode Reward: 254.6727272727273\n",
      "Episode Reward: 256.6126126126126\n",
      "Episode Reward: 261.23214285714283\n",
      "Episode Reward: 260.34513274336285\n",
      "Episode Reward: 260.4035087719298\n",
      "Episode Reward: 261.4869565217391\n",
      "Episode Reward: 263.8103448275862\n",
      "Episode Reward: 263.2991452991453\n",
      "Episode Reward: 263.5169491525424\n",
      "Episode Reward: 267.8655462184874\n",
      "Episode Reward: 270.175\n",
      "Episode Reward: 288.9834710743802\n",
      "Episode Reward: 287.55737704918033\n",
      "Episode Reward: 586.2113821138212\n",
      "Episode Reward: 594.4596774193549\n",
      "Episode Reward: 669.48\n",
      "Episode Reward: 705.2380952380952\n",
      "Episode Reward: 702.3622047244095\n",
      "Episode Reward: 698.21875\n",
      "Episode Reward: 694.046511627907\n",
      "Episode Reward: 690.3076923076923\n",
      "Episode Reward: 687.0916030534352\n",
      "Episode Reward: 684.3787878787879\n",
      "Episode Reward: 684.593984962406\n",
      "Episode Reward: 681.4626865671642\n",
      "Episode Reward: 677.1925925925926\n",
      "Episode Reward: 673.6911764705883\n",
      "Episode Reward: 670.1605839416059\n",
      "Episode Reward: 1168.644927536232\n",
      "Episode Reward: 1242.1294964028777\n",
      "Episode Reward: 1236.45\n",
      "Episode Reward: 1248.7163120567375\n",
      "Episode Reward: 1255.119718309859\n",
      "Episode Reward: 1270.7902097902097\n",
      "Episode Reward: 1272.5277777777778\n",
      "Episode Reward: 1480.5655172413792\n",
      "Episode Reward: 1487.7534246575342\n",
      "Episode Reward: 1477.734693877551\n",
      "Episode Reward: 1467.8243243243244\n",
      "Episode Reward: 1494.5369127516778\n",
      "Episode Reward: 1647.9333333333334\n",
      "Episode Reward: 1642.1523178807947\n",
      "Episode Reward: 1651.8092105263158\n",
      "Episode Reward: 2199.516339869281\n",
      "Episode Reward: 2796.7077922077924\n",
      "Episode Reward: 2849.451612903226\n",
      "Episode Reward: 2837.4615384615386\n",
      "Episode Reward: 2886.331210191083\n",
      "Episode Reward: 3116.53164556962\n",
      "Episode Reward: 3102.4654088050315\n",
      "Episode Reward: 3116.375\n",
      "Episode Reward: 3485.0124223602484\n",
      "Episode Reward: 3465.5061728395062\n",
      "Episode Reward: 3450.9141104294476\n",
      "Episode Reward: 3439.1158536585367\n",
      "Episode Reward: 3422.151515151515\n",
      "Episode Reward: 3451.7289156626507\n",
      "Episode Reward: 3431.1616766467064\n",
      "Episode Reward: 3476.625\n",
      "Episode Reward: 3484.6331360946747\n",
      "Episode Reward: 3482.5470588235294\n",
      "Episode Reward: 3487.216374269006\n",
      "Episode Reward: 3593.313953488372\n",
      "Episode Reward: 3588.7976878612717\n",
      "Episode Reward: 3584.706896551724\n",
      "Episode Reward: 3580.845714285714\n",
      "Episode Reward: 3606.2897727272725\n",
      "Episode Reward: 3594.994350282486\n",
      "Episode Reward: 3576.016853932584\n",
      "Episode Reward: 3557.3072625698323\n",
      "Episode Reward: 3538.4333333333334\n",
      "Episode Reward: 3519.4696132596687\n",
      "Episode Reward: 3501.3736263736264\n",
      "Episode Reward: 3483.9890710382515\n",
      "Episode Reward: 3466.945652173913\n",
      "Episode Reward: 3522.2162162162163\n",
      "Episode Reward: 3504.1881720430106\n",
      "Episode Reward: 3492.4973262032086\n",
      "Episode Reward: 3476.398936170213\n",
      "Episode Reward: 4335.280423280423\n",
      "Episode Reward: 4319.221052631579\n",
      "Episode Reward: 4310.371727748691\n",
      "Episode Reward: 4289.59375\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"seed\": 2024,\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    \"is_q_target\": False,\n",
    "    \"buffer_limit\": 1000,\n",
    "    \"is_norm_obs\": True,\n",
    "    \"is_norm_rew\": False,\n",
    "    'interval_compute_episode_reward': 200,\n",
    "    'top_epsilon': 0.08,\n",
    "    'bottom_epsilon': 0.01,\n",
    "    'anneal_epsilon_scaler': 200,\n",
    "    'start_learn_at_memory_size': 100,\n",
    "    'gamma': 0.99,\n",
    "    'batch_size': 64,\n",
    "    'num_learn_epochs_per_eposide': 10,\n",
    "    'interval_update_target_q': 20,\n",
    "    'is_detach_target': True,\n",
    "    'interval_compute_episode_reward': 200,\n",
    "    'top_epsilon': 0.08,\n",
    "    'bottom_epsilon': 0.01,\n",
    "    'anneal_epsilon_scaler': 200,\n",
    "    'train_on_batch_kwargs': {\n",
    "        'is_log_progress': False,\n",
    "        'is_return_results_every_t': False,\n",
    "        'is_checking_after_callback_after_t': False,\n",
    "    },\n",
    "}\n",
    "x = BaseTrainable(config)\n",
    "x.train(10000)\n",
    "\n",
    "# 'PCTrainer_kwargs': {\n",
    "#     'update_x_at': 'all',\n",
    "#     'optimizer_x_fn': 'SGD',\n",
    "#     'optimizer_x_kwargs': {\n",
    "#         'lr': 0.05,\n",
    "#     },\n",
    "#     'x_lr_discount': 1.0,\n",
    "#     'x_lr_amplifier': 1.0,\n",
    "#     'update_p_at': 'last',\n",
    "#     'optimizer_p_fn': 'SGD',\n",
    "#     'optimizer_p_kwargs': {\n",
    "#         'lr': 0.05,\n",
    "#     },\n",
    "#     'T': \"self.config['T'] if self.config['predictive_coding'] else 1\",\n",
    "#     'plot_progress_at': \"[]\",\n",
    "# },"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
