{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import predictive_coding as pc\n",
    "import collections\n",
    "import random\n",
    "\n",
    "class QNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            pc.PCLayer(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        # options for the update of the latent state x\n",
    "        optimizer_x_fn = optim.SGD          # optimizer for latent state x, SGD perform gradient descent. Other alternative are Adam, RMSprop, etc. \n",
    "        optimizer_x_kwargs = {'lr': 0.01}   # optimizer parameters for latent state x to pass to the optimizer. The best learning rate will depend on the task and the optimiser. \n",
    "                                            # Other parameters such as momentum, weight_decay could also be set here with additional elements, e.g., \"momentum\": 0.9, \"weight_decay\": 0.01\n",
    "\n",
    "        # options for the update of the parameters p\n",
    "        update_p_at = 'all'                 # update parameters p at the last iteration, can be set to 'all' to implement ipc (https://arxiv.org/abs/2212.00720)\n",
    "        optimizer_p_fn = optim.Adam         # optimizer for parameters p\n",
    "        optimizer_p_kwargs = {'lr': 0.001}  # optimizer parameters for parameters p, 0.001 is a good starting point for Adam, but it should be adjusted for the task\n",
    "\n",
    "        T = 20\n",
    "\n",
    "        self.trainer = pc.PCTrainer(self.model, \n",
    "            T = 20, \n",
    "            optimizer_x_fn = optimizer_x_fn,\n",
    "            optimizer_x_kwargs = optimizer_x_kwargs,\n",
    "            update_p_at = update_p_at,   \n",
    "            optimizer_p_fn = optimizer_p_fn,\n",
    "            optimizer_p_kwargs = optimizer_p_kwargs,\n",
    "            plot_progress_at = [],\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, batch_size=128, max_size=1e4):\n",
    "        self.buffer = collections.deque(maxlen=int(max_size))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, observation, action, reward, done, next_observation):\n",
    "        transition = (observation, action, reward, done, next_observation)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def can_sample(self):\n",
    "        return len(self.buffer) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        transitions = random.sample(self.buffer, self.batch_size)\n",
    "        batch = list(zip(*transitions))\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env: gym.Env, \n",
    "            initial_epsilon: float, \n",
    "            min_epsilon: float, \n",
    "            epsilon_decay: float, \n",
    "            buffer_size: int,\n",
    "            batch_size: int,\n",
    "            gamma: float\n",
    "        ) -> None:\n",
    "\n",
    "        self.env    = env\n",
    "        self.gamma  = gamma\n",
    "\n",
    "        # Q-network\n",
    "        self.qnet           = QNet(env.observation_space.shape[0], 128, env.action_space.n)\n",
    "        self.target_network = QNet(env.observation_space.shape[0], 128, env.action_space.n)\n",
    "        self.target_network.load_state_dict(self.qnet.state_dict())\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        self.epsilon        = initial_epsilon\n",
    "        self.min_epsilon    = min_epsilon\n",
    "        self.epsilon_decay  = epsilon_decay\n",
    "\n",
    "        # Replay Memory\n",
    "        self.buffer_size    = buffer_size\n",
    "        self.batch_size     = batch_size\n",
    "        self.replay_buffer  = ReplayBuffer(batch_size, buffer_size)\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        self.qnet.eval()\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            return self.qnet(state).argmax().item()\n",
    "        \n",
    "    \n",
    "    def learn(self):\n",
    "            \n",
    "        if not self.replay_buffer.can_sample():\n",
    "            return\n",
    "\n",
    "        self.qnet.train()\n",
    "\n",
    "        batch = self.replay_buffer.sample()\n",
    "\n",
    "        observations        = torch.Tensor(batch[0])\n",
    "        actions             = torch.Tensor(batch[1])\n",
    "        rewards             = torch.Tensor(batch[2])\n",
    "        dones               = torch.Tensor(batch[3])\n",
    "        next_observations   = torch.Tensor(batch[4])\n",
    "\n",
    "        # Target:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = rewards + self.gamma * self.target_network(next_observations).max(dim=1).values * (1 - dones)\n",
    "        \n",
    "        # Loss:\n",
    "\n",
    "        def loss_fn(outputs, actions, target):\n",
    "            predicted = outputs.gather(1, actions.long().unsqueeze(1)).squeeze(1)\n",
    "            loss = (predicted - target).pow(2).sum() * 0.5\n",
    "            return loss\n",
    "\n",
    "        self.qnet.trainer.train_on_batch(\n",
    "        inputs=observations,\n",
    "        loss_fn=loss_fn,\n",
    "        loss_fn_kwargs = {\n",
    "            'actions': actions,\n",
    "            'target': target                    \n",
    "        }\n",
    "    )\n",
    "        \n",
    "    def load_target_network(self):\n",
    "        self.target_network.load_state_dict(self.qnet.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 completed - Reward 18.0 - Epsilon 0.798979448275733\n",
      "Episode 1 completed - Reward 18.0 - Epsilon 0.7847194125331725\n",
      "Episode 2 completed - Reward 40.0 - Epsilon 0.7539350353479782\n",
      "Episode 3 completed - Reward 10.0 - Epsilon 0.7464295217570214\n",
      "Episode 4 completed - Reward 12.0 - Epsilon 0.7375214679987703\n",
      "Episode 5 completed - Reward 30.0 - Epsilon 0.7157136715673376\n",
      "Episode 6 completed - Reward 20.0 - Epsilon 0.7015345712765669\n",
      "Episode 7 completed - Reward 15.0 - Epsilon 0.691084895594664\n",
      "Episode 8 completed - Reward 14.0 - Epsilon 0.6814723449173306\n",
      "Episode 9 completed - Reward 14.0 - Epsilon 0.6719934986967332\n",
      "Episode 10 completed - Reward 78.0 - Epsilon 0.6215458242302211\n",
      "Episode 11 completed - Reward 16.0 - Epsilon 0.6116753296042869\n",
      "Episode 12 completed - Reward 32.0 - Epsilon 0.5924020979840192\n",
      "Episode 13 completed - Reward 41.0 - Epsilon 0.5685931262319229\n",
      "Episode 14 completed - Reward 45.0 - Epsilon 0.5435613584374495\n",
      "Episode 15 completed - Reward 35.0 - Epsilon 0.5248566005766121\n",
      "Episode 16 completed - Reward 21.0 - Epsilon 0.5139441369219604\n",
      "Episode 17 completed - Reward 59.0 - Epsilon 0.48448431481121046\n",
      "Episode 18 completed - Reward 100.0 - Epsilon 0.4383576034409493\n",
      "Episode 19 completed - Reward 123.0 - Epsilon 0.3875998459020395\n",
      "Episode 20 completed - Reward 17.0 - Epsilon 0.3810630994529462\n",
      "Episode 21 completed - Reward 75.0 - Epsilon 0.35351554247137473\n",
      "Episode 22 completed - Reward 70.0 - Epsilon 0.3296041629966128\n",
      "Episode 23 completed - Reward 110.0 - Epsilon 0.29525441007435477\n",
      "Episode 24 completed - Reward 106.0 - Epsilon 0.2655450102253539\n",
      "Episode 25 completed - Reward 107.0 - Epsilon 0.23858623580046678\n",
      "Episode 26 completed - Reward 17.0 - Epsilon 0.2345625558476492\n",
      "Episode 27 completed - Reward 127.0 - Epsilon 0.20657401464697137\n",
      "Episode 28 completed - Reward 110.0 - Epsilon 0.18504586919282773\n",
      "Episode 29 completed - Reward 112.0 - Epsilon 0.16542992620971048\n",
      "Episode 30 completed - Reward 143.0 - Epsilon 0.14337680301152073\n",
      "Episode 31 completed - Reward 111.0 - Epsilon 0.12830633076244433\n",
      "Episode 32 completed - Reward 104.0 - Epsilon 0.11562689433595524\n",
      "Episode 33 completed - Reward 108.0 - Epsilon 0.10378428300365633\n",
      "Episode 34 completed - Reward 104.0 - Epsilon 0.09352815448221953\n",
      "Episode 35 completed - Reward 117.0 - Epsilon 0.08319639099276262\n",
      "Episode 36 completed - Reward 115.0 - Epsilon 0.07415417939340611\n",
      "Episode 37 completed - Reward 12.0 - Epsilon 0.07326920713925339\n",
      "Episode 38 completed - Reward 109.0 - Epsilon 0.06569914361785618\n",
      "Episode 39 completed - Reward 10.0 - Epsilon 0.06504510077302347\n",
      "Episode 40 completed - Reward 9.0 - Epsilon 0.06446203103409309\n",
      "Episode 41 completed - Reward 112.0 - Epsilon 0.05762867922323461\n",
      "Episode 42 completed - Reward 107.0 - Epsilon 0.05177807573320888\n",
      "Episode 43 completed - Reward 130.0 - Epsilon 0.04546313455172459\n",
      "Episode 44 completed - Reward 116.0 - Epsilon 0.04048144533328125\n",
      "Episode 45 completed - Reward 115.0 - Epsilon 0.036081713684066374\n",
      "Episode 46 completed - Reward 113.0 - Epsilon 0.03222458443981226\n",
      "Episode 47 completed - Reward 94.0 - Epsilon 0.029332104174833395\n",
      "Episode 48 completed - Reward 98.0 - Epsilon 0.026592616155405157\n",
      "Episode 49 completed - Reward 87.0 - Epsilon 0.024375781065625803\n",
      "Episode 50 completed - Reward 102.0 - Epsilon 0.022010927312380666\n",
      "Episode 51 completed - Reward 112.0 - Epsilon 0.01967764045194677\n",
      "Episode 52 completed - Reward 115.0 - Epsilon 0.017538973298006914\n",
      "Episode 53 completed - Reward 123.0 - Epsilon 0.015508122350849633\n",
      "Episode 54 completed - Reward 141.0 - Epsilon 0.013467688367211902\n",
      "Episode 55 completed - Reward 14.0 - Epsilon 0.013280361400928021\n",
      "Episode 56 completed - Reward 118.0 - Epsilon 0.011801507980220014\n",
      "Episode 57 completed - Reward 125.0 - Epsilon 0.010414142899824298\n",
      "Episode 58 completed - Reward 120.0 - Epsilon 0.009235961624686546\n",
      "Episode 59 completed - Reward 97.0 - Epsilon 0.00838174564915221\n",
      "Episode 60 completed - Reward 121.0 - Epsilon 0.007426061695734422\n",
      "Episode 61 completed - Reward 120.0 - Epsilon 0.006585930450936582\n",
      "Episode 62 completed - Reward 131.0 - Epsilon 0.005776916599570464\n",
      "Episode 63 completed - Reward 128.0 - Epsilon 0.005082514082827744\n",
      "Episode 64 completed - Reward 131.0 - Epsilon 0.005\n",
      "Episode 65 completed - Reward 122.0 - Epsilon 0.005\n",
      "Episode 66 completed - Reward 126.0 - Epsilon 0.005\n",
      "Episode 67 completed - Reward 124.0 - Epsilon 0.005\n",
      "Episode 68 completed - Reward 135.0 - Epsilon 0.005\n",
      "Episode 69 completed - Reward 130.0 - Epsilon 0.005\n",
      "Episode 70 completed - Reward 123.0 - Epsilon 0.005\n",
      "Episode 71 completed - Reward 130.0 - Epsilon 0.005\n",
      "Episode 72 completed - Reward 138.0 - Epsilon 0.005\n",
      "Episode 73 completed - Reward 130.0 - Epsilon 0.005\n",
      "Episode 74 completed - Reward 122.0 - Epsilon 0.005\n",
      "Episode 75 completed - Reward 131.0 - Epsilon 0.005\n",
      "Episode 76 completed - Reward 126.0 - Epsilon 0.005\n",
      "Episode 77 completed - Reward 135.0 - Epsilon 0.005\n",
      "Episode 78 completed - Reward 124.0 - Epsilon 0.005\n",
      "Episode 79 completed - Reward 124.0 - Epsilon 0.005\n",
      "Episode 80 completed - Reward 105.0 - Epsilon 0.005\n",
      "Episode 81 completed - Reward 131.0 - Epsilon 0.005\n",
      "Episode 82 completed - Reward 132.0 - Epsilon 0.005\n",
      "Episode 83 completed - Reward 112.0 - Epsilon 0.005\n",
      "Episode 84 completed - Reward 109.0 - Epsilon 0.005\n",
      "Episode 85 completed - Reward 113.0 - Epsilon 0.005\n",
      "Episode 86 completed - Reward 291.0 - Epsilon 0.005\n",
      "Episode 87 completed - Reward 109.0 - Epsilon 0.005\n",
      "Episode 88 completed - Reward 103.0 - Epsilon 0.005\n",
      "Episode 89 completed - Reward 101.0 - Epsilon 0.005\n",
      "Episode 90 completed - Reward 105.0 - Epsilon 0.005\n",
      "Episode 91 completed - Reward 17.0 - Epsilon 0.005\n",
      "Episode 92 completed - Reward 20.0 - Epsilon 0.005\n",
      "Episode 93 completed - Reward 315.0 - Epsilon 0.005\n",
      "Episode 94 completed - Reward 373.0 - Epsilon 0.005\n",
      "Episode 95 completed - Reward 298.0 - Epsilon 0.005\n",
      "Episode 96 completed - Reward 303.0 - Epsilon 0.005\n",
      "Episode 97 completed - Reward 35.0 - Epsilon 0.005\n",
      "Episode 98 completed - Reward 342.0 - Epsilon 0.005\n",
      "Episode 99 completed - Reward 345.0 - Epsilon 0.005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(env, 0.9, 0.001, 0.999, 1000, 64, 0.99)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "action = agent.act(torch.from_numpy(obs))\n",
    "\n",
    "next_obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "agent.replay_buffer.add(obs, action, reward, termination, next_obs)\n",
    "\n",
    "next_obs = obs \n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    action = agent.act(torch.from_numpy(obs))\n",
    "\n",
    "    next_obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "    agent.replay_buffer.add(obs, action, reward, termination, next_obs)\n",
    "\n",
    "    if termination or truncation:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "agent.learn()\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action = agent.act(torch.from_numpy(obs))\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        agent.replay_buffer.add(obs, action, reward, done, next_obs)\n",
    "        obs = next_obs\n",
    "        agent.learn()\n",
    "    agent.load_target_network()\n",
    "    print(f'Episode {episode} completed - Reward {episode_rewards} - Epsilon {agent.epsilon}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
