@article{smith2022,
title = {A step-by-step tutorial on active inference and its application to empirical data},
journal = {Journal of Mathematical Psychology},
volume = {107},
pages = {102632},
year = {2022},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2021.102632},
url = {https://www.sciencedirect.com/science/article/pii/S0022249621000973},
author = {Ryan Smith and Karl J. Friston and Christopher J. Whyte},
}

@article{bogacz2017tutorial,
  title={A tutorial on the free-energy framework for modelling perception and learning},
  author={Bogacz, Rafal},
  journal={Journal of mathematical psychology},
  volume={76},
  pages={198--211},
  year={2017},
  publisher={Elsevier}
}

@article{himst2020deep,
  title={Deep active inference for partially observable MDPs},
  author={Himst, O and Lanillos, PL},
  year={2020},
  publisher={Cham: Springer}
}

@article{millidge2020deep,
  title={Deep active inference as variational policy gradients},
  author={Millidge, Beren},
  journal={Journal of Mathematical Psychology},
  volume={96},
  pages={102348},
  year={2020},
  publisher={Elsevier}
}

@article{dacosta2020active,
title = {Active inference on discrete state-spaces: A synthesis},
journal = {Journal of Mathematical Psychology},
volume = {99},
pages = {102447},
year = {2020},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2020.102447},
url = {https://www.sciencedirect.com/science/article/pii/S0022249620300857},
author = {Lancelot {Da Costa} and Thomas Parr and Noor Sajid and Sebastijan Veselic and Victorita Neacsu and Karl Friston},
keywords = {Active inference, Free energy principle, Process theory, Variational Bayesian inference, Markov decision process, Mathematical review}
}


@article{fountas2020deep,
  title={Deep active inference agents using Monte-Carlo methods},
  author={Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro and Friston, Karl},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11662--11675},
  year={2020}
}

@article{friston2006free,
  title={A free energy principle for the brain},
  author={Friston, Karl and Kilner, James and Harrison, Lee},
  journal={Journal of physiology-Paris},
  volume={100},
  number={1-3},
  pages={70--87},
  year={2006},
  publisher={Elsevier}
}

@article{friston2017active,
  title={Active inference: a process theory},
  author={Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
  journal={Neural computation},
  volume={29},
  number={1},
  pages={1--49},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info}
}

@article{winn2005variational,
  title={Variational message passing.},
  author={Winn, John and Bishop, Christopher M and Jaakkola, Tommi},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={4},
  year={2005}
}

@article{parr2019neuronal,
  title={Neuronal message passing using Mean-field, Bethe, and Marginal approximations},
  author={Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J and Friston, Karl J},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={1889},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{fountas2020deep,
  title={Deep active inference agents using Monte-Carlo methods},
  author={Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro and Friston, Karl},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11662--11675},
  year={2020}
}

@article{sajid2021active,
  title={Active inference: demystified and compared},
  author={Sajid, Noor and Ball, Philip J and Parr, Thomas and Friston, Karl J},
  journal={Neural computation},
  volume={33},
  number={3},
  pages={674--712},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{millidge2021applications,
  title={Applications of the free energy principle to machine learning and neuroscience},
  author={Millidge, Beren},
  journal={arXiv preprint arXiv:2107.00140},
  year={2021}
}

@article{ororbia2024review,
  title={A review of neuroscience-inspired machine learning},
  author={Ororbia, Alexander and Mali, Ankur and Kohan, Adam and Millidge, Beren and Salvatori, Tommaso},
  journal={arXiv preprint arXiv:2403.18929},
  year={2024}
}

@article{emde2024stable,
  title={A Stable ‚Fast ‚and Fully Automatic Learning Algorithm for Predictive Coding Networks},
  author={Emde, Cornelius and Lukasiewicz, Thomas and Salvatori, Tommaso and Sha, Lei and Song, Yuhang and Xu, Zhenghua and Yordanov, Yordan},
  year={2024}
}

@article{whittington2017approximation,
  title={An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity},
  author={Whittington, James CR and Bogacz, Rafal},
  journal={Neural computation},
  volume={29},
  number={5},
  pages={1229--1262},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{millidge2022beyond,
  title={Predictive coding: Towards a future of deep learning beyond backpropagation?},
  author={Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Bogacz, Rafal and Lukasiewicz, Thomas},
  journal={arXiv preprint arXiv:2202.09467},
  year={2022}
}

millidge2022predictive

@article{song2024inferring,
  title={Inferring neural activity before plasticity as a foundation for learning beyond backpropagation},
  author={Song, Yuhang and Millidge, Beren and Salvatori, Tommaso and Lukasiewicz, Thomas and Xu, Zhenghua and Bogacz, Rafal},
  journal={Nature neuroscience},
  volume={27},
  number={2},
  pages={348--358},
  year={2024},
  publisher={Nature Publishing Group US New York}
}

@article{rao1999predictive,
  title={Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
  author={Rao, Rajesh PN and Ballard, Dana H},
  journal={Nature neuroscience},
  volume={2},
  number={1},
  pages={79--87},
  year={1999},
  publisher={Nature Publishing Group}
}

@article{millidge2019implementing,
  title={Implementing predictive processing and active inference: Preliminary steps and results},
  author={Millidge, Beren},
  year={2019},
  publisher={PsyArXiv}
}

@article{friston2003learning,
  title={Learning and inference in the brain},
  author={Friston, Karl},
  journal={Neural Networks},
  volume={16},
  number={9},
  pages={1325--1352},
  year={2003},
  publisher={Elsevier}
}

@article{friston2008hierarchical,
  title={Hierarchical models in the brain},
  author={Friston, Karl},
  journal={PLoS computational biology},
  volume={4},
  number={11},
  pages={e1000211},
  year={2008},
  publisher={Public Library of Science San Francisco, USA}
}

@article{friston2023free,
title = {The free energy principle made simpler but not too simple},
journal = {Physics Reports},
volume = {1024},
pages = {1-29},
year = {2023},
note = {The free energy principle made simpler but not too simple},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2023.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S037015732300203X},
author = {Karl Friston and Lancelot {Da Costa} and Noor Sajid and Conor Heins and Kai Ueltzhöffer and Grigorios A. Pavliotis and Thomas Parr},
keywords = {Self-organisation, Nonequilibrium, Variational inference, Bayesian, Markov blanket}
}

@article{friston2012active,
  title={Active inference and agency: optimal control without cost functions},
  author={Friston, Karl and Samothrakis, Spyridon and Montague, Read},
  journal={Biological cybernetics},
  volume={106},
  pages={523--541},
  year={2012},
  publisher={Springer}
}

@article{friston2020relationship,
  title={The relationship between dynamic programming and active inference: the discrete, finite-horizon case},
  author={Friston, Karl and Smith, Ryan},
  journal={arXiv preprint arXiv:2009.08111},
  year={2020}
}

@article{friston2005theory,
  title={A theory of cortical responses},
  author={Friston, Karl},
  journal={Philosophical transactions of the Royal Society B: Biological sciences},
  volume={360},
  number={1456},
  pages={815--836},
  year={2005},
  publisher={The Royal Society London}
}

@article{song2020can,
  title={Can the brain do backpropagation?---exact implementation of backpropagation in predictive coding networks},
  author={Song, Yuhang and Lukasiewicz, Thomas and Xu, Zhenghua and Bogacz, Rafal},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22566--22579},
  year={2020}
}

@article{millidge2022predictive,
  title={Predictive coding approximates backprop along arbitrary computation graphs},
  author={Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L},
  journal={Neural Computation},
  volume={34},
  number={6},
  pages={1329--1368},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{buckley2017free,
  title={The free energy principle for action and perception: A mathematical review},
  author={Buckley, Christopher L and Kim, Chang Sub and McGregor, Simon and Seth, Anil K},
  journal={Journal of mathematical psychology},
  volume={81},
  pages={55--79},
  year={2017},
  publisher={Elsevier}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@article{hodson2023empirical,
  title={The empirical status of predictive coding and active inference},
  author={Hodson, Rowan and Mehta, Marishka and Smith, Ryan},
  journal={Neuroscience \& Biobehavioral Reviews},
  pages={105473},
  year={2023},
  publisher={Elsevier}
}

@article{barto1983neuronlike,
  author={Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Neuronlike adaptive elements that can solve difficult learning control problems}, 
  year={1983},
  volume={SMC-13},
  number={5},
  pages={834-846},
  keywords={Adaptive systems;Problem-solving;Training;Pattern recognition;Neurons;Supervised learning;Biological neural networks},
  doi={10.1109/TSMC.1983.6313077}
 }
 
 @article{towers2024gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

@book{pearl1988Probabilistic,
author = {Pearl, Judea},
title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
year = {1988},
isbn = {1558604790},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA}
}

@book{parr2022ActiveInference,
    author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
    title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior},
    publisher = {The MIT Press},
    year = {2022},
    month = {03},
    isbn = {9780262369978},
    doi = {10.7551/mitpress/12441.001.0001},
    url = {https://doi.org/10.7551/mitpress/12441.001.0001}
}

@book{doya2007bayesian,
    author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.},
    title = {Bayesian Brain: Probabilistic Approaches to Neural Coding},
    publisher = {The MIT Press},
    year = {2007},
    isbn = {9780262294188},
    doi = {10.7551/mitpress/9780262042383.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001}
}

@article{helmholtz1867concerning,
  title={Concerning the perceptions in general},
  author={Helmholtz, Hermann von},
  year={1867},
  publisher={Appleton Century Crofts}
}

@article{friston2012free,
  title={A free energy principle for biological systems},
  author={Friston, Karl},
  journal={Entropy},
  volume={14},
  number={11},
  pages={2100-2121},
  year={2012},
  publisher={MDPI}
}

@article{millidge2024temporal,
    doi = {10.1371/journal.pcbi.1011183},
    author = {Millidge, Beren AND Tang, Mufeng AND Osanlouy, Mahyar AND Harper, Nicol S. AND Bogacz, Rafal},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Predictive coding networks for temporal prediction},
    year = {2024},
    month = {04},
    volume = {20},
    url = {https://doi.org/10.1371/journal.pcbi.1011183},
    pages = {1-31},
    abstract = {One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction.},
    number = {4},

}

@article{friston2010filtering,
author = {Friston, Karl and Klaas, Stephan and Li, Baojuan and Jean, Daunizeau},
year = {2010},
month = {06},
pages = {},
title = {Generalised Filtering},
volume = {2010},
journal = {Mathematical Problems in Engineering},
doi = {10.1155/2010/621670}
}

@article{rao2023active,
    author = {Rao, Rajesh P. N. and Gklezakos, Dimitrios C. and Sathish, Vishwas},
    title = {Active Predictive Coding: A Unifying Neural Model for Active Perception, Compositional Learning, and Hierarchical Planning},
    journal = {Neural Computation},
    volume = {36},
    number = {1},
    pages = {1-32},
    year = {2023},
    month = {12},
    abstract = {There is growing interest in predictive coding as a model of how the brain learns through predictions and prediction errors. Predictive coding models have traditionally focused on sensory coding and perception. Here we introduce active predictive coding (APC) as a unifying model for perception, action, and cognition. The APC model addresses important open problems in cognitive science and AI, including (1) how we learn compositional representations (e.g., part-whole hierarchies for equivariant vision) and (2) how we solve large-scale planning problems, which are hard for traditional reinforcement learning, by composing complex state dynamics and abstract actions from simpler dynamics and primitive actions. By using hypernetworks, self-supervised learning, and reinforcement learning, APC learns hierarchical world models by combining task-invariant state transition networks and task-dependent policy networks at multiple abstraction levels. We illustrate the applicability of the APC model to active visual perception and hierarchical planning. Our results represent, to our knowledge, the first proof-of-concept demonstration of a unified approach to addressing the part-whole learning problem in vision, the nested reference frames learning problem in cognition, and the integrated state-action hierarchy learning problem in reinforcement learning.},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01627},
    url = {https://doi.org/10.1162/neco\_a\_01627},
    eprint = {https://direct.mit.edu/neco/article-pdf/36/1/1/2195582/neco\_a\_01627.pdf},
}
@article{millidge2019combining,
  title={Combining active inference and hierarchical predictive coding: A tutorial introduction and case study},
  author={Millidge, Beren},
  year={2019},
  publisher={PsyArXiv}
}

@article{podusenko2022aida,
  author = {Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries, Bert},
  title = {{AIDA: An Active Inference-Based Design Agent for Audio Processing Algorithms}},
  journal = {Frontiers in Signal Processing},
  volume = {2},
  year = {2022},
  doi = {10.3389/frsip.2022.842477},
  url = {https://www.frontiersin.org/journals/signal-processing/articles/10.3389/frsip.2022.842477},
  issn = {2673-8198}
}

@article{lanillos2021active,
  title={Active Inference in Robotics and Artificial Agents: Survey and Challenges},
  author={Pablo Lanillos and Cristian Meo and Corrado Pezzato and Ajith Anil Meera and Mohamed Baioumy and Wataru Ohata and Alexander Tschantz and Beren Millidge and Martijn Wisse and Christopher L. Buckley and Jun Tani},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.01871},
  url={https://api.semanticscholar.org/CorpusID:244896517}
}

@article{champion2021realizing,
    author = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
    title = {Realizing Active Inference in Variational Message Passing: The Outcome-Blind Certainty Seeker},
    journal = {Neural Computation},
    volume = {33},
    number = {10},
    pages = {2762-2826},
    year = {2021},
    month = {09},
    abstract = {Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in AI. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and AI research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01422},
    url = {https://doi.org/10.1162/neco\_a\_01422},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/10/2762/1963368/neco\_a\_01422.pdf},
}

@article{hohwy2016evidencing,
author = {Hohwy, Jakob},
title = {The Self-Evidencing Brain},
journal = {Noûs},
volume = {50},
number = {2},
pages = {259-285},
doi = {https://doi.org/10.1111/nous.12062},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nous.12062},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/nous.12062},
abstract = {Abstract An exciting theory in neuroscience is that the brain is an organ for prediction error minimization (PEM). This theory is rapidly gaining influence and is set to dominate the science of mind and brain in the years to come. PEM has extreme explanatory ambition, and profound philosophical implications. Here, I assume the theory, briefly explain it, and then I argue that PEM implies that the brain is essentially self-evidencing. This means it is imperative to identify an evidentiary boundary between the brain and its environment. This boundary defines the mind-world relation, opens the door to skepticism, and makes the mind transpire as more inferentially secluded and neurocentrically skull-bound than many would nowadays think. Therefore, PEM somewhat deflates contemporary hypotheses that cognition is extended, embodied and enactive; however, it can nevertheless accommodate the kinds of cases that fuel these hypotheses.},
year = {2016}
}

@article{knill2004bayesian,
  title={The Bayesian brain: the role of uncertainty in neural coding and computation},
  author={Knill, David C and Pouget, Alexandre},
  journal={TRENDS in Neurosciences},
  volume={27},
  number={12},
  pages={712--719},
  year={2004},
  publisher={Elsevier}
}

@book{helmholtz1910treatise,
  title={Treatise on Physiological Optics, Volume III},
  author={Helmholtz, Hermann Von},
  volume={3},
  year={1910},
  publisher={Courier Corporation}
}

@article{barlow1961possible,
author = {Barlow, Horace},
year = {1961},
month = {01},
pages = {},
title = {Possible Principles Underlying the Transformations of Sensory Messages},
volume = {1},
isbn = {9780262518420},
journal = {Sensory Communication},
doi = {10.7551/mitpress/9780262518420.003.0013}
}

@article{barlow1972doctrine,
  author = {H. B. Barlow},
  title = {Single Units and Sensation: A Neuron Doctrine for Perceptual Psychology?},
  journal = {Perception},
  volume = {1},
  number = {4},
  pages = {371--394},
  year = {1972},
  doi = {10.1068/p010371},
  note = {PMID: 4377168},
  URL = {https://doi.org/10.1068/p010371},
  eprint = {https://doi.org/10.1068/p010371},
  abstract = {
    The problem discussed is the relationship between the firing of single neurons 
    in sensory pathways and subjectively experienced sensations. The conclusions 
    are formulated as the following five dogmas: 
    To understand nervous function one needs to look at interactions at a cellular 
    level, rather than either a more macroscopic or microscopic level, because 
    behavior depends upon the organized pattern of these intercellular interactions. 
    The sensory system is organized to achieve as complete a representation of the 
    sensory stimulus as possible with the minimum number of active neurons. 
    Trigger features of sensory neurons are matched to redundant patterns of 
    stimulation by experience as well as by developmental processes. 
    Perception corresponds to the activity of a small selection from the very 
    numerous high-level neurons, each of which corresponds to a pattern of 
    external events of the order of complexity of the events symbolized by a word. 
    High impulse frequency in such neurons corresponds to high certainty that the 
    trigger feature is present. 
    The development of the concepts leading up to these speculative dogmas, 
    their experimental basis, and some of their limitations are discussed.
  }
}

@article{klint2010sensitive,
author = {Klint, Richard and Mazzaro, Nazarena and Nielsen, Jens and Sinkjaer, Thomas and Grey, Michael},
year = {2010},
month = {03},
pages = {2747-56},
title = {Load Rather Than Length Sensitive Feedback Contributes to Soleus Muscle Activity During Human Treadmill Walking},
volume = {103},
journal = {Journal of neurophysiology},
doi = {10.1152/jn.00547.2009}
}

@article{aggelopoulos2015perceptual,
title = {Perceptual inference},
journal = {Neuroscience \& Biobehavioral Reviews},
volume = {55},
pages = {375-392},
year = {2015},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S014976341500127X},
author = {Nikolaos C. Aggelopoulos},
keywords = {Perception, Inference, Vision, Sensorimotor, Reflex, Active inference, Bayesian inference},
abstract = {Perceptual inference refers to the ability to infer sensory stimuli from predictions that result from internal neural representations built through prior experience. Methods of Bayesian statistical inference and decision theory model cognition adequately by using error sensing either in guiding action or in “generative” models that predict the sensory information. In this framework, perception can be seen as a process qualitatively distinct from sensation, a process of information evaluation using previously acquired and stored representations (memories) that is guided by sensory feedback. The stored representations can be utilised as internal models of sensory stimuli enabling long term associations, for example in operant conditioning. Evidence for perceptual inference is contributed by such phenomena as the cortical co-localisation of object perception with object memory, the response invariance in the responses of some neurons to variations in the stimulus, as well as from situations in which perception can be dissociated from sensation. In the context of perceptual inference, sensory areas of the cerebral cortex that have been facilitated by a priming signal may be regarded as comparators in a closed feedback loop, similar to the better known motor reflexes in the sensorimotor system. The adult cerebral cortex can be regarded as similar to a servomechanism, in using sensory feedback to correct internal models, producing predictions of the outside world on the basis of past experience.}
}

@article{penfield1961activation,
  title={Activation of the record of human experience: Summary of the lister oration delivered at the Royal College of Surgeons of England* on 27th April 1961},
  author={Penfield, Wilder},
  journal={Annals of the Royal College of Surgeons of England},
  volume={29},
  number={2},
  pages={77},
  year={1961},
  publisher={Royal College of Surgeons of England}
}

@article{sterpenich2007sleep,
  title={Sleep-related hippocampo-cortical interplay during emotional memory recollection},
  author={Sterpenich, Virginie and Albouy, Genevi{\`e}ve and Boly, M{\'e}lanie and Vandewalle, Gilles and Darsaud, Annabelle and Balteau, Evelyne and Dang-Vu, Thien Thanh and Desseilles, Martin and D'Argembeau, Arnaud and Gais, Steffen and others},
  journal={PLoS biology},
  volume={5},
  number={11},
  pages={e282},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

@article{ishai2000distributed,
title = {Distributed Neural Systems for the Generation of Visual Images},
journal = {Neuron},
volume = {28},
number = {3},
pages = {979-990},
year = {2000},
issn = {0896-6273},
doi = {https://doi.org/10.1016/S0896-6273(00)00168-9},
url = {https://www.sciencedirect.com/science/article/pii/S0896627300001689},
author = {Alumit Ishai and Leslie G. Ungerleider and James V. Haxby},
abstract = {Visual perception of houses, faces, and chairs evoke differential responses in ventral temporal cortex. Using fMRI, we compared activations evoked by perception and imagery of these object categories. We found content-related activation during imagery in extrastriate cortex, but this activity was restricted to small subsets of the regions that showed category-related activation during perception. Within ventral temporal cortex, activation during imagery evoked stronger responses on the left whereas perception evoked stronger responses on the right. Additionally, visual imagery evoked activity in parietal and frontal cortex, but this activity was not content related. These results suggest that content-related activation during imagery in visual extrastriate cortex may be implemented by “top-down” mechanisms in parietal and frontal cortex that mediate the retrieval of face and object representations from long-term memory and their maintenance through visual imagery.}
}

@article{miall1996forward,
title = {Forward Models for Physiological Motor Control},
journal = {Neural Networks},
volume = {9},
number = {8},
pages = {1265-1279},
year = {1996},
note = {Four Major Hypotheses in Neuroscience},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(96)00035-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608096000354},
author = {R.C. Miall and D.M. Wolpert},
keywords = {Motor control},
abstract = {Based on theoretical and computational studies it has been suggested that the central nervous system (CNS) internally simulates the behaviour of the motor system in planning, control and learning. Such an internal “forward” model is a representation of the motor system that uses the current state of the motor system and motor command to predict the next state. We will outline the uses of such internal models for solving several fundamental computational problems in motor control and then review the evidence for their existence and use by the CNS. Finally we speculate how the location of an internal model within the CNS may be identified. Copyright © 1996 Elsevier Science Ltd.}
}

@article{welniarz2021forward,
  author = {Quentin Welniarz and Yulia Worbe and Cecile Gallea},
  title = {The Forward Model: A Unifying Theory for the Role of the Cerebellum in Motor Control and Sense of Agency},
  journal = {Frontiers in Systems Neuroscience},
  volume = {15},
  year = {2021},
  url = {https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/fnsys.2021.644059},
  doi = {10.3389/fnsys.2021.644059},
  issn = {1662-5137}
 }
 
 @inproceedings{salvatori2024a,
title={A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks},
author={Tommaso Salvatori and Yuhang Song and Yordan Yordanov and Beren Millidge and Lei Sha and Cornelius Emde and Zhenghua Xu and Rafal Bogacz and Thomas Lukasiewicz},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=RyUvzda8GH}
}

@article{millidge2019combining,
  title={Combining Active Inference and Hierarchical Predictive Coding: A Tutorial Introduction and Case Study},
  author={Millidge, Beren},
  year={2019},
  journal={PsyArXiv},
  month={March 11},
  doi={10.31234/osf.io/kf6wc}
}

@inproceedings{florian2005CorrectEF,
  title={Correct equations for the dynamics of the cart-pole system},
  author={Razvan V. Florian},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:13144387}
}

@misc{mishkin2016needgoodinit,
      title={All you need is a good init}, 
      author={Dmytro Mishkin and Jiri Matas},
      year={2016},
      eprint={1511.06422},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.06422}, 
}

@article{van2019simulating,
  title={Simulating Active Inference Processes by Message Passing},
  author={van de Laar, Thijs W. and de Vries, Bert},
  journal={Frontiers in Robotics and AI},
  volume={6},
  pages={20},
  year={2019},
  url={https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2019.00020},
  doi={10.3389/frobt.2019.00020},
  issn={2296-9144},
  abstract={The free energy principle (FEP) offers a variational calculus-based description for how biological agents persevere through interactions with their environment. Active inference (AI) is a corollary of the FEP, which states that biological agents act to fulfill prior beliefs about preferred future observations (target priors). Purposeful behavior then results from variational free energy minimization with respect to a generative model of the environment with included target priors. However, manual derivations for free energy minimizing algorithms on custom dynamic models can become tedious and error-prone. While probabilistic programming (PP) techniques enable automatic derivation of inference algorithms on free-form models, full automation of AI requires specialized tools for inference on dynamic models, together with the description of an experimental protocol that governs the interaction between the agent and its simulated environment. The contributions of the present paper are two-fold. Firstly, we illustrate how AI can be automated with the use of ForneyLab, a recent PP toolbox that specializes in variational inference on flexibly definable dynamic models. More specifically, we describe AI agents in a dynamic environment as probabilistic state space models (SSM) and perform inference for perception and control in these agents by message passing on a factor graph representation of the SSM. Secondly, we propose a formal experimental protocol for simulated AI. We exemplify how this protocol leads to goal-directed behavior for flexibly definable AI agents in two classical RL examples, namely the Bayesian thermostat and the mountain car parking problems.}
}

@article{trommershauser2003statisticala,
  title={Statistical decision theory and trade-offs in the control of motor response},
  author={Trommershauser, Julia and Maloney, Laurence T and Landy, Michael S},
  journal={Spatial vision},
  volume={16},
  number={3},
  pages={255--275},
  year={2003}
}

@article{trommershauser2003statisticalb,
  title={Statistical decision theory and the selection of rapid, goal-directed movements},
  author={Trommershauser, Julia and Maloney, Laurence T and Landy, Michael S},
  journal={JOSA A},
  volume={20},
  number={7},
  pages={1419--1433},
  year={2003},
  publisher={Optica Publishing Group}
}

@article{moore90efficientmemory,
    author = {Andrew William Moore},
    title = {Efficient Memory-based Learning for Robot Control},
    institution = {University of Cambridge},
    year = {1990}
}

@article{ueltzhoffer2018deep,
   title={Deep active inference},
   volume={112},
   ISSN={1432-0770},
   url={http://dx.doi.org/10.1007/s00422-018-0785-7},
   DOI={10.1007/s00422-018-0785-7},
   number={6},
   journal={Biological Cybernetics},
   publisher={Springer Science and Business Media LLC},
   author={Ueltzhöffer, Kai},
   year={2018},
   month=oct, pages={547–573} }


@article{cox2019factor,
title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
journal = {International Journal of Approximate Reasoning},
volume = {104},
pages = {185-204},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304298},
author = {Marco Cox and Thijs {van de Laar} and Bert {de Vries}},
keywords = {Probabilistic programming, Bayesian inference, Julia, Factor graphs, Message passing},
abstract = {The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs (FFGs), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed “ForneyLab”2 as a Julia toolbox for message passing-based inference in FFGs. We show by example how ForneyLab enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the FFG framework, both the model specification and inference methods are readily extensible in ForneyLab. In order to test this framework, we compared variational message passing as implemented by ForneyLab with automatic differentiation variational inference (ADVI) and Monte Carlo methods as implemented by state-of-the-art tools “Edward” and “Stan”. In terms of performance, extensibility and stability issues, ForneyLab appears to enjoy an edge relative to its competitors for automated inference in state-space models.}
}

@article{bagaev2023rxinfer,
  doi = {10.21105/joss.05161},
  url = {https://doi.org/10.21105/joss.05161},
  year = {2023},
  publisher = {The Open Journal},
  volume = {8},
  number = {84},
  pages = {5161},
  author = {Dmitry Bagaev and Albert Podusenko and Bert de Vries},
  title = {RxInfer: A Julia package for reactive real-time Bayesian inference},
  journal = {Journal of Open Source Software}
}

@misc{bagaev2021reactive,
      title={Reactive Message Passing for Scalable Bayesian Inference}, 
      author={Dmitry Bagaev and Bert de Vries},
      year={2021},
      eprint={2112.13251},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.13251}, 
}

@inproceedings{petersen2018approximate,
   title={On Approximate Nonlinear Gaussian Message Passing on Factor Graphs},
   url={http://dx.doi.org/10.1109/SSP.2018.8450699},
   DOI={10.1109/ssp.2018.8450699},
   booktitle={2018 IEEE Statistical Signal Processing Workshop (SSP)},
   publisher={IEEE},
   author={Petersen, Eike and Hoffmann, Christian and Rostalski, Philipp},
   year={2018},
   month=jun, pages={513–517} }
